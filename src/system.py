import pickle
from sklearn.utils import resample
from comodification_analysis import (
    run_comodification_analysis,
    CoModificationAnalyzer,
    correlate_with_gamma,
    plot_comodification_evolution,
    plot_global_correlation_summary
)
from sklearn.linear_model import LogisticRegression, LinearRegression, Ridge
from sklearn.preprocessing import StandardScaler
from lifelines import KaplanMeierFitter, CoxPHFitter
from lifelines.statistics import logrank_test
import statsmodels.api as sm
from sklearn.metrics import roc_auc_score, brier_score_loss, mean_squared_error, average_precision_score
from scipy.stats import wilcoxon
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from itertools import product

import matplotlib.patches as patches
import bisect
import random
import warnings

warnings.filterwarnings("ignore", category=FutureWarning, module="statsmodels")

from collections import defaultdict
import concurrent.futures
import multiprocessing
import os
import pygit2
from scipy.optimize import curve_fit, OptimizeWarning, brentq
from sklearn.metrics import r2_score
import warnings
from tqdm import tqdm
warnings.filterwarnings("ignore", category=OptimizeWarning)
warnings.filterwarnings("ignore", category=RuntimeWarning)

try:
    import git
except ImportError:
    print("‚ö†Ô∏è La librairie 'gitpython' manque. Installez-la via 'pip install gitpython'")

try:
    from scipy import stats
    from diptest import diptest
except ImportError:
    def diptest(data):
        return 0, 1.0


# ==============================================================================
# 1. CONFIGURATION
# ==============================================================================
BASE_PATH = "/Users/toto/repo/analyse/"
CACHE_DIR = BASE_PATH + "cache_v34/"
#'LINUX': {'path': BASE_PATH + 'linux', 'branch': 'master', 'core_paths': ['kernel/', 'mm/', 'fs/', 'arch/x86/'],
#          'ignore_paths': ['drivers/', 'tools/'], 'color': '#2c3e50'},

SURVIVAL_HORIZON_MONTHS = 6

PROJECT_STATUS = {
    # --- TITANS & KERNELS ---
    'LINUX': 'alive', 'KUBERNETES': 'alive', 'FREEBSD': 'alive',

    # --- BROWSERS ---
    'GECKO_FIREFOX': 'alive', 'WEBKIT': 'alive',

    # --- DATABASES ---
    'POSTGRES': 'alive', 'REDIS': 'alive', 'SQLITE': 'alive',

    # --- SERVERS ---
    'NGINX': 'alive', 'HTTPD_APACHE': 'alive',

    # --- COMPILERS & LANGUAGES ---
    'GCC': 'alive', 'LLVM': 'alive', 'CPYTHON': 'alive',
    'GO': 'alive', 'RUST': 'alive', 'PHP': 'alive', 'NODE': 'alive',

    # --- AI/ML ---
    'PYTORCH': 'alive', 'TENSORFLOW': 'alive', 'SCIPY': 'alive',
    'OCTAVE': 'alive', 'MATPLOTLIB': 'alive',

    # --- WEB FRAMEWORKS ---
    'REACT': 'alive', 'VUE': 'alive', 'ANGULAR': 'alive',
    'RAILS': 'alive', 'DJANGO': 'alive', 'FASTAPI': 'alive',

    # --- TOOLS ---
    'GIT_SCM': 'alive', 'VSCODE': 'alive', 'FFMPEG': 'alive',
    'CURL': 'alive', 'EMACS': 'alive', 'GIMP': 'alive',
    'WIRESHARK': 'alive', 'SUBVERSION': 'alive',

    # --- APPS & PLATFORMS ---
    'BITCOIN': 'alive', 'GODOT': 'alive', 'WORDPRESS': 'alive',
    'LIBREOFFICE': 'alive', 'MEDIAWIKI': 'alive',

    # --- DEAD/DECLINING ---
    'METEOR': 'declining',
    'ANGULAR_JS_LEGACY': 'dead',
    # --- DEAD
    'PY_MINI_PROJECTS': 'dead',
    'WENYAN_LANG': 'dead',
    'TSDX': 'dead',
    'ZY_PLAYER': 'dead',
    'SNOWPACK': 'dead',
    'DALLE_MINI': 'dead',
    'GENERIC_TOOLS': 'dead',
}

# ==============================================================================
# CONFIGURATION DU CORPUS MASSIF (TITANS & CATH√âDRALES)
# ==============================================================================
BASE_PATH = "/Users/toto/repo/analyse/"

REPOS_CONFIGTOT = {

    # --- 3. INFRASTRUCTURE & BASES DE DONN√âES (CATH√âDRALES) ---

    'POSTGRES': {
        'path': BASE_PATH + 'postgres', 'branch': 'master',
        'core_paths': ['src/backend/'],
        'ignore_paths': ['src/test/', 'doc/'],
        'color': '#336791'
    },
    'REDIS': {
        'path': BASE_PATH + 'redis', 'branch': 'unstable',
        'core_paths': ['src/'],
        'ignore_paths': ['tests/', 'deps/'],
        'color': '#a30000'
    },
    'SQLITE': {
        'path': BASE_PATH + 'sqlite', 'branch': 'master',
        'core_paths': ['src/'],
        'ignore_paths': ['test/', 'tool/'],
        'color': '#003b57'
    },

    'NGINX': {
        'path': BASE_PATH + 'nginx', 'branch': 'master',
        'core_paths': ['src/core/', 'src/http/', 'src/event/'],
        'ignore_paths': ['docs/', 'misc/'],
        'color': '#009639'
    },
    'HTTPD_APACHE': {
        'path': BASE_PATH + 'httpd', 'branch': 'trunk',
        'core_paths': ['server/', 'modules/http/'],
        'ignore_paths': ['docs/', 'test/'],
        'color': '#d22128'
    },

    # --- 4. COMPILATEURS & LANGAGES ---
    'GCC': {
        'path': BASE_PATH + 'gcc', 'branch': 'master',
        'core_paths': ['gcc/'],
        'ignore_paths': ['libgo/', 'libjava/', 'gcc/testsuite/'],
        'color': '#f39c12'
    },

    'CPYTHON': {
        'path': BASE_PATH + 'cpython', 'branch': 'main',
        'core_paths': ['Python/', 'Objects/', 'Lib/'],
        'ignore_paths': ['Doc/', 'Tools/', 'Lib/test/'],
        'color': '#3776ab'
    },
    'GO': {
        'path': BASE_PATH + 'go', 'branch': 'master',
        'core_paths': ['src/runtime/', 'src/cmd/compile/'],
        'ignore_paths': ['test/', 'doc/'],
        'color': '#00add8'
    },
    'RUST': {
        'path': BASE_PATH + 'rust', 'branch': 'master',
        'core_paths': ['compiler/', 'library/std/'],
        'ignore_paths': ['src/test/', 'tests/'],
        'color': '#dea584'
    },

    'PHP': {
        'path': BASE_PATH + 'php-src', 'branch': 'master',
        'core_paths': ['Zend/', 'main/', 'ext/standard/'],
        'ignore_paths': ['tests/', 'docs/'],
        'color': '#4f5d95'
    },
    'NODE': {
        'path': BASE_PATH + 'node', 'branch': 'main',
        'core_paths': ['src/', 'lib/'],
        'ignore_paths': ['test/', 'doc/'],
        'color': '#026e00'
    },

    # --- 5. LIBRARIES IA & SCIENCE ---
    'PYTORCH': {
        'path': BASE_PATH + 'pytorch', 'branch': 'main',
        'core_paths': ['torch/csrc/', 'aten/src/'],
        'ignore_paths': ['test/', 'docs/'],
        'color': '#ee4c2c'
    },
    'TENSORFLOW': {
        'path': BASE_PATH + 'tensorflow', 'branch': 'master',
        'core_paths': ['tensorflow/core/'],
        'ignore_paths': ['tensorflow/examples/', 'third_party/'],
        'color': '#ff6f00'
    },
    'SCIPY': {
        'path': BASE_PATH + 'scipy', 'branch': 'main',
        'core_paths': ['scipy/'],
        'ignore_paths': ['doc/', 'benchmarks/'],
        'color': '#0054a6'
    },
    'OCTAVE': {
        'path': BASE_PATH + 'octave', 'branch': 'default',
        'core_paths': ['libinterp/', 'liboctave/'],
        'ignore_paths': ['test/', 'doc/'],
        'color': '#e07a33'
    },

    # --- 6. WEB FRAMEWORKS & TOOLS (DISSIPATIFS) ---
    'REACT': {
        'path': BASE_PATH + 'react', 'branch': 'main',
        'core_paths': ['packages/react/', 'packages/react-reconciler/'],
        'ignore_paths': ['fixtures/', 'scripts/'],
        'color': '#61dafb'
    },
    'VUE': {
        'path': BASE_PATH + 'vuecore', 'branch': 'main',
        'core_paths': ['packages/runtime-core/'],
        'ignore_paths': ['packages/docs/'],
        'color': '#41b883'
    },
    'ANGULAR': {
        'path': BASE_PATH + 'angular', 'branch': 'main',
        'core_paths': ['packages/core/'],
        'ignore_paths': ['aio/', 'devtools/'],
        'color': '#dd0031'
    },
    'RAILS': {
        'path': BASE_PATH + 'rails', 'branch': 'main',
        'core_paths': ['activerecord/', 'actionpack/'],
        'ignore_paths': ['guides/', 'activestorage/'],
        'color': '#cc0000'
    },
    'DJANGO': {
        'path': BASE_PATH + 'django', 'branch': 'main',
        'core_paths': ['django/core/', 'django/db/'],
        'ignore_paths': ['docs/', 'tests/'],
        'color': '#092e20'
    },
    'METEOR': {
        'path': BASE_PATH + 'meteor', 'branch': 'devel',
        'core_paths': ['packages/', 'tools/'],
        'ignore_paths': ['examples/', 'docs/'],
        'color': '#de4f4f'
    },
    'VSCODE': {
        'path': BASE_PATH + 'vscode', 'branch': 'main',
        'core_paths': ['src/vs/'],
        'ignore_paths': ['test/', 'extensions/'],
        'color': '#007acc'
    },
    'GIT_SCM': { # Renomm√© pour √©viter conflit avec le mot cl√© git
        'path': BASE_PATH + 'git', 'branch': 'master',
        'core_paths': ['builtin/', 'xdiff/'],
        'ignore_paths': ['t/', 'Documentation/'],
        'color': '#e67e22'
    },
    'FFMPEG': {
        'path': BASE_PATH + 'FFmpeg', 'branch': 'master',
        'core_paths': ['libavcodec/', 'libavformat/'],
        'ignore_paths': ['doc/', 'tests/'],
        'color': '#005f0f'
    },
    'BITCOIN': {
        'path': BASE_PATH + 'bitcoin', 'branch': 'master',
        'core_paths': ['src/'],
        'ignore_paths': ['doc/', 'test/'],
        'color': '#f7931a'
    },
    'WIRESHARK': {
        'path': BASE_PATH + 'wireshark', 'branch': 'master',
        'core_paths': ['epan/', 'wiretap/'],
        'ignore_paths': ['doc/', 'test/'],
        'color': '#1f5bff'
    },
    'GODOT': {
        'path': BASE_PATH + 'godot', 'branch': 'master',
        'core_paths': ['core/', 'scene/', 'servers/'],
        'ignore_paths': ['doc/', 'tests/'],
        'color': '#478cbf'
    },
    'EMACS': {        'path': BASE_PATH + 'emacs', 'branch': 'master',        'core_paths': ['src/', 'lisp/'],
        'ignore_paths': ['doc/', 'test/'],        'color': '#7f5ab6'    },
        # --- 7. LES OUBLI√âS MAJEURS (AJOUTS) ---

    'CURL': {  # Le standard mondial
        'path': BASE_PATH + 'curl', 'branch': 'master',
        'core_paths': ['lib/', 'src/'],
        'ignore_paths': ['tests/', 'docs/'],
        'color': '#073642'
    },
    'GIMP': {  # Desktop App complexe (C)
        'path': BASE_PATH + 'gimp', 'branch': 'master',
        'core_paths': ['app/', 'libgimp/'],
        'ignore_paths': ['po/', 'icons/'],
        'color': '#5c5238'
    },
    'WORDPRESS': {  # Le web (PHP)
        'path': BASE_PATH + 'wordpress-develop', 'branch': 'trunk',
        'core_paths': ['src/wp-includes/', 'src/wp-admin/'],
        'ignore_paths': ['tests/', 'tools/'],
        'color': '#21759b'
    },




    'MEDIAWIKI': {          'path': BASE_PATH + 'mediawiki', 'branch': 'master',
        'core_paths': ['includes/', 'languages/'],        'ignore_paths': ['tests/', 'docs/'],        'color': '#eaecf0'    },

    'MATPLOTLIB': {  # Python Science
        'path': BASE_PATH + 'matplotlib', 'branch': 'main',
        'core_paths': ['lib/matplotlib/', 'src/'],
        'ignore_paths': ['doc/', 'examples/'],
        'color': '#11557c'
    },
    'FASTAPI': {  # Python Moderne
        'path': BASE_PATH + 'fastapi', 'branch': 'master',
        'core_paths': ['fastapi/'],
        'ignore_paths': ['docs/', 'tests/'],
        'color': '#05998b'
    },
    'SUBVERSION': {  # L'anc√™tre de Git (C)
        'path': BASE_PATH + 'subversion', 'branch': 'trunk',
        'core_paths': ['subversion/libsvn_client/', 'subversion/libsvn_wc/'],
        'ignore_paths': ['subversion/tests/', 'doc/'],
        'color': '#8e1917'
    },
    'ANGULAR_JS_LEGACY': {  # Pour la mort
        'path': BASE_PATH + 'angular.js', 'branch': 'master',
        'core_paths': ['src/'],
        'ignore_paths': ['test/', 'docs/'],
        'color': '#b52e31'
    },

    # --- 1. LES TITANS (OS & KERNELS) ---

    'KUBERNETES': {'path': BASE_PATH + 'kubernetes', 'branch': 'master', 'core_paths': ['pkg/', 'cmd/', 'staging/'],
                   'ignore_paths': ['test/', 'docs/', 'vendor/'], 'color': '#326ce5'},
    'FREEBSD': {     'path': BASE_PATH + 'freebsd-src', 'branch': 'main',
        'core_paths': ['sys/kern/', 'sys/vm/', 'sys/sys/'],        'ignore_paths': ['sys/dev/', 'contrib/', 'tests/'],        'color': '#ab2b28'    },

    # --- 2. LES NAVIGATEURS (COMPLEXIT√â MAXIMALE) ---



     'LIBREOFFICE': {         'path': BASE_PATH + 'libreoffice-core', 'branch': 'master',        'core_paths': ['sw/', 'sc/', 'sal/', 'vcl/'],  # Writer, Calc, System Abstraction, GUI
        'ignore_paths': ['solenv/', 'translations/', 'instdir/'],        'color': '#18a303'    },
   # 'GECKO_FIREFOX': {        'path': BASE_PATH + 'gecko-dev', 'branch': 'master',        'core_paths': ['dom/', 'js/src/', 'layout/'],
    #    'ignore_paths': ['testing/', 'python/', 'third_party/'],        'color': '#e66000'    },
    'WEBKIT': {        'path': BASE_PATH + 'WebKit', 'branch': 'main',        'core_paths': ['Source/WebCore/', 'Source/JavaScriptCore/'],
        'ignore_paths': ['LayoutTests/', 'ManualTests/'],        'color': '#8e44ad'    },
    'LLVM': {        'path': BASE_PATH + 'llvm-project', 'branch': 'main',        'core_paths': ['llvm/lib/', 'clang/lib/'],
         'ignore_paths': ['llvm/test/', 'clang/test/', 'lldb/'],        'color': '#2c3e50'    },
    'LINUX': {'path': BASE_PATH + 'linux', 'branch': 'master', 'core_paths': ['kernel/', 'mm/', 'fs/', 'sched/'],
              'ignore_paths': ['drivers/', 'arch/', 'tools/', 'Documentation/', 'samples/'], 'color': '#000000'},

    'PY_MINI_PROJECTS': {
        'path': BASE_PATH + 'python-mini-projects',
        'branch': 'master',
        'core_paths': ['projects/'],
         'ignore_paths': ['docs/', 'Notebooks/', '.github/', 'requirementsALL.txt', 'CODE_OF_CONDUCT.md', 'LICENSE', 'README_TEMPLATE.md'],
        'color': '#3498db'
    },

    # 2. wenyan-lang/wenyan
    'WENYAN_LANG': {
        'path': BASE_PATH + 'wenyan',
        'branch': 'master',
        'core_paths': ['src/', 'lib/', 'tools/'],
        'ignore_paths': ['examples/', 'documentation/', 'test/', 'site/', 'renders/', 'static/'],
        'color': '#f1c40f'
    },

    # 3. jaredpalmer/tsdx
    'TSDX': {
        'path': BASE_PATH + 'tsdx',
        'branch': 'master',
        'core_paths': ['src/', 'templates/', 'test/'],
        'ignore_paths': ['website/', '.github/', 'jest.config.js', 'CODE_OF_CONDUCT.md', 'CONTRIBUTING.md'],
        'color': '#e74c3c'
    },

    # 4. Hunlongyu/ZY-Player
    'ZY_PLAYER': {
        'path': BASE_PATH + 'ZY-Player',
        'branch': 'master',
        'core_paths': ['src/'],
        'ignore_paths': ['build/', 'docs/', 'public/', 'package-lock.json', 'babel.config.js', 'vue.config.js'],
        'color': '#2ecc71'
    },

    # 5. FredKSchott/snowpack

    'SNOWPACK': {
        'path': BASE_PATH + 'snowpack',
        'branch': 'main',
        'core_paths': ['snowpack/', 'esinstall/', 'plugins/', 'create-snowpack-app/'],
        'ignore_paths': ['docs/', 'examples/', 'www/', 'test/', 'test-dev/', 'scripts/', 'jest.config.js'],
        'color': '#9b59b6'
    },

    # 6. borisdayma/dalle-mini

    'DALLE_MINI': {
        'path': BASE_PATH + 'dalle-mini',
        'branch': 'main',
        'core_paths': ['src/', 'app/', 'tools/'],
        'ignore_paths': ['Docker/', 'img/', 'README.md', 'run_docker_image.sh', 'setup.cfg', 'setup.py'],
        'color': '#34495e'
    },

    # 7. tools (Hypoth√®se d'un d√©p√¥t Rust, bas√© sur Cargo.toml/crates)

    'GENERIC_TOOLS': {
        'path': BASE_PATH + 'tools',
        'branch': 'main',
        'core_paths': ['crates/', 'xtask/'],
        'ignore_paths': ['website/', 'rfcs/', 'benchmark/', 'ci/', 'editors/', 'npm/', 'justfile'],
        'color': '#f39c12'
    },

}

REPOS_CONFIGCACHE = {


    'GECKO_FIREFOX': {        'path': BASE_PATH + 'gecko-dev', 'branch': 'master',        'core_paths': ['dom/', 'js/src/', 'layout/'],
        'ignore_paths': ['testing/', 'python/', 'third_party/'],        'color': '#e66000'    },
    'WEBKIT': {        'path': BASE_PATH + 'WebKit', 'branch': 'main',        'core_paths': ['Source/WebCore/', 'Source/JavaScriptCore/'],
        'ignore_paths': ['LayoutTests/', 'ManualTests/'],        'color': '#8e44ad'    },
    'LLVM': {        'path': BASE_PATH + 'llvm-project', 'branch': 'main',        'core_paths': ['llvm/lib/', 'clang/lib/'],
         'ignore_paths': ['llvm/test/', 'clang/test/', 'lldb/'],        'color': '#2c3e50'    },

}

REPOS_CONFIG =REPOS_CONFIGTOT
SAMPLE_RATE = 1
BLAME_SAMPLE_SIZE = 50
MAX_WORKERS = 6

# ==============================================================================
# MODULE V41 : VALIDATION EXTERNE (ANTI-CIRCULARIT√â & GOUVERNANCE)
# ==============================================================================



# =============================================================================
# 1. DICTIONNAIRE DE GOUVERNANCE RAFFIN√â
# =============================================================================

GOVERNANCE_TIER = {
    # --- NIVEAU 3 : FONDATION / CONSENSUS (Autonomie R√©elle) ---
    # Crit√®re : Appartenance √† une fondation neutre (Apache, Linux, NumFOCUS, CNCF)
    'LINUX': 3, 'KUBERNETES': 3, 'FREEBSD': 3, 'POSTGRES': 3,
    'HTTPD_APACHE': 3, 'LLVM': 3, 'GCC': 3, 'CPYTHON': 3,
    'RUST': 3, 'NODE': 3, 'WEBKIT': 3, 'LIBREOFFICE': 3,
    'MEDIAWIKI': 3, 'GIT_SCM': 3, 'CURL': 3, 'FFMPEG': 3,
    'EMACS': 3, 'SUBVERSION': 3, 'GIMP': 3, 'GODOT': 3,
    'BITCOIN': 3,  'DJANGO': 3,  'WIRESHARK': 3,
    # Science (NumFOCUS = Fondation Neutre)
    'SCIPY': 3, 'MATPLOTLIB': 3,

    # --- NIVEAU 2 : CORPORATIF / SPONSORIS√â (H√©g√©monie) ---
    # Crit√®re : Roadmap dict√©e par une seule entit√© commerciale majeure
    'REACT': 2,  # Meta
    'ANGULAR': 2,  # Google
    'VUE': 2,  # Mod√®le hybride, mais forte d√©pendance sponsors
    'PYTORCH': 2,  # Meta (Transition fondation r√©cente, historique corp)
    'TENSORFLOW': 2,  # Google
    'VSCODE': 2,  # Microsoft
    'GO': 2,  # Google
    'OCTAVE': 2,
    'GECKO_FIREFOX': 2,  # Mozilla Corp
    'RAILS': 2,  # 37signals

    'FASTAPI': 2,  # Sponsor-driven
    'PHP': 2,  # Historiquement Zend/Community mix
    'REDIS': 2,  # Redis Ltd
    'SQLITE': 2,  # Hwaci (Unique cr√©ateur + entreprise)
    'NGINX': 2,  # F5

    'WORDPRESS': 2,  # Automattic

    # --- NIVEAU 1 : PERSONNEL / AD-HOC (D√©pendance Individuelle) ---
    # Crit√®re : Projet port√© par un individu ou sans structure l√©gale
    'METEOR': 1, 'ANGULAR_JS_LEGACY': 1, 'PY_MINI_PROJECTS': 1,
    'WENYAN_LANG': 1, 'TSDX': 1, 'ZY_PLAYER': 1, 'SNOWPACK': 1,
    'DALLE_MINI': 1, 'GENERIC_TOOLS': 1
}



class ScientificValidator:
    """
    MODULE V44 : VALIDATION SCIENTIFIQUE 'REVIEWER-PROOF'
    Int√®gre bootstrap par projet, alignement strict et m√©triques de sym√©trie.
    """

    def __init__(self, all_dataframes):
        # On ne garde que les projets significatifs (> 24 mois)
        self.dfs = {k: v for k, v in all_dataframes.items() if v is not None and len(v) > 24}
        self.project_keys = list(self.dfs.keys())

        # Donn√©es align√©es (Gamma + Granger) calcul√©es √† la vol√©e
        self.aligned_data = None

    def _align_granger_gamma(self, crossover_results):
        """
        HELPER CRITIQUE : Aligne strictement Gamma et les scores Granger par date.
        Complexit√© O(N), utilise les index pandas (plus robuste que les listes).
        """
        pooled_data = []

        for name, res in crossover_results.items():
            if name not in self.dfs: continue

            df = self.dfs[name]

            # Pr√©paration Granger en DataFrame
            granger_df = pd.DataFrame({
                'date': res['dates'],
                's_ag': res['strength_ag'],  # Act -> Struct
                's_ga': res['strength_ga']  # Struct -> Act
            }).set_index('date')

            # Pr√©paration Gamma
            gamma_series = df['monthly_gamma']

            # INNER JOIN strict sur les dates (ne garde que les mois o√π on a TOUT)
            merged = granger_df.join(gamma_series, how='inner')
            merged['project'] = name

            # Calcul de la m√©trique de Sym√©trie (Coupling Ratio)
            # 1.0 = Parfaite sym√©trie (Operational Closure)
            # 0.0 = Dominance totale unilat√©rale
            # On utilise min/max comme sugg√©r√© par le reviewer
            merged['coupling_ratio'] = merged[['s_ag', 's_ga']].min(axis=1) / (
                        merged[['s_ag', 's_ga']].max(axis=1) + 1e-9)

            pooled_data.append(merged)

        if pooled_data:
            self.aligned_data = pd.concat(pooled_data)
        else:
            self.aligned_data = pd.DataFrame()

    def solve_gmm_intersection(self, gmm):
        """Trouve le point d'intersection (seuil naturel) entre deux gaussiennes."""
        means = gmm.means_.flatten()
        stds = np.sqrt(gmm.covariances_.flatten())
        weights = gmm.weights_

        # Ordre : idx 0 = bas, idx 1 = haut
        if means[0] > means[1]:
            means = means[::-1]
            stds = stds[::-1]
            weights = weights[::-1]

        def diff_pdf(x):
            p1 = weights[0] * stats.norm.pdf(x, means[0], stds[0])
            p2 = weights[1] * stats.norm.pdf(x, means[1], stds[1])
            return p1 - p2

        try:
            # Recherche racine entre les moyennes
            threshold = brentq(diff_pdf, means[0], means[1])
        except Exception:
            threshold = (means[0] + means[1]) / 2

        return threshold, means, stds, weights

    def run_test_1_endogenous_threshold(self):
        """TEST 1 : Seuil Endog√®ne GMM."""
        print("\nüß™ [TEST 1] D√©termination du Seuil Endog√®ne (GMM)...")

        # Collecte de tous les gammas
        all_gamma = []
        for df in self.dfs.values():
            all_gamma.extend(df['monthly_gamma'].dropna().values)
        all_gamma = np.array(all_gamma).reshape(-1, 1)

        gmm = GaussianMixture(n_components=2, random_state=42, n_init=10)
        gmm.fit(all_gamma)

        threshold, means, stds, weights = self.solve_gmm_intersection(gmm)

        print(f"   R√©gime 1 (Explo) : Œº={means[0]:.3f}, œÉ={stds[0]:.3f}, w={weights[0]:.2f}")
        print(f"   R√©gime 2 (Sedim) : Œº={means[1]:.3f}, œÉ={stds[1]:.3f}, w={weights[1]:.2f}")
        print(f"   üéØ SEUIL NATUREL : {threshold:.4f}")

        return threshold

    def run_test_2_local_robustness(self, natural_threshold):
        """
        TEST 2 : Robustesse Locale (Variance ET Sym√©trie).
        V√©rifie si la Variance baisse ET si le Couplage augmente dans le r√©gime haut,
        quel que soit le seuil exact choisi.
        """
        print(f"\nüß™ [TEST 2] Robustesse Locale (Variance & Sym√©trie)...")

        if self.aligned_data is None or self.aligned_data.empty:
            print("   ‚ö†Ô∏è Pas de donn√©es align√©es pour tester la sym√©trie.")
            return

        test_thresholds = np.arange(natural_threshold - 0.1, natural_threshold + 0.1, 0.02)
        results = []

        for t in test_thresholds:
            # S√©paration bas√©e sur le seuil t
            low_regime = self.aligned_data[self.aligned_data['monthly_gamma'] < t]
            high_regime = self.aligned_data[self.aligned_data['monthly_gamma'] >= t]

            if len(low_regime) < 10 or len(high_regime) < 10: continue

            # M√©trique 1 : Variance Collapse (Doit √™tre > 1.0)
            var_ratio = low_regime['monthly_gamma'].var() / (high_regime['monthly_gamma'].var() + 1e-9)

            # M√©trique 2 : Symmetry Gain (Doit √™tre positif)
            # On veut que le r√©gime HAUT soit plus sym√©trique (plus proche de 1) que le BAS
            sym_low = low_regime['coupling_ratio'].mean()
            sym_high = high_regime['coupling_ratio'].mean()
            sym_gain = sym_high - sym_low

            results.append({
                'threshold': t,
                'var_ratio': var_ratio,
                'sym_gain': sym_gain,
                'sym_high': sym_high
            })

        df_res = pd.DataFrame(results)

        # Export pour SI (Supplementary Information)
        df_res.to_csv("omega_v44_robustness_grid.csv", index=False)

        # Analyse
        mean_gain = df_res['sym_gain'].mean()
        valid_steps = (df_res['sym_gain'] > 0).mean() * 100

        print(f"   Analyse sur {len(df_res)} pas de seuils autour de {natural_threshold:.2f} :")
        print(f"   Gain moyen de Sym√©trie (Haut - Bas) : +{mean_gain:.3f}")
        print(f"   Robustesse (Cas positifs) : {valid_steps:.1f}%")

        if valid_steps > 80:
            print("   ‚úÖ ROBUSTE : Le r√©gime haut est structurellement plus sym√©trique (Operational Closure).")
        else:
            print("   ‚ö†Ô∏è INSTABLE : La sym√©trie d√©pend trop du seuil choisi.")

    def run_test_3_continuous_dynamics(self):
        """
        TEST 3 : Dynamique Continue (LOESS).
        Relation Gamma -> Sym√©trie (Coupling Ratio) sans seuil.
        Utilise les donn√©es align√©es strictement.
        """
        print("\nüß™ [TEST 3] Dynamique Continue (Gamma -> Sym√©trie)...")

        if self.aligned_data is None or self.aligned_data.empty:
            print("   ‚ö†Ô∏è Donn√©es insuffisantes.")
            return

        # X = Gamma, Y = Coupling Ratio (0..1)
        data = self.aligned_data.dropna(subset=['monthly_gamma', 'coupling_ratio'])

        x = data['monthly_gamma'].values
        y = data['coupling_ratio'].values

        # Tri pour LOESS
        sort_idx = np.argsort(x)
        x_sorted = x[sort_idx]
        y_sorted = y[sort_idx]

        # Lissage LOESS
        lowess = sm.nonparametric.lowess(y_sorted, x_sorted, frac=0.3)
        y_smooth = lowess[:, 1]

        # Calcul de pente globale (Lin√©aire simple pour la tendance)
        slope, intercept = np.polyfit(x_sorted, y_sorted, 1)

        # Comparaison d√©but vs fin de la courbe liss√©e (plus robuste que la pente lin√©aire)
        start_val = y_smooth[:50].mean()  # Moyenne des faibles gammas
        end_val = y_smooth[-50:].mean()  # Moyenne des forts gammas
        delta = end_val - start_val

        print(f"   Points analys√©s : {len(x)}")
        print(f"   Pente globale : {slope:.4f}")
        print(f"   Progression Sym√©trie (Liss√©e) : {start_val:.2f} -> {end_val:.2f} (Œî={delta:+.2f})")

        # Export des donn√©es liss√©es pour plot
        pd.DataFrame({'gamma': x_sorted, 'symmetry_smooth': y_smooth}).to_csv("omega_v44_loess_symmetry.csv")

        if delta > 0.1:
            print("   ‚úÖ VALID√â : Tendance continue vers la sym√©trie causale.")
        else:
            print("   ‚ùå NON VALID√â : Pas de tendance claire.")

    def run_test_4_bootstrap_by_project(self, n_iterations=100):
        """
        TEST 4 : Bootstrap PAR PROJET (Correct I.I.D. assumption).
        Pr√©serve la structure temporelle interne des projets.
        """
        print(f"\nüß™ [TEST 4] Bootstrap Structurel (par Projet, n={n_iterations})...")

        boot_thresholds = []

        for i in range(n_iterations):
            # 1. R√©√©chantillonner la LISTE des projets (avec remise)
            # Ex: [Linux, Redis, Linux, React...]
            sampled_keys = resample(self.project_keys, replace=True, random_state=i)

            # 2. Reconstruire un corpus fictif
            fake_corpus_gamma = []
            for k in sampled_keys:
                # On prend tout le vecteur gamma de ce projet
                fake_corpus_gamma.extend(self.dfs[k]['monthly_gamma'].dropna().values)

            fake_corpus_gamma = np.array(fake_corpus_gamma).reshape(-1, 1)

            # 3. Recalculer le seuil GMM sur ce corpus fictif
            try:
                gmm = GaussianMixture(n_components=2, random_state=42)  # Random state fixe pour stabilit√© algo
                gmm.fit(fake_corpus_gamma)
                t, _, _, _ = self.solve_gmm_intersection(gmm)

                # Filtrage basique des √©checs de convergence (seuils absurdes)
                if 0.3 < t < 0.9:
                    boot_thresholds.append(t)
            except:
                pass

        if not boot_thresholds:
            print("   ‚ö†Ô∏è √âchec du Bootstrap.")
            return

        mean_t = np.mean(boot_thresholds)
        ci_lower = np.percentile(boot_thresholds, 2.5)
        ci_upper = np.percentile(boot_thresholds, 97.5)
        width = ci_upper - ci_lower

        print(f"   Seuil Moyen (Bootstrap) : {mean_t:.4f}")
        print(f"   IC 95% : [{ci_lower:.4f} - {ci_upper:.4f}]")
        print(f"   Largeur IC : {width:.4f}")

        if width < 0.2:
            print("   ‚úÖ STABLE : Le seuil est une propri√©t√© robuste du corpus.")
        else:
            print("   ‚ö†Ô∏è LARGE : Le seuil d√©pend fortement de quelques projets cl√©s.")

    def run_full_suite(self, crossover_results):
        print("\n" + "=" * 70)
        print("D√âMARRAGE SUITE DE VALIDATION V44 (HARD SCIENCE / REVIEWER PROOF)")
        print("=" * 70)

        # 0. Pr√©-calcul (Alignement Gamma-Granger)
        # Indispensable pour Test 2 et 3
        print("üõ†Ô∏è  Alignement temporel strict (Gamma ‚Üî Granger)...")
        self._align_granger_gamma(crossover_results)

        # Test 1 : Seuil Naturel
        natural_threshold = self.run_test_1_endogenous_threshold()

        # Test 2 : Robustesse Locale (Variance + Sym√©trie)
        self.run_test_2_local_robustness(natural_threshold)

        # Test 3 : Dynamique Continue
        self.run_test_3_continuous_dynamics()

        # Test 4 : Bootstrap Structurel
        self.run_test_4_bootstrap_by_project()

        print("=" * 70 + "\n")

class ExternalMaturityValidator:
    def __init__(self, all_dataframes, governance_tier=None):
        self.dfs = {k: v for k, v in all_dataframes.items()
                    if v is not None and len(v) > 12}
        self.governance = governance_tier or GOVERNANCE_TIER

    def get_mature_viability(self, df):
        """
        Calcule l'Index de Viabilit√© (V) moyen sur la phase mature.
        V = Monthly_Gamma * Normalized_Activity

        Pourquoi ?
        - Projet mort : Gamma=1.0, Activity=0.0 -> V=0.0 (Correctement p√©nalis√©)
        - Projet vivant : Gamma=0.9, Activity=0.8 -> V=0.72 (Score √©lev√©)
        """
        n = len(df)
        if n < 12: return np.nan

        # On analyse la deuxi√®me moiti√© de vie du projet (phase mature)
        window = df.iloc[n // 2:].copy()

        # 1. R√©cup√©ration de l'activit√© normalis√©e
        # Si 'norm_total' n'existe pas, on le recalcule par rapport au max historique du projet
        if 'norm_total' in window.columns:
            activity_norm = window['norm_total']
        else:
            max_w = df['total_weight'].max()
            activity_norm = window['total_weight'] / (max_w + 1e-9)

        # 2. Calcul de V pour chaque mois
        window['monthly_viability'] = window['monthly_gamma'] * activity_norm

        # On retourne la m√©diane pour √™tre robuste aux outliers
        return window['monthly_viability'].median()

    def validate_governance(self):
        print("\n" + "=" * 80)
        print("üîç VALIDATION EXTERNE V42 : VIABILIT√â (V) vs GOUVERNANCE")
        print("=" * 80)
        print("Hypoth√®se : L'autonomie institutionnelle n√©cessite une VIABILIT√â (V) √©lev√©e,")
        print("            distinguant les syst√®mes vivants des fossiles structurels.")

        # Collecte des donn√©es
        data_by_tier = {1: [], 2: [], 3: []}

        for name, df in self.dfs.items():
            if name not in self.governance: continue

            # C'EST ICI QUE TOUT CHANGE : ON UTILISE V, PAS GAMMA SEUL
            v_score = self.get_mature_viability(df)

            if not np.isnan(v_score):
                tier = self.governance[name]
                data_by_tier[tier].append(v_score)

        # 1. STATS DESCRIPTIVES
        print(f"\n{'Niveau':<25} | {'N':<5} | {'Moyenne V':<10} | {'M√©diane V':<10} | {'Std':<10}")
        print("-" * 75)
        labels = {1: '1. Personnel (Fossile)', 2: '2. Corporatif', 3: '3. Fondation (Autonome)'}

        for t in [1, 2, 3]:
            d = data_by_tier[t]
            if d:
                print(
                    f"{labels[t]:<25} | {len(d):<5} | {np.mean(d):<10.3f} | {np.median(d):<10.3f} | {np.std(d):<10.3f}")
            else:
                print(f"{labels[t]:<25} | 0     | N/A        | N/A        | N/A")

        # 2. TEST GLOBAL (Kruskal-Wallis)
        # H0: Les distributions sont identiques
        if all(len(d) > 1 for d in data_by_tier.values()):
            stat_kw, p_kw = stats.kruskal(data_by_tier[1], data_by_tier[2], data_by_tier[3])
        else:
            stat_kw, p_kw = 0, 1.0

        print(f"\nüìä TESTS GLOBAUX")
        print(f"   Kruskal-Wallis : H={stat_kw:.2f}, p={p_kw:.4e} {'‚úÖ' if p_kw < 0.05 else '‚ùå'}")

        # 3. TESTS POST-HOC (Mann-Whitney Pairwise avec correction Bonferroni)
        print(f"\nüî¨ COMPARAISONS PAR PAIRES (One-tailed: High > Low)")
        comparisons = [(1, 2), (2, 3), (1, 3)]
        alpha = 0.05 / 3  # Correction Bonferroni
        pairwise_p = {}

        for t_low, t_high in comparisons:
            if len(data_by_tier[t_high]) > 1 and len(data_by_tier[t_low]) > 1:
                u_stat, p_val = stats.mannwhitneyu(data_by_tier[t_high], data_by_tier[t_low], alternative='greater')
                sig = "Significatif" if p_val < alpha else "Non significatif"
                print(f"   {labels[t_high]} > {labels[t_low]} : p={p_val:.4f} ({sig})")
                pairwise_p[f"{t_high}vs{t_low}"] = p_val
            else:
                pairwise_p[f"{t_high}vs{t_low}"] = 1.0

        # 4. VISUALISATION
        self.plot_results(data_by_tier, pairwise_p)

        return {'p_global': p_kw, 'pairwise': pairwise_p}

    def plot_results(self, data_by_tier, pairwise_p):
        """G√©n√®re le boxplot V42 avec annotations."""
        plt.style.use('seaborn-v0_8-whitegrid')
        fig, ax = plt.subplots(figsize=(10, 6))

        data = [data_by_tier[1], data_by_tier[2], data_by_tier[3]]
        pos = [1, 2, 3]

        # Boxplot
        bp = ax.boxplot(data, positions=pos, patch_artist=True, widths=0.6,
                        medianprops=dict(color='black', linewidth=1.5))

        # Couleurs s√©mantiques : Gris (Mort), Orange (D√©pendant), Vert (Vivant/Autonome)
        colors = ['#95a5a6', '#f39c12', '#2ecc71']
        for patch, color in zip(bp['boxes'], colors):
            patch.set_facecolor(color)
            patch.set_alpha(0.8)

        # Jitter plot
        for i, d in enumerate(data):
            y = d
            x = np.random.normal(i + 1, 0.04, size=len(y))
            ax.scatter(x, y, alpha=0.6, color='#2c3e50', s=25, zorder=3)

        # Annotations statistiques (Brackets)
        def annotate_bracket(x1, x2, y_h, p_val):
            bar_h = y_h * 1.02
            ax.plot([x1, x1, x2, x2], [y_h, bar_h, bar_h, y_h], lw=1.5, c='k')
            stars = "***" if p_val < 0.001 else "**" if p_val < 0.01 else "*" if p_val < 0.05 else "ns"
            ax.text((x1 + x2) * 0.5, bar_h, stars, ha='center', va='bottom', fontweight='bold', fontsize=12)

        max_y = max([max(d) for d in data if d]) if any(data) else 1.0

        # Comparaison critique : 2 vs 3
        annotate_bracket(2, 3, max_y + 0.05, pairwise_p.get('3vs2', 1.0))
        # Comparaison Fossile : 1 vs 3
        annotate_bracket(1, 3, max_y + 0.15, pairwise_p.get('3vs1', 1.0))

        ax.set_xticks(pos)
        ax.set_xticklabels(['Personnel\n(Fragile/Inerte)', 'Corporatif\n(Sponsoris√©)', 'Fondation\n(Autonome)'],
                           fontsize=11)
        ax.set_ylabel(r'Index de Viabilit√© ($V = \Gamma \times A_{norm}$)', fontsize=12)
        ax.set_title('Validation V42 : Viabilit√© Sociotechnique vs Gouvernance', fontsize=14, fontweight='bold')
        ax.set_ylim(0, max_y + 0.25)

        plt.tight_layout()
        plt.savefig("omega_v42_viability_validation.png", dpi=300)
        print("‚úÖ Graphique V42 sauvegard√© : omega_v42_viability_validation.png")
        plt.close()


class HindcastingValidator:
    """
    Protocole de validation pr√©dictive (Hindcasting).
    Objectif : Prouver que Gamma pr√©dit la survie MIEUX que l'√¢ge seul.
    """

    def __init__(self, all_dataframes, project_status=None):
        # Filtre sur les projets longs (> 36 mois)
        self.dfs = {k: v for k, v in all_dataframes.items()
                    if v is not None and len(v) > 36}
        self.project_status = project_status or {}
        self.panel = None

        # Param√®tres temporels
        self.MIN_TRAIN = 24
        self.GAP = 6
        self.TEST_WINDOW = 12
        self.STEP = 6

    def prepare_data(self):
        """Transforme les s√©ries temporelles en Panel Data."""
        print("   [Hindcasting] Pr√©paration du Panel Data...")
        panel_rows = []

        for name, df in self.dfs.items():
            df = df.copy().sort_index()
            n = len(df)

            # Index temporel relatif
            df['month_idx'] = range(n)
            df['project_name'] = name
            df['project_status'] = self.project_status.get(name, 'unknown')

            # === FEATURES (X) ===
            df['project_age'] = df['month_idx']
            df['gamma'] = df['monthly_gamma']
            df['gamma_ma6'] = df['gamma'].rolling(6, min_periods=1).mean()
            df['gamma_trend'] = df['gamma'].diff(3).fillna(0)
            df['gamma_volatility'] = df['gamma'].rolling(6, min_periods=1).std().fillna(0)

            # Activity (normalis√©e par max CONNU √† t, pas futur)
            df['max_activity_so_far'] = df['total_weight'].expanding().max()
            df['activity_norm'] = df['total_weight'] / (df['max_activity_so_far'] + 1e-9)
            df['activity_trend'] = df['activity_norm'].diff(3).fillna(0)

            # === TARGET (Y) : Viabilit√© √† t + GAP ===
            # === TARGET (Y) : Viabilit√© √† t + GAP ===

            # Activit√© future moyenne (liss√©e sur le gap)
            future_activity = df['total_weight'].shift(-self.GAP).rolling(self.GAP, min_periods=1).mean()

            # R√©f√©rence : Activit√© moyenne r√©cente (12 derniers mois)
            recent_activity = df['total_weight'].rolling(12, min_periods=1).mean()

            UNIVERSAL_ACTIVITY_THRESHOLD = 10.0  # Commits pond√©r√©s/mois minimum

            # Un projet est viable s'il maintient une activit√© minimale absolue
            df['target_viable'] = (future_activity > UNIVERSAL_ACTIVITY_THRESHOLD).astype(float)

            df['target_gamma_future'] = df['gamma'].shift(-self.GAP)
            # Nettoyage
            df = df.dropna(subset=['target_viable'])

            panel_rows.append(df)

        self.panel = pd.concat(panel_rows, ignore_index=True)

        # === ORTHOGONALISATION GLOBALE ===
        # Gamma r√©siduel = Gamma - E[Gamma | Age]
        mask = self.panel['gamma'].notna() & self.panel['project_age'].notna()

        lm = LinearRegression()
        X_age = self.panel.loc[mask, ['project_age']].values
        y_gamma = self.panel.loc[mask, 'gamma'].values
        lm.fit(X_age, y_gamma)

        self.panel['gamma_residual'] = np.nan
        self.panel.loc[mask, 'gamma_residual'] = y_gamma - lm.predict(X_age)

        # Stats
        n_obs = len(self.panel)
        n_projects = self.panel['project_name'].nunique()
        viability_rate = self.panel['target_viable'].mean()

        print(f"   Panel : {n_obs} observations, {n_projects} projets")
        print(f"   Taux de viabilit√© : {viability_rate:.1%}")




    def define_feature_sets(self):
        """D√©finit les ensembles de features √† comparer."""
        return {
            'Age Only': ['project_age'],
            'Gamma Only': ['gamma', 'gamma_ma6', 'gamma_trend', 'gamma_volatility'],
            'Gamma Residual': ['gamma_residual', 'gamma_trend', 'gamma_volatility'],
            'Activity Only': ['activity_norm', 'activity_trend'],
            'Full Model': ['project_age', 'gamma', 'gamma_trend', 'activity_norm'],
            'Full No Age': ['gamma', 'gamma_trend', 'gamma_volatility', 'activity_norm'],
        }

    def run_classification(self):
        """Hindcasting pour la viabilit√© binaire (Avec PR-AUC)."""
        if self.panel is None:
            self.prepare_data()

        print(f"\n   [Hindcasting] Classification (Viabilit√© Future)...")

        max_time = self.panel['month_idx'].max()
        current_train_end = self.MIN_TRAIN

        feature_sets = self.define_feature_sets()
        # AJOUT ICI : 'pr_auc' dans le dictionnaire
        results = {name: {'auc': [], 'brier': [], 'pr_auc': []} for name in feature_sets}

        fold_count = 0

        while current_train_end + self.GAP + self.TEST_WINDOW < max_time:
            test_start = current_train_end + self.GAP
            test_end = test_start + self.TEST_WINDOW

            mask_train = self.panel['month_idx'] <= current_train_end
            mask_test = (self.panel['month_idx'] > test_start) & \
                        (self.panel['month_idx'] <= test_end)

            train = self.panel[mask_train]
            test = self.panel[mask_test]

            # Volume minimum
            if len(train) < 100 or len(test) < 30:
                current_train_end += self.STEP
                continue

            y_train = train['target_viable'].values
            y_test = test['target_viable'].values

            # Skip si pas de variance
            if y_test.mean() > 0.98 or y_test.mean() < 0.02:
                current_train_end += self.STEP
                continue

            fold_count += 1

            for model_name, features in feature_sets.items():
                available = [f for f in features if f in train.columns]
                if not available:
                    continue

                X_train = train[available].fillna(0).values
                X_test = test[available].fillna(0).values

                # Normalisation
                scaler = StandardScaler()
                X_train_sc = scaler.fit_transform(X_train)
                X_test_sc = scaler.transform(X_test)

                # Mod√®le
                clf = LogisticRegression(C=0.1, solver='liblinear',
                                         max_iter=1000, random_state=42)
                clf.fit(X_train_sc, y_train)

                probs = clf.predict_proba(X_test_sc)[:, 1]

                try:
                    auc = roc_auc_score(y_test, probs)
                    brier = brier_score_loss(y_test, probs)
                    # AJOUT ICI : Calcul PR-AUC
                    pr_auc = average_precision_score(y_test, probs)

                    results[model_name]['auc'].append(auc)
                    results[model_name]['brier'].append(brier)
                    results[model_name]['pr_auc'].append(pr_auc)
                except Exception:
                    pass

            current_train_end += self.STEP

        print(f"   Folds compl√©t√©s : {fold_count}")
        return results, fold_count

    def run_regression(self):
        """Hindcasting pour Gamma futur (r√©gression)."""
        if self.panel is None:
            self.prepare_data()

        print(f"\n   [Hindcasting] R√©gression (Gamma Futur)...")

        # Filtrer les lignes avec target continu valide
        panel_reg = self.panel.dropna(subset=['target_gamma_future'])

        max_time = panel_reg['month_idx'].max()
        current_train_end = self.MIN_TRAIN

        feature_sets = {
            'Age Only': ['project_age'],
            'Gamma Only': ['gamma', 'gamma_ma6', 'gamma_trend'],
            'Gamma Residual': ['gamma_residual', 'gamma_trend'],
        }

        results = {name: {'rmse': [], 'r2': []} for name in feature_sets}
        fold_count = 0

        while current_train_end + self.GAP + self.TEST_WINDOW < max_time:
            test_start = current_train_end + self.GAP
            test_end = test_start + self.TEST_WINDOW

            mask_train = panel_reg['month_idx'] <= current_train_end
            mask_test = (panel_reg['month_idx'] > test_start) & \
                        (panel_reg['month_idx'] <= test_end)

            train = panel_reg[mask_train]
            test = panel_reg[mask_test]

            if len(train) < 100 or len(test) < 30:
                current_train_end += self.STEP
                continue

            y_train = train['target_gamma_future'].values
            y_test = test['target_gamma_future'].values

            fold_count += 1

            for model_name, features in feature_sets.items():
                available = [f for f in features if f in train.columns]
                X_train = train[available].fillna(0).values
                X_test = test[available].fillna(0).values

                scaler = StandardScaler()
                X_train_sc = scaler.fit_transform(X_train)
                X_test_sc = scaler.transform(X_test)

                reg = Ridge(alpha=1.0)
                reg.fit(X_train_sc, y_train)

                y_pred = reg.predict(X_test_sc)

                rmse = np.sqrt(mean_squared_error(y_test, y_pred))
                ss_res = np.sum((y_test - y_pred) ** 2)
                ss_tot = np.sum((y_test - np.mean(y_test)) ** 2)
                r2 = 1 - (ss_res / (ss_tot + 1e-9))

                results[model_name]['rmse'].append(rmse)
                results[model_name]['r2'].append(r2)

            current_train_end += self.STEP

        return results, fold_count

    def run_full_validation(self):
        """Protocole complet."""
        print("\n" + "=" * 70)
        print("PROTOCOLE DE HINDCASTING COMPLET")
        print("=" * 70)

        # 1. Classification
        clf_results, clf_folds = self.run_classification()

        # 2. R√©gression
        reg_results, reg_folds = self.run_regression()

        # 3. Analyse et Visualisation
        self._analyze_results(clf_results, clf_folds, 'classification')
        self._analyze_results(reg_results, reg_folds, 'regression')

        # 4. Visualisation
        self._plot_results(clf_results, reg_results)

        return {'classification': clf_results, 'regression': reg_results}

    def _analyze_results(self, results, n_folds, task):
        """Analyse statistique compl√®te (AUC + PR-AUC)."""
        print(f"\n" + "-" * 80)
        print(f"R√âSULTATS HINDCASTING - {task.upper()} ({n_folds} Folds)")
        print("-" * 80)

        metric_main = 'auc' if task == 'classification' else 'rmse'
        higher_better = (task == 'classification')

        # Pour PR-AUC, la ligne de base n'est pas 0.5 mais le taux de pr√©valence (ex: 0.91)
        # Mais on va surtout comparer les mod√®les entre eux.

        # Calculer les moyennes
        means = {}
        for name, scores in results.items():
            if metric_main in scores and len(scores[metric_main]) >= 3:
                means[name] = {
                    'main_mean': np.mean(scores[metric_main]),
                    'main_std': np.std(scores[metric_main]),
                    'main_vals': np.array(scores[metric_main]),
                }
                # Si dispo, on ajoute PR-AUC
                if 'pr_auc' in scores and scores['pr_auc']:
                    means[name]['pr_mean'] = np.mean(scores['pr_auc'])

        if not means:
            print("   Pas assez de donn√©es.")
            return

        # Affichage tableau
        # En-t√™te adapt√© selon la t√¢che
        if task == 'classification':
            headers = f"{'Mod√®le':<20} | {'AUC':<8} | {'PR-AUC':<8} | {'Vs Hasard'}"
        else:
            headers = f"{'Mod√®le':<20} | {'RMSE':<8} | {'Std':<8} | {'Ratio'}"

        print(headers)
        print("-" * 80)

        # Trier
        sorted_models = sorted(means.items(),
                               key=lambda x: x[1]['main_mean'],
                               reverse=higher_better)

        base_stats = means.get('Age Only')
        base_val = base_stats['main_mean'] if base_stats else (0.5 if higher_better else 1.0)

        for name, stats in sorted_models:
            mu = stats['main_mean']
            sigma = stats['main_std']

            # Gestion de l'affichage sp√©cifique Classification
            if task == 'classification':
                pr = stats.get('pr_mean', 0.0)
                gain = (mu - 0.5) * 100
                perf_str = f"{'+' if gain > 0 else ''}{gain:.1f} pts"

                print(f"{name:<20} | {mu:<8.4f} | {pr:<8.4f} | {perf_str:<10}")
            else:
                ratio = mu / base_val if base_val > 0 else 1.0
                print(f"{name:<20} | {mu:<8.4f} | {sigma:<8.4f} | {ratio:.2f}x")

        # === TEST STATISTIQUE (WILCOXON) ===
        if 'Gamma Residual' in means and 'Age Only' in means:
            gamma_vals = means['Gamma Residual']['main_vals']
            age_vals = means['Age Only']['main_vals']

            min_len = min(len(gamma_vals), len(age_vals))
            gamma_vals = gamma_vals[:min_len]
            age_vals = age_vals[:min_len]

            if min_len >= 5:
                try:
                    alt = 'greater' if higher_better else 'less'
                    stat, p_value = wilcoxon(gamma_vals, age_vals, alternative=alt)

                    print(f"\nüî¨ Test Wilcoxon (Gamma Residual vs Age Only):")
                    print(f"   p-value = {p_value:.4e}")
                    print(f"   {'‚úÖ SIGNIFICATIF' if p_value < 0.05 else '‚ùå NON SIGNIFICATIF'}")

                except Exception as e:
                    pass
        print("\n" + "=" * 80)
    def _plot_results(self, clf_results, reg_results):
        """Visualisation des r√©sultats."""
        fig, axes = plt.subplots(1, 2, figsize=(14, 5))

        # === PLOT 1 : Classification (Boxplot AUC) ===
        ax1 = axes[0]

        clf_data = []
        clf_labels = []
        for name, scores in clf_results.items():
            if 'auc' in scores and len(scores['auc']) >= 3:
                clf_data.append(scores['auc'])
                clf_labels.append(name)

        if clf_data:
            colors = ['#e74c3c' if 'Age' in l else '#27ae60' if 'Gamma' in l
            else '#3498db' for l in clf_labels]

            bp = ax1.boxplot(clf_data, labels=clf_labels, patch_artist=True)
            for patch, color in zip(bp['boxes'], colors):
                patch.set_facecolor(color)
                patch.set_alpha(0.7)

            ax1.axhline(0.5, color='gray', linestyle='--', label='Chance')
            ax1.set_ylabel('AUC-ROC')
            ax1.set_title('Classification: Viabilit√© Future')
            ax1.tick_params(axis='x', rotation=45)

        # === PLOT 2 : R√©gression (Boxplot RMSE) ===
        ax2 = axes[1]

        reg_data = []
        reg_labels = []
        for name, scores in reg_results.items():
            if 'rmse' in scores and len(scores['rmse']) >= 3:
                reg_data.append(scores['rmse'])
                reg_labels.append(name)

        if reg_data:
            colors = ['#e74c3c' if 'Age' in l else '#27ae60' if 'Gamma' in l
            else '#3498db' for l in reg_labels]

            bp = ax2.boxplot(reg_data, labels=reg_labels, patch_artist=True)
            for patch, color in zip(bp['boxes'], colors):
                patch.set_facecolor(color)
                patch.set_alpha(0.7)

            ax2.set_ylabel('RMSE (lower = better)')
            ax2.set_title('R√©gression: Gamma Futur')
            ax2.tick_params(axis='x', rotation=45)

        plt.tight_layout()
        plt.savefig("hindcasting_validation.png", dpi=150)
        print(f"\n‚úÖ Figure sauvegard√©e : hindcasting_validation.png")
        plt.close()



class SurvivalAsymmetryValidator:
    """
    MODULE V46.1 (FINAL LOCK) : ANALYSE DE SURVIE DESCRIPTIVE
    Approche : "Non-Contradiction Test".
    M√©thodologie :
      1. D√©finition stricte de l'√©v√©nement (pas de 'zombie pivot' artificiel).
      2. Asym√©trie cumulative (exposition temporelle).
      3. Inf√©rence conditionnelle (Cox uniquement si puissance suffisante).
    """

    def __init__(self, all_dataframes, crossover_results, project_status):
        self.dfs = all_dataframes
        self.crossover = crossover_results
        self.status_dict = project_status
        self.survival_df = None

    def prepare_data(self):
        """
        Pr√©paration du dataset (1 ligne par projet).
        Variable : Exposition cumul√©e √† l'asym√©trie causale.
        √âv√©nement : Inactivit√© totale (> 24 mois) ou statut d√©clar√© mort.
        """
        rows = []
        print("\n[Survival] Pr√©paration du dataset (Approche Conservative)...")

        # Fin de l'√©tude (date max du corpus)
        all_dates = [df.index.max() for df in self.dfs.values() if df is not None and not df.empty]
        if not all_dates: return
        study_end_date = max(all_dates)

        # Seuil d'arr√™t strict : 24 mois sans activit√©
        cutoff_threshold = pd.Timedelta(days=365 * 2)

        for name, df in self.dfs.items():
            if df is None or len(df) < 24: continue

            # 1. √âv√©nement (E) : Approche stricte (Dead or Abandoned)
            last_active_date = df.index.max()
            is_inactive = (study_end_date - last_active_date) > cutoff_threshold
            is_declared_dead = self.status_dict.get(name, 'alive') in ['dead', 'legacy']

            E = 1 if (is_inactive or is_declared_dead) else 0

            # 2. Temps (T)
            T = len(df)

            # 3. Asym√©trie Cumulative (Exposition)
            if name not in self.crossover: continue
            res = self.crossover[name]
            n_points = min(len(res['strength_ag']), len(res['strength_ga']))
            if n_points < 12: continue

            s_ag = np.array(res['strength_ag'][:n_points])
            s_ga = np.array(res['strength_ga'][:n_points])

            with np.errstate(divide='ignore', invalid='ignore'):
                inst_ratios = np.minimum(s_ag, s_ga) / (np.maximum(s_ag, s_ga) + 1e-9)

            # Seuil d'asym√©trie conservateur (0.6)
            # On mesure la fraction de la vie pass√©e en d√©s√©quilibre
            months_in_asymmetry = np.sum(inst_ratios < 0.6)
            asym_exposure = months_in_asymmetry / n_points

            # 4. Contr√¥les
            avg_activity = df['total_weight'].mean()
            size_proxy = df['files_touched'].sum()

            rows.append({
                'project': name,
                'T': T,
                'E': E,
                'Asym_Exposure': asym_exposure,
                'Log_Activity': np.log1p(avg_activity),
                'Log_Size': np.log1p(size_proxy)
            })

        self.survival_df = pd.DataFrame(rows)
        n_events = self.survival_df['E'].sum()
        print(f"   Dataset pr√™t : {len(self.survival_df)} projets.")
        print(f"   √âv√©nements terminaux (E=1) : {n_events} ({n_events / len(self.survival_df):.1%})")

        if n_events < 5:
            print("   ‚ÑπÔ∏è NOTE : Faible nombre d'√©v√©nements (High Resilience Corpus).")
            print("             L'analyse sera purement descriptive.")

    def run_kaplan_meier(self):
        """
        ANALYSE 1 : Kaplan-Meier Descriptif.
        [CORRIG√â] : Ajout de conversions de types explicites pour √©viter l'erreur
        "Values must be numeric".
        """
        if self.survival_df is None or self.survival_df.empty: return

        df = self.survival_df.copy()

        # --- FIX CRITIQUE : Conversion explicite des types ---
        # Lifelines plante si ces colonnes sont de type 'object'
        try:
            df['T'] = pd.to_numeric(df['T'], errors='coerce')
            df['E'] = pd.to_numeric(df['E'], errors='coerce')
            df['Asym_Exposure'] = pd.to_numeric(df['Asym_Exposure'], errors='coerce')

            # On supprime les lignes qui auraient des NaNs apr√®s conversion
            df = df.dropna(subset=['T', 'E', 'Asym_Exposure'])
        except Exception as e:
            print(f"   ‚ö†Ô∏è Erreur de nettoyage des donn√©es : {e}")
            return

        # Tentative de groupement robuste (Tertiles)
        try:
            # On essaie de cr√©er 3 groupes : Low, Mid, High exposure
            df['Exposure_Group'] = pd.qcut(df['Asym_Exposure'], 3, labels=["Low Asym", "Mid", "High Asym"])
            # On ne garde que les extr√™mes pour le contraste
            df_plot = df[df['Exposure_Group'].isin(["Low Asym", "High Asym"])].copy()
            method = "Tertiles (Top vs Bottom 33%)"
        except ValueError:
            # Fallback M√©diane si pas assez de variance (distribution trop plate)
            median_val = df['Asym_Exposure'].median()
            df['Exposure_Group'] = np.where(df['Asym_Exposure'] > median_val, 'High Asym', 'Low Asym')
            df_plot = df.copy()
            method = "Median Split"

        print(f"\nüìä [Survival] Kaplan-Meier ({method})")

        kmf = KaplanMeierFitter()
        plt.figure(figsize=(10, 6))

        # observed=True corrige le FutureWarning de Pandas
        for name, grouped_df in df_plot.groupby('Exposure_Group', observed=True):
            if len(grouped_df) == 0: continue

            label = f"{name} (n={len(grouped_df)})"

            try:
                kmf.fit(grouped_df['T'], grouped_df['E'], label=label)
                kmf.plot_survival_function(linewidth=2.5)
            except Exception as e:
                print(f"   ‚ö†Ô∏è Erreur lors du fit pour le groupe {name}: {e}")

        plt.title(f'Survival Trajectories by Asymmetry Exposure ({method})', fontsize=12, fontweight='bold')
        plt.xlabel('Duration (Months)')
        plt.ylabel('Survival Probability')
        plt.grid(True, alpha=0.3)
        plt.legend()
        plt.tight_layout()
        plt.savefig("omega_v46_kaplan_meier.png", dpi=300)
        print("   ‚úÖ Courbe descriptive sauvegard√©e : omega_v46_kaplan_meier.png")

        # Log-rank informatif
        try:
            g1 = df_plot[df_plot['Exposure_Group'] == 'High Asym']
            g2 = df_plot[df_plot['Exposure_Group'] == 'Low Asym']

            if len(g1) > 0 and len(g2) > 0:
                res = logrank_test(g1['T'], g2['T'], event_observed_A=g1['E'], event_observed_B=g2['E'])
                print(f"   Log-Rank p-value : {res.p_value:.4f}")
            else:
                print("   ‚ö†Ô∏è Pas assez de donn√©es dans les groupes pour le Log-Rank.")
        except Exception:
            pass

    def run_cox_model(self):
        """
        ANALYSE 2 : Cox Proportional Hazards (Conditionnel).
        Ne s'ex√©cute QUE si la puissance statistique est suffisante (n_events >= 5).
        Sinon, rapporte la limitation technique sans interpr√©tation conceptuelle.
        """
        if self.survival_df is None or self.survival_df.empty: return

        n_events = self.survival_df['E'].sum()

        # SEUIL DE S√âCURIT√â SCIENTIFIQUE
        if n_events < 5:
            print("\n‚ö†Ô∏è [Survival] Cox Model SKIPPED")
            print(f"   Reason : Insufficient events ({n_events}) for multivariate inference.")
            print("   Action : Discussion of survivor bias deferred to manuscript.")
            return

        # Ex√©cution standard si donn√©es suffisantes
        data = self.survival_df[['T', 'E', 'Asym_Exposure', 'Log_Activity', 'Log_Size']]
        print(f"\nüìä [Survival] Cox Proportional Hazards Model...")
        try:
            cph = CoxPHFitter()
            cph.fit(data, duration_col='T', event_col='E')
            cph.print_summary()
        except Exception as e:
            print(f"   ‚ö†Ô∏è Cox convergence failed: {e}")

# ==============================================================================
# 2. MOTEUR D'ANALYSE (Avec Cache)
# ==============================================================================

def run_statistical_tests(all_dataframes):
    """
    Ex√©cute les tests statistiques cl√©s pour l'article (H_2).
    """
    all_gamma_values = pd.concat(
        [df['monthly_gamma'] for df in all_dataframes.values() if not df.empty]).dropna().values

    if len(all_gamma_values) < 50:
        print("‚ùå Nombre de points Gamma insuffisant pour le test de bimodalit√©.")
        return

    # H_2 : Test de Hartigan pour la bimodalit√©
    dip_stat, p_value = diptest(all_gamma_values)

    print("\n--- TEST STATISTIQUE H_2 (Bimodalit√© - Hartigan's Dip Test) ---")
    print(f"Statistique Dip : {dip_stat:.4f}")
    print(f"P-value : {p_value:.4f}")

    if p_value < 0.05:
        print("‚úÖ H_2 Valid√©e (p < 0.05). La distribution est significativement non unimodale (bimodale).")
    else:
        print("‚ùå H_2 Non Valid√©e. La distribution est statistiquement unimodale.")
    print("----------------------------------------------------------------")

    return {'dip_stat': dip_stat, 'p_value': p_value}


# ==============================================================================
# V36 : APPROCHE HYBRIDE (Structure + Contenu) avec Horizon Fixe
# ==============================================================================

# Constantes √† ajouter en haut du fichier
SURVIVAL_HORIZON_MONTHS = 6
CODE_EXTENSIONS = {'.py', '.js', '.ts', '.jsx', '.tsx', '.c', '.h', '.cpp',
                   '.hpp', '.java', '.go', '.rs', '.rb', '.php', '.vue', '.sql',
                   '.swift', '.kt', '.scala', '.sh', '.pl', '.r', '.m'}


class OmegaV34Engine:
    def __init__(self, name, config, position=0):
        self.name = name
        self.config = config
        self.position = position
        self.monthly_authors = defaultdict(set)
        self.repo = None
        self.time_data = defaultdict(lambda: {
            'total_weight': 0,
            'sedimented_weight': 0,
            'v2_lines': 0,
            'v3_lines': 0,
            'n_contributors':0,
            'files_touched': 0,
            'files_survived': 0,
            'gamma_structure': [],  # Liste pour moyenne mensuelle
            'gamma_content': [],
            'gamma_hybrid': []
        })
        self.global_metrics = {}
        self.cache_file = os.path.join(CACHE_DIR, f"{name}_v36_analysis.pkl")

        # Cache des commits par date pour acc√©l√©rer get_commit_at_date
        self._commit_date_cache = None

        self._commit_timestamps = []
        self._commit_objects = []
    # ==========================================================================
    # NOUVELLES M√âTHODES
    # ==========================================================================


    # ...
    def get_commit_stats_pygit2(self, commit_sha):
        """R√©cup√®re fichiers modifi√©s + stats via PyGit2 (C-speed)"""
        try:
            # R√©cup√©rer l'objet commit PyGit2 via le SHA (qui vient de GitPython)
            py_commit = self.repo_pygit2[commit_sha]

            if not py_commit.parents:
                # Premier commit (pas de parent)
                diff = py_commit.tree.diff_to_tree(swap=True)
            else:
                # Diff avec le premier parent
                diff = self.repo_pygit2.diff(py_commit.parents[0], py_commit)

            # On construit le dictionnaire 'stats' attendu par ton code
            # Structure : {'chemin/fichier': {'insertions': X, 'deletions': Y}}
            files_stats = {}

            # It√©ration rapide sur les deltas (C-level)
            for patch in diff:
                path = patch.delta.new_file.path
                # patch.line_stats retourne (context, insertions, deletions)
                _, ins, dels = patch.line_stats
                files_stats[path] = {'insertions': ins, 'deletions': dels}

            return files_stats

        except Exception:
            return {}
    def _build_commit_date_cache(self):
        if self._commit_date_cache is not None: return
        print(f"[{self.name}] Indexation des dates (Bisect ready)...")

        # On stocke (timestamp, commit_object)
        # IMPORTANT: On trie du plus ANCIEN au plus R√âCENT pour bisect
        temp_cache = []
        for c in self.repo.iter_commits(self.config['branch']):
            temp_cache.append((c.committed_datetime.timestamp(), c))

        # Tri ascendant (Old -> New)
        self._commit_date_cache = sorted(temp_cache, key=lambda x: x[0])
        # On extrait juste les cl√©s (timestamps) pour le module bisect
        self._commit_timestamps = [x[0] for x in self._commit_date_cache]

    def get_commit_at_date(self, target_date):
        """Trouve le commit par dichotomie (Instantan√©)."""
        if not self._commit_timestamps:
            self._build_commit_date_cache()

        target_ts = target_date.timestamp()

        # Recherche binaire : trouve le point d'insertion
        idx = bisect.bisect_right(self._commit_timestamps, target_ts)

        if idx == 0:
            return self._commit_objects[0]
        if idx >= len(self._commit_objects):
            return self._commit_objects[-1]

        # On retourne le commit juste avant la date cible
        return self._commit_objects[idx - 1]
    def file_exists_at_ref(self, filepath, ref_commit):
        """
        V√©rifie si un fichier existe dans un commit de r√©f√©rence.

        Returns:
            bool
        """
        try:
            ref_commit.tree[filepath]
            return True
        except KeyError:
            return False

    def run_git_blame_at_ref(self, filepath, target_commit_sha, ref_commit, timeout=30):
        """
        Version PyGit2 : Blame en m√©moire C++ (Ultra-rapide).
        """
        try:
            # ref_commit est un objet GitPython, on a besoin du SHA string pour PyGit2
            ref_sha = ref_commit.hexsha

            # On utilise les 12 premiers chars pour comparer (optimisation string)
            target_short = target_commit_sha[:12]

            # Blame direct via libgit2
            # flags='w' n'est pas dispo simplement, mais la vitesse compense l'impr√©cision whitespace
            try:
                blame = self.repo_pygit2.blame(filepath, newest_commit=ref_sha)
            except KeyError:
                # Le fichier n'existe pas dans le commit de r√©f√©rence
                return 0, 0
            except ValueError:
                # Fichier binaire ou autre erreur
                return 0, 0

            lines_from_target = 0
            total_lines = 0

            for hunk in blame:
                count = hunk.lines_in_hunk
                total_lines += count

                # hunk.orig_commit_id est un objet Oid, on convertit en string
                if str(hunk.orig_commit_id).startswith(target_short):
                    lines_from_target += count

            return lines_from_target, total_lines

        except Exception as e:
            return 0, 0


    # ==========================================================================
    # M√âTHODE run_analysis() MODIFI√âE
    # ==========================================================================

    def _build_head_index(self):
        """
        Construit un index en m√©moire de l'√©tat actuel (HEAD) pour une recherche O(1).
        Stocke les chemins complets ET les noms de fichiers pour d√©tecter les d√©placements.
        """
        print(f"[{self.name}] üß† Indexation de HEAD en RAM...")
        self.head_paths = set()
        self.head_filenames = defaultdict(int)

        try:
            # On parcourt r√©cursivement l'arbre HEAD une seule fois
            for blob in self.repo.head.commit.tree.traverse():
                if blob.type == 'blob':  # Seulement les fichiers
                    path = blob.path
                    self.head_paths.add(path)
                    # On indexe aussi le nom de fichier (ex: 'http.js')
                    filename = os.path.basename(path)
                    self.head_filenames[filename] += 1
        except Exception as e:
            print(f"‚ö†Ô∏è Erreur indexation HEAD: {e}")

    def run_analysis(self):
        # 1. CHARGEMENT ET V√âRIFICATION DU CACHE
        # On v√©rifie si le cache existe ET s'il contient les donn√©es V37 (n_contributors)
        cached_df = self.load_cache()
        if cached_df is not None:
            if 'n_contributors' in cached_df.columns:
                return cached_df, self.global_metrics
            else:
                print(f"[{self.name}] ‚ö†Ô∏è Cache obsol√®te (V36 sans auteurs). Relance de l'analyse V37...")

        # 2. R√âCUP√âRATION DES COMMITS
        try:
            commits = list(self.repo.iter_commits(self.config['branch']))
        except Exception as e:
            print(f"[{self.name}] Erreur iter_commits: {e}")
            return None, None

        if not commits: return None, None

        # 3. INDEXATION DE HEAD (Optimisation O(1) pour la topologie)
        # N√©cessaire pour savoir si un fichier a surv√©cu √† la fin du projet
        self._build_head_index()

        total_analyzed = 0
        reactive_count = 0

        # Calcul de l'√¢ge du projet pour les m√©triques globales
        last_date = pd.to_datetime(commits[0].committed_datetime).tz_localize(None)
        first_date = pd.to_datetime(commits[-1].committed_datetime).tz_localize(None)
        project_age_years = (last_date - first_date).days / 365.25
        if project_age_years < 0.1: project_age_years = 0.1

        print(f"[{self.name}] ‚è≥ Analyse V37 (Rosen + Covariates) de {len(commits)} commits...")

        # ======================================================================
        # BOUCLE PRINCIPALE SUR LES COMMITS
        # ======================================================================
        for commit in tqdm(commits, desc=f"[{self.name[:10]}]", position=self.position, leave=False):
            try:
                # A. Gestion du Temps
                dt = pd.to_datetime(commit.committed_datetime).tz_localize(None)
                month_key = dt.replace(day=1, hour=0, minute=0, second=0, microsecond=0)

                # B. AJOUT V37 : Extraction de l'auteur (Covariate Control)
                if commit.author:
                    # On stocke l'email dans un set pour d√©doublonner par mois plus tard
                    self.monthly_authors[month_key].add(commit.author.email)

                # C. Analyse des Fichiers
                stats = commit.stats.files
                files = list(stats.keys())

                # Calcul du poids (Metabolic Intensity)
                w = self.get_topology_weight(files)
                if w == 0: continue

                total_analyzed += 1
                self.time_data[month_key]['total_weight'] += w

                if self.is_reactive(commit.message):
                    reactive_count += 1

                # D. LOGIQUE DE SURVIE (ROSEN / HYBRIDE)
                # On √©chantillonne pour la performance si le commit est trop gros
                check_files = files
                if len(files) > BLAME_SAMPLE_SIZE:
                    check_files = random.sample(files, BLAME_SAMPLE_SIZE)

                if check_files:
                    survived_score_sum = 0.0
                    valid_files_count = 0
                    current_v2 = 0  # Lignes maintenues
                    current_v3 = 0  # Lignes perdues (Entropie)

                    for f in check_files:
                        if f not in stats: continue

                        lines_added = stats[f]['insertions']
                        f_basename = os.path.basename(f)

                        # --- ALGORITHME DE SURVIE ---
                        # Niveau 1 : Identit√© Positionnelle (Exact)
                        if f in self.head_paths:
                            survival_factor = 1.0
                        # Niveau 2 : Identit√© Fonctionnelle (Renomm√©/D√©plac√©)
                        elif f_basename in self.head_filenames:
                            survival_factor = 0.95  # P√©nalit√© de r√©organisation
                        # Niveau 3 : Mort (Disparu)
                        else:
                            survival_factor = 0.0

                        # Accumulation
                        survived_score_sum += survival_factor
                        valid_files_count += 1

                        # Projection sur le contenu (approximation rapide du Blame)
                        current_v2 += lines_added * survival_factor
                        current_v3 += lines_added * (1.0 - survival_factor)

                    # Stockage des r√©sultats agr√©g√©s par mois
                    if valid_files_count > 0:
                        # Pour compatibilit√© graphes V34
                        ratio = survived_score_sum / valid_files_count
                        self.time_data[month_key]['sedimented_weight'] += (w * ratio)

                        # Donn√©es brutes pour V36/V37 Gamma
                        self.time_data[month_key]['files_survived'] += survived_score_sum
                        self.time_data[month_key]['files_touched'] += valid_files_count
                        self.time_data[month_key]['v2_lines'] += current_v2
                        self.time_data[month_key]['v3_lines'] += current_v3

            except Exception:
                continue

        # ======================================================================
        # POST-PROCESSING (L'ORDRE EST CRITIQUE ICI)
        # ======================================================================

        # 1. D'ABORD : Calculer le nombre de contributeurs par mois
        # On transforme le set d'emails en un nombre entier
        for mk in self.time_data:
            count = len(self.monthly_authors.get(mk, set()))
            self.time_data[mk]['n_contributors'] = count

        # 2. ENSUITE : Calculer les m√©triques globales du projet
        total_w = sum(d['total_weight'] for d in self.time_data.values())
        sed_w = sum(d['sedimented_weight'] for d in self.time_data.values())

        if total_w > 0:
            self.global_metrics = {
                'metabolic_intensity': total_w / project_age_years,
                'sedimentation_rate': sed_w / total_w,
                'reactivity_index': reactive_count / total_analyzed if total_analyzed else 0,
                'total_commits': total_analyzed
            }

        # 3. ENFIN : G√©n√©rer le DataFrame
        # get_dataframe() va utiliser 'n_contributors' qu'on vient de calculer en √©tape 1
        df = self.get_dataframe()

        # 4. Sauvegarder le nouveau cache V37
        if df is not None:
            self.save_cache(df)

        return df, self.global_metrics
    def get_dataframe(self):
        """
        V36 : G√©n√®re le DataFrame avec le Score Composite (Gamma Hybride).
        Formule : Gamma = Gamma_Structure * Gamma_Content
        """
        df = pd.DataFrame.from_dict(self.time_data, orient='index').sort_index()
        if df.empty:
            return None

        # 1. Lissages
        df['total_smooth'] = df['total_weight'].rolling(3, min_periods=1).mean()
        df['sedimented_smooth'] = df['sedimented_weight'].rolling(3, min_periods=1).mean()

        # 2. Calcul des composants bruts
        # Gamma Structure (Survie des fichiers pond√©r√©e : 1.0 si exact, 0.8 si d√©plac√©)
        df['gamma_s'] = df['files_survived'] / (df['files_touched'] + 0.001)

        # Gamma Content (Survie des lignes estim√©e)
        df['gamma_c'] = df['v2_lines'] / (df['v2_lines'] + df['v3_lines'] + 0.001)

        # 3. LE SCORE COMPOSITE (Formule "Rosen")
        # Gamma = Gamma_s * Gamma_c
        # Cela p√©nalise doublement l'instabilit√© : si on bouge le fichier ET qu'on change les lignes, le score baisse vite.
        df['monthly_gamma'] = df['gamma_s'] * df['gamma_c']

        # S√©curit√© : Si pas de lignes analys√©es, on garde la structure
        df['monthly_gamma'] = df['monthly_gamma'].fillna(df['gamma_s']).clip(lower=0)
        # 4. Normalisation Robuste (Clipper √† 99e percentile pour ignorer les imports massifs ponctuels)
        max_val_robust = df['total_smooth'].quantile(0.99)
        if max_val_robust == 0: max_val_robust = df['total_smooth'].max()

        if max_val_robust > 0:
            df['norm_total'] = (df['total_smooth'] / max_val_robust).clip(upper=1.0)
            # La courbe verte suit le Gamma Composite
            df['norm_sedimented'] = (df['monthly_gamma'] * df['norm_total']).clip(upper=1.0)
        else:
            df['norm_total'] = 0
            df['norm_sedimented'] = 0

        # Donn√©es pour la r√©gression
        df['temps_depuis_debut'] = range(len(df))
        df['y_growth_model'] = df['monthly_gamma']
        df['n_contributors'] = df.get('n_contributors', 0).replace(0, 1)
        df['intensity_per_dev'] = df['total_weight'] / df['n_contributors']
        return df
    def _build_commit_date_cache(self):
        """Indexation des commits tri√©s par date pour Bisect (O(log N))."""
        if self._commit_timestamps:
            return

        print(f"[{self.name}] Construction du cache temporel optimis√©...")
        temp_list = []

        # On r√©cup√®re tous les commits et leur timestamp
        for commit in self.repo.iter_commits(self.config['branch']):
            ts = commit.committed_datetime.timestamp()
            temp_list.append((ts, commit))

        # TRI CRUCIAL : Du plus ANCIEN au plus R√âCENT pour bisect
        temp_list.sort(key=lambda x: x[0])

        # S√©paration des listes pour le module bisect
        self._commit_timestamps = [x[0] for x in temp_list]
        self._commit_objects = [x[1] for x in temp_list]

        print(f"[{self.name}] Cache construit : {len(self._commit_timestamps)} commits index√©s")
    def load_repo(self):
        """Charge le repository Git (GitPython + PyGit2)."""
        try:
            # GitPython (pour l'API haut niveau)
            self.repo = git.Repo(self.config['path'])
            # PyGit2 (pour la performance brute du Blame)
            self.repo_pygit2 = pygit2.Repository(self.config['path'])
            return True
        except Exception as e:
            print(f"[{self.name}] ‚ùå Erreur chargement repo: {e}")
            return False

    def load_cache(self):
        """Charge les r√©sultats depuis le cache s'il existe."""
        if os.path.exists(self.cache_file):
            try:
                with open(self.cache_file, 'rb') as f:
                    cached_data = pickle.load(f)
                    self.time_data = cached_data.get('time_data', self.time_data)
                    self.global_metrics = cached_data.get('global_metrics', {})
                    print(f"[{self.name}] ‚úÖ Cache charg√©")
                    return cached_data.get('dataframe')
            except Exception as e:
                print(f"[{self.name}] ‚ö†Ô∏è Cache corrompu: {e}")
        return None

    def save_cache(self, df):
        """Sauvegarde les r√©sultats dans le cache."""
        try:
            cache_data = {
                'time_data': dict(self.time_data),
                'global_metrics': self.global_metrics,
                'dataframe': df
            }
            with open(self.cache_file, 'wb') as f:
                pickle.dump(cache_data, f)
            print(f"[{self.name}] üíæ Cache sauvegard√©")
        except Exception as e:
            print(f"[{self.name}] ‚ö†Ô∏è Erreur sauvegarde: {e}")

    def get_topology_weight(self, files):
        """Calcule le poids topologique bas√© sur core_paths/ignore_paths."""
        if not files:
            return 0
        core_paths = self.config.get('core_paths', [])
        ignore_paths = self.config.get('ignore_paths', [])
        weight = 0
        for f in files:
            if any(f.startswith(ig) for ig in ignore_paths):
                continue
            if any(f.startswith(cp) for cp in core_paths):
                weight += 2  # Core = poids double
            else:
                weight += 1
        return weight

    def is_reactive(self, message):
        """D√©termine si un commit est r√©actif (fix, bug, etc.)."""
        if not message:
            return False
        message_lower = message.lower()
        reactive_keywords = [
            'fix', 'bug', 'hotfix', 'patch', 'repair', 'resolve',
            'issue', 'error', 'crash', 'broken', 'revert',
            'typo', 'correct', 'mistake', 'wrong'
        ]
        return any(kw in message_lower for kw in reactive_keywords)


def compare_gamma_metrics(all_dataframes):
    """
    Compare les diff√©rentes m√©triques Gamma pour validation.
    Affiche les corr√©lations et g√©n√®re un graphique comparatif.
    Mise √† jour V36 pour supporter les colonnes gamma_s et gamma_c.
    """
    print("\n" + "=" * 70)
    print("COMPARAISON DES M√âTRIQUES GAMMA (V36)")
    print("=" * 70)

    all_structure = []
    all_content = []
    all_hybrid = []

    # Listes pour les m√©triques secondaires
    all_v2v3 = []
    all_files = []

    for name, df in all_dataframes.items():
        if df is None or df.empty:
            continue

        # D√âTECTION DES COLONNES (Compatibilit√© V34/V36)
        col_s = 'gamma_s' if 'gamma_s' in df.columns else 'gamma_structure_monthly'
        col_c = 'gamma_c' if 'gamma_c' in df.columns else 'gamma_content_monthly'
        # Le gamma hybride est maintenant le gamma principal 'monthly_gamma' dans la V36
        col_h = 'monthly_gamma'

        # V√©rifier que les colonnes existent
        if col_s not in df.columns or col_c not in df.columns:
            continue

        # Collecter les valeurs non-NaN
        mask = df[col_s].notna() & df[col_c].notna()

        all_structure.extend(df.loc[mask, col_s].values)
        all_content.extend(df.loc[mask, col_c].values)
        all_hybrid.extend(df.loc[mask, col_h].values)

    # Convertir en arrays
    all_structure = np.array(all_structure)
    all_content = np.array(all_content)
    all_hybrid = np.array(all_hybrid)

    if len(all_structure) == 0:
        print("‚ö†Ô∏è Pas assez de donn√©es pour la comparaison Gamma.")
        return {}

    print(f"\nPoints de donn√©es : {len(all_structure)}")

    # Statistiques descriptives
    print(f"\n{'M√©trique':<20} | {'Moyenne':<10} | {'M√©diane':<10} | {'Std':<10}")
    print("-" * 55)
    print(
        f"{'Œì_structure (s)':<20} | {np.mean(all_structure):<10.3f} | {np.median(all_structure):<10.3f} | {np.std(all_structure):<10.3f}")
    print(
        f"{'Œì_content (c)':<20} | {np.mean(all_content):<10.3f} | {np.median(all_content):<10.3f} | {np.std(all_content):<10.3f}")
    print(
        f"{'Œì_composite (Œì)':<20} | {np.mean(all_hybrid):<10.3f} | {np.median(all_hybrid):<10.3f} | {np.std(all_hybrid):<10.3f}")

    # Corr√©lations
    print(f"\n--- CORR√âLATIONS ---")
    if len(all_structure) > 1:
        corr_sc = np.corrcoef(all_structure, all_content)[0, 1]
        corr_sh = np.corrcoef(all_structure, all_hybrid)[0, 1]
        corr_ch = np.corrcoef(all_content, all_hybrid)[0, 1]

        print(f"Œì_s vs Œì_c : r = {corr_sc:.3f}")
        print(f"Œì_s vs Œì   : r = {corr_sh:.3f}")
        print(f"Œì_c vs Œì   : r = {corr_ch:.3f}")
    else:
        corr_sc, corr_sh, corr_ch = 0, 0, 0

    # Test de bimodalit√© sur chaque m√©trique
    try:
        print(f"\n--- TESTS DE BIMODALIT√â (Dip Test) ---")
        for name, data in [('Œì_structure', all_structure),
                           ('Œì_content', all_content),
                           ('Œì_composite', all_hybrid)]:
            dip_stat, p_value = diptest(data)
            verdict = "‚úÖ Bimodal" if p_value < 0.05 else "‚ùå Unimodal"
            print(f"{name:<15} : dip = {dip_stat:.4f}, p = {p_value:.4f} ‚Üí {verdict}")
    except:
        pass

    # Graphique comparatif
    try:
        fig, axes = plt.subplots(2, 3, figsize=(15, 10))

        # Histogrammes
        axes[0, 0].hist(all_structure, bins=50, color='#3498db', alpha=0.7, edgecolor='black')
        axes[0, 0].set_title('Distribution Œì_structure (s)\n(Fichiers)')
        axes[0, 0].set_xlabel('Œì_s')

        axes[0, 1].hist(all_content, bins=50, color='#e74c3c', alpha=0.7, edgecolor='black')
        axes[0, 1].set_title('Distribution Œì_content (c)\n(Lignes)')
        axes[0, 1].set_xlabel('Œì_c')

        axes[0, 2].hist(all_hybrid, bins=50, color='#27ae60', alpha=0.7, edgecolor='black')
        axes[0, 2].set_title('Distribution Œì_composite\n(Œì = s * c)')
        axes[0, 2].set_xlabel('Œì')

        # Scatter plots
        axes[1, 0].scatter(all_structure, all_content, alpha=0.3, s=10)
        axes[1, 0].plot([0, 1], [0, 1], 'r--', label='y=x')
        axes[1, 0].set_xlabel('Œì_s')
        axes[1, 0].set_ylabel('Œì_c')
        axes[1, 0].set_title(f'Structure vs Content\n(r = {corr_sc:.3f})')
        axes[1, 0].legend()

        axes[1, 1].scatter(all_structure, all_hybrid, alpha=0.3, s=10, color='green')
        axes[1, 1].plot([0, 1], [0, 1], 'r--')
        axes[1, 1].set_xlabel('Œì_s')
        axes[1, 1].set_ylabel('Œì')
        axes[1, 1].set_title(f'Structure vs Composite\n(r = {corr_sh:.3f})')

        axes[1, 2].scatter(all_content, all_hybrid, alpha=0.3, s=10, color='purple')
        axes[1, 2].plot([0, 1], [0, 1], 'r--')
        axes[1, 2].set_xlabel('Œì_c')
        axes[1, 2].set_ylabel('Œì')
        axes[1, 2].set_title(f'Content vs Composite\n(r = {corr_ch:.3f})')

        plt.tight_layout()
        plt.savefig("omega_v36_gamma_comparison.png", dpi=150)
        print(f"\n‚úÖ Graphique de comparaison sauvegard√© : omega_v36_gamma_comparison.png")
        plt.close()
    except Exception as e:
        print(f"Erreur lors du plot comparatif : {e}")



    # --- MODIFICATION DELTA 2 : Histogramme Isol√© Œì Composite ---
    try:
        fig_iso, ax_iso = plt.subplots(figsize=(8, 6))

        # Esth√©tique soign√©e
        ax_iso.hist(all_hybrid, bins=50, color='#27ae60', alpha=0.8, edgecolor='black', density=True)

        # Ajout de la courbe de densit√© (KDE) si possible
        try:
            density = stats.gaussian_kde(all_hybrid)
            xs = np.linspace(0, 1, 200)
            ax_iso.plot(xs, density(xs), 'r--', linewidth=2, label='Densit√© KDE')
        except:
            pass

        ax_iso.set_title("Distribution de l'Efficacit√© M√©tabolique (Œì Composite)", fontsize=14, fontweight='bold')
        ax_iso.set_xlabel("Œì (Structure √ó Contenu)", fontsize=12)
        ax_iso.set_ylabel("Densit√© de Probabilit√©", fontsize=12)
        ax_iso.set_xlim(0, 1.0)
        ax_iso.grid(True, alpha=0.3)
        ax_iso.legend()

        plt.tight_layout()
        plt.savefig("omega_v36_gamma_composite_isolated.png", dpi=150)
        print(f"‚úÖ Histogramme isol√© sauvegard√© : omega_v36_gamma_composite_isolated.png")
        plt.close(fig_iso)
    except Exception as e:
        print(f"Erreur lors du plot isol√© : {e}")

    return {
        'corr_structure_content': corr_sc,
        'corr_structure_hybrid': corr_sh,
        'corr_content_hybrid': corr_ch,
        'n_points': len(all_structure)
    }
def check_irreversibility(df, threshold_high=0.7, threshold_low=0.4, window=3):
    """
    TEST 1: V√©rifie si le projet r√©gresse durablement apr√®s avoir atteint la maturit√©.
    """
    if df is None or df.empty: return False, None, False, None

    # 1. Trouver le premier mois en r√©gime haut
    high_regime_indices = df[df['monthly_gamma'] >= threshold_high].index

    if len(high_regime_indices) == 0:
        return False, None, False, None  # N'a jamais atteint le haut

    first_high_date = high_regime_indices[0]

    # 2. Analyser la p√©riode APRES ce point
    subsequent_data = df[df.index > first_high_date]['monthly_gamma']

    if len(subsequent_data) < window:
        return True, first_high_date, False, None  # Pas assez de donn√©es pour r√©gresser

    # 3. Chercher une fen√™tre glissante de 'window' mois sous 'threshold_low'
    is_regressed = False
    regression_date = None

    # Rolling min : si le max d'une fen√™tre est <= 0.4, alors toute la fen√™tre est <= 0.4
    rolling_max = subsequent_data.rolling(window=window).max()

    # On cherche o√π le max de la fen√™tre est inf√©rieur au seuil bas
    regression_mask = rolling_max <= threshold_low

    if regression_mask.any():
        is_regressed = True
        regression_date = regression_mask[regression_mask].index[0]

    return True, first_high_date, is_regressed, regression_date


def calculate_pvsd(df):
    """
    TEST 4: Pattern Vall√©e -> D√©passement (PVSD).
    D√©tecte les crises et v√©rifie si le rebond d√©passe le niveau pr√©-crise.
    """
    gamma = df['monthly_gamma']
    # D√©tection crise: Chute > 0.15 vs moyenne des 3 mois pr√©c√©dents
    rolling_pre = gamma.rolling(3).mean().shift(1)
    drops = rolling_pre - gamma

    crises = drops[drops > 0.15]

    results = []

    for date, drop_val in crises.items():
        idx_loc = df.index.get_loc(date)
        if idx_loc < 3 or idx_loc > len(df) - 7: continue

        g_pre = df.iloc[idx_loc - 3:idx_loc]['monthly_gamma'].mean()
        g_vallee = df.iloc[idx_loc]['monthly_gamma']
        g_post = df.iloc[idx_loc + 1:idx_loc + 7]['monthly_gamma'].mean()

        if g_pre == 0: continue

        pvsd_ratio = (g_post - g_vallee) / g_pre
        success = g_post > g_pre
        results.append({'date': date, 'success': success, 'ratio': pvsd_ratio})

    return results


def run_complementary_tests(all_dataframes, logistic_params):
    print("\n" + "#" * 80)
    print("DEMARRAGE DES TESTS COMPL√âMENTAIRES (ONTODYNAMIQUE)")
    print("#" * 80)

    # ==========================================================================
    # TEST 1 : IRR√âVERSIBILIT√â
    # ==========================================================================
    print("\n" + "=" * 60)
    print("TEST EXPLORATOIRE : IRR√âVERSIBILIT√â (Non inclus dans hypoth√®ses)")
    print("=" * 60)
    print(f"{'Projet':<15} | {'Atteint Haut':<12} | {'Mois':<10} | {'R√©gress√©':<10} | {'Date R√©gression'}")
    print("-" * 75)

    total_mature = 0
    total_regressed = 0

    for name, df in all_dataframes.items():
        has_reached, date_reached, has_regressed, date_reg = check_irreversibility(df)

        if has_reached:
            total_mature += 1
            reached_str = "Oui"
            month_reached = str(len(df[df.index < date_reached]))  # Mois relatif
            if has_regressed:
                total_regressed += 1
                reg_str = "OUI"
                month_reg = str(len(df[df.index < date_reg]))
            else:
                reg_str = "Non"
                month_reg = "-"

            print(f"{name:<15} | {reached_str:<12} | {month_reached:<10} | {reg_str:<10} | {month_reg}")

    regression_rate = (total_regressed / total_mature * 100) if total_mature > 0 else 0
    print("-" * 75)
    print(f"üìä R√âSULTAT H_7 :")
    print(f"   Projets ayant atteint r√©gime haut : {total_mature}")
    print(f"   Projets ayant r√©gress√© : {total_regressed} ({regression_rate:.2f}%)")

    print(f"   OBSERVATION : {regression_rate:.1f}% de r√©gression")
    print(f"   NOTE : Taux > 20%, non retenu comme hypoth√®se publiable")



    # ==========================================================================
    # TEST 3 : VARIANCE PAR R√âGIME (Axiome VI)
    # ==========================================================================
    print("\n" + "=" * 60)
    print("TEST 3 : VARIANCE PAR R√âGIME (Stabilit√© Structurelle)")
    print("=" * 60)

    all_low_regime = []
    all_high_regime = []

    for df in all_dataframes.values():
        if df is None or df.empty: continue
        vals = df['monthly_gamma'].dropna()
        all_low_regime.extend(vals[vals <= 0.4].values)
        all_high_regime.extend(vals[vals >= 0.7].values)

    var_low = np.var(all_low_regime)
    var_high = np.var(all_high_regime)
    ratio_var = var_low / var_high if var_high > 0 else 0

    # Test F (approximatif via scipy.stats.f)
    df1 = len(all_low_regime) - 1
    df2 = len(all_high_regime) - 1
    p_val_var = 1 - stats.f.cdf(ratio_var, df1, df2)

    print(f"üìä R√âSULTAT :")
    print(f"   Variance R√©gime BAS  (n={len(all_low_regime)}) : {var_low:.4f}")
    print(f"   Variance R√©gime HAUT (n={len(all_high_regime)}) : {var_high:.4f}")
    print(f"   Ratio (Bas/Haut) : {ratio_var:.2f} (Cible > 1.5)")
    print(f"   P-value (F-test) : {p_val_var:.4e}")

    verdict_3 = "‚úÖ VALID√â" if ratio_var > 1.5 and p_val_var < 0.05 else "‚ùå NON VALID√â"
    print(f"   VERDICT : {verdict_3}")

    # ==========================================================================
    # TEST H_4 : TRAJECTOIRE UNIVERSELLE (Universal Reach)
    # ==========================================================================
    print("\n" + "=" * 60)
    print("TEST H_4 : TRAJECTOIRE UNIVERSELLE (High Gamma Reach)")
    print("=" * 60)

    projects_reaching_high = 0
    total_projects = 0

    for name, df in all_dataframes.items():
        if df is None or df.empty: continue
        total_projects += 1
        # On regarde si le projet a atteint la maturit√© au moins une fois
        if df['monthly_gamma'].max() >= 0.7:
            projects_reaching_high += 1

    pct_universal = (projects_reaching_high / total_projects * 100) if total_projects > 0 else 0

    print(f"üìä R√âSULTAT H_4 :")
    print(f"   Projets atteignant Œì >= 0.7 : {projects_reaching_high}/{total_projects} ({pct_universal:.1f}%)")

    verdict_H_4 = "‚úÖ VALID√â" if pct_universal >= 90 else "‚ùå NON VALID√â"
    print(f"   VERDICT : {verdict_H_4} (Cible > 90%)")
    return pct_universal
# ==============================================================================
# 3. VISUALISATIONS (Dual + Phase + Dispersion)
# ==============================================================================

def plot_bimodality_histogram(all_dfs):
    """
    VERSION ANGLAISE : Figure 1 (Corrig√©e).
    Couvre l'int√©gralit√© des r√©gimes bas (<0.4) et haut (>0.7).
    """
    try:
        plt.style.use('seaborn-v0_8-whitegrid')
        plt.rcParams['font.family'] = 'sans-serif'
    except:
        pass

    all_gamma_values = pd.concat([df['monthly_gamma'] for df in all_dfs.values() if not df.empty])
    all_gamma_values = all_gamma_values.dropna()

    if all_gamma_values.empty: return

    fig, ax = plt.subplots(figsize=(10, 6))

    # Histogramme
    # On utilise edgecolor pour bien voir les barres individuelles, notamment √† 0 et 1
    ax.hist(all_gamma_values, bins=50, density=True, color='#3498db', alpha=0.7, edgecolor='black', linewidth=0.5)

    # Titres et Labels (Anglais)
    ax.set_title("Empirical Distribution of Metabolic Efficiency ($\\Gamma$)", fontsize=14, fontweight='bold',
                 pad=15)
    ax.set_xlabel(r"Metabolic Efficiency ($\Gamma$)", fontsize=12)
    ax.set_ylabel("Probability Density", fontsize=12)

    # --- CORRECTION DES ZONES ---
    # Zone Rouge : De 0.0 √† 0.4 (Inclut le pic √† 0)
    ax.axvspan(0.0, 0.4, color='#e74c3c', alpha=0.15, label='Exploratory Regime ($\\Gamma < 0.4$)')

    # Zone de Transition (Optionnel : on peut laisser blanc ou griser l√©g√®rement)
    # ax.axvspan(0.4, 0.7, color='gray', alpha=0.05)

    # Zone Verte : De 0.7 √† 1.0 (Inclut le pic √† 1)
    ax.axvspan(0.7, 1.0, color='#2ecc71', alpha=0.15, label='Sedimented Regime ($\\Gamma > 0.7$)')

    # Ajout d'une ligne verticale pour marquer la fronti√®re nette si besoin
    ax.axvline(0.4, color='#e74c3c', linestyle=':', alpha=0.5)
    ax.axvline(0.7, color='#2ecc71', linestyle=':', alpha=0.5)

    ax.set_xlim(0, 1.0)
    ax.legend(loc='upper center', frameon=True, ncol=2)  # L√©gende en haut au centre
    plt.tight_layout()

    filename = "omega_v34_bimodality_histogram.png"
    plt.savefig(filename, dpi=300)
    print(f"‚úÖ  (Bimodality) saved (CORRECTED ZONES): {filename}")
    plt.close(fig)


def plot_dual_view(df, name, config):
    """
    VERSION ANGLAISE : Case Study (LLVM, Kubernetes, etc.)
    """
    try:
        plt.style.use('seaborn-v0_8-whitegrid')
        plt.rcParams['font.family'] = 'sans-serif'
    except:
        pass

    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 10), sharex=True, gridspec_kw={'height_ratios': [3, 1.2]})

    # --- PLOT 1 : Flux M√©tabolique ---
    ax1.fill_between(df.index, 0, df['norm_sedimented'], color='#27ae60', alpha=0.3, label='Preserved Structure')
    ax1.plot(df.index, df['norm_total'], color='#c0392b', linestyle='--', linewidth=1.5, label='Metabolic Activity')
    ax1.plot(df.index, df['norm_sedimented'], color='#2c3e50', linewidth=2.5, label='Sedimented Core')

    ax1.set_title(f"Metabolic Signature: {name}", fontsize=16, fontweight='bold')
    ax1.set_ylabel("Normalized Intensity")
    ax1.legend(loc='upper left', frameon=True)

    # --- PLOT 2 : Efficiency Gamma ---
    gamma = (df['sedimented_smooth'] / (df['total_smooth'] + 0.001)).fillna(0)
    ax2.plot(df.index, gamma, color='#34495e', linewidth=1.5)

    # Zones color√©es
    ax2.fill_between(df.index, gamma, 0.7, where=(gamma >= 0.7), interpolate=True, color='#27ae60', alpha=0.5,
                     label='High Efficiency')
    ax2.fill_between(df.index, gamma, 0.4, where=(gamma <= 0.4), interpolate=True, color='#c0392b', alpha=0.5,
                     label='Low Efficiency')

    ax2.set_ylabel(r"Efficiency Ratio ($\Gamma$)")
    ax2.set_ylim(0, 1.05)

    filename = f"omega_v34_{name.lower()}_dual.png"
    plt.savefig(filename, dpi=300)
    plt.close(fig)

def plot_phase_diagram(results):
    fig, ax = plt.subplots(figsize=(12, 10))
    ax.add_patch(patches.Rectangle((50, 0.6), 50, 0.4, fc='#e8f8f5', alpha=0.5))
    ax.add_patch(patches.Rectangle((50, 0.0), 50, 0.6, fc='#fdedec', alpha=0.5))
    ax.add_patch(patches.Rectangle((0, 0.6), 50, 0.4, fc='#f4f6f7', alpha=0.5))
    ax.text(98, 0.98, "MODE 4", ha='right', va='top', color='#16a085', fontweight='bold')
    ax.text(98, 0.02, "MODE 1", ha='right', va='bottom', color='#c0392b', fontweight='bold')
    ax.text(2, 0.98, "MODE 2", ha='left', va='top', color='#7f8c8d', fontweight='bold')

    max_int = max([m['metabolic_intensity'] for m in results.values()]) if results else 1

    for name, metrics in results.items():
        x = (metrics['metabolic_intensity'] / max_int) * 90 + 5
        y = metrics['sedimentation_rate']
        s = (1 - metrics['reactivity_index']) * 800 + 200
        ax.scatter(x, y, s=s, c=REPOS_CONFIG[name]['color'], alpha=0.8, edgecolors='k', label=name)
        ax.annotate(name, (x, y), xytext=(0, 0), textcoords='offset points', ha='center', va='center',
                    fontweight='bold', fontsize=9)

    ax.set_xlim(0, 100)
    ax.set_ylim(0, 1.05)
    ax.set_title("Diagramme de Phase Global (Moyennes)")
    plt.savefig("omega_v34_global_phase.png", dpi=150)
    # plt.show() # Comment√© pour √©viter l'affichage si le script est lanc√© sans interface


def plot_dispersion_cloud(all_dfs):
    fig, ax = plt.subplots(figsize=(14, 10))
    ax.axhspan(0.7, 1.0, color='#27ae60', alpha=0.05)
    ax.axhspan(0.0, 0.4, color='#c0392b', alpha=0.05)

    print("\n--- G√âN√âRATION DU NUAGE DE DISPERSION ---")

    max_monthly_intensity = 0
    for df in all_dfs.values():
        if not df.empty:
            max_monthly_intensity = max(max_monthly_intensity, df['total_weight'].max())

    if max_monthly_intensity == 0: max_monthly_intensity = 1

    for name, df in all_dfs.items():
        if df.empty: continue

        active_months = df[df['total_weight'] > 5]
        x = (active_months['total_weight'] / max_monthly_intensity) * 100
        y = active_months['monthly_gamma']
        color = REPOS_CONFIG[name]['color']

        ax.scatter(x, y, c=color, s=30, alpha=0.3, label=name, edgecolors='none')

        mean_x = x.mean()
        mean_y = y.mean()
        ax.scatter(mean_x, mean_y, c=color, s=200, marker='X', edgecolors='black', linewidth=1.5)
        ax.annotate(name, (mean_x, mean_y), xytext=(0, 10), textcoords='offset points', ha='center', fontsize=8,
                    fontweight='bold')

    ax.set_xlim(0, 100)
    ax.set_ylim(0, 1.05)
    ax.set_xlabel("Intensit√© Mensuelle (Impact Structurel)")
    ax.set_ylabel("Taux de S√©dimentation Mensuel (Œì)")
    ax.set_title("NUAGE DE DISPERSION ONTOLOGIQUE (V34)\n(Chaque point est un mois de vie du projet)")

    handles, labels = ax.get_legend_handles_labels()
    by_label = dict(zip(labels, handles))
    ax.legend(by_label.values(), by_label.keys(), loc='center left', bbox_to_anchor=(1, 0.5))

    plt.tight_layout()
    plt.savefig("omega_v34_dispersion_cloud.png", dpi=150)
    print("‚úÖ Nuage de Dispersion sauvegard√© : omega_v34_dispersion_cloud.png")
    # plt.show() # Comment√©


# ==============================================================================
# 4. MOD√àLES DE CROISSANCE
# ==============================================================================

# Fonction Logistique (S-Curve)
def logistic_model(t, L, k, t0):
    """L: asymptote max, k: taux de croissance, t0: point d'inflexion."""
    return L / (1 + np.exp(-k * (t - t0)))

def simple_logistic_model(t, k, t0):
    """L=1 / (1 + exp(-k * (t - t0)))"""
    # L'asymptote L est fix√©e √† 1
    return 1.0 / (1 + np.exp(-k * (t - t0)))

# Fonction Gompertz
def gompertz_model(t, L, k, t0):
    """L: asymptote max, k: taux de d√©croissance, t0: d√©calage temporel."""
    return L * np.exp(-np.exp(-k * (t - t0)))


# Fonction Richards (Inclut la Logistique et Gompertz comme cas sp√©ciaux)
def richards_model(t, L, k, t0, v):
    """L: asymptote max, k: taux, t0: inflexion, v: param√®tre de forme."""
    # S'assurer que les valeurs v sont valides pour la puissance
    if v <= 0: return np.full_like(t, np.nan)
    return L / (1 + v * np.exp(-k * (t - t0))) ** (1 / v)


def calculate_aic_bic(y_true, y_pred, k):
    """
    Calcule l'AIC et le BIC √† partir des vraies valeurs (y_true),
    des pr√©dictions (y_pred), et du nombre de param√®tres (k).
    """
    n = len(y_true)
    if n == 0:
        return np.nan, np.nan

    # Somme des carr√©s des erreurs (SSE)
    sse = np.sum((y_true - y_pred) ** 2)

    # Estimation de la variance de l'erreur (Mean Squared Error, MSE)
    mse = sse / n
    if mse == 0:
        # Cas parfait, mais num√©riquement instable pour log-likelihood.
        return 1e10, 1e10

        # Log-Likelihood (Approximation bas√©e sur les moindres carr√©s)
    log_likelihood = -n / 2 * np.log(2 * np.pi * mse) - n / 2

    # AIC = 2k - 2*ln(L)
    aic = 2 * k - 2 * log_likelihood

    # BIC = k*ln(n) - 2*ln(L)
    bic = k * np.log(n) - 2 * log_likelihood

    return aic, bic

MODELS = {
    # Mod√®le Logistique Simplifi√© (L fix√© √† 1) - k=2 param√®tres ajustables
    'Logistique-L1': {'func': simple_logistic_model, 'p0': [0.1, 10], 'bounds': ([0.001, 0], [1.0, 100])},
        # L: 0.9 -> 1.0
        # Bornes L: [0.5, 1.2]
        'Logistique': {'func': logistic_model, 'p0': [1.0, 0.1, 10], 'bounds': ([0.5, 0.001, 0], [1.2, 1.0, 100])},
        'Gompertz':   {'func': gompertz_model, 'p0': [1.0, 0.1, 10], 'bounds': ([0.5, 0.001, 0], [1.2, 1.0, 100])},
        # Richards doit avoir une borne sup√©rieure pour 'v' plus l√¢che
        'Richards':   {'func': richards_model, 'p0': [1.0, 0.1, 10, 0.5], 'bounds': ([0.5, 0.001, 0, 0.001], [1.2, 1.0, 100, 50])}
        # v=50 est plus l√¢che que v=10 pour le param√®tre de forme
}


def sliding_holdout_regression(df, train_window=36, hold_out=1):
    """
    Ex√©cute une r√©gression √† fen√™tre glissante et calcule le R¬≤ en hold-out.

    Args:
        df: DataFrame avec 'temps_depuis_debut' (X) et 'y_growth_model' (Y).
        train_window: Taille de la fen√™tre d'entra√Ænement (mois).
        hold_out: Nombre de mois dans la fen√™tre de test (d√©faut: 1 mois).
    """

    T = df['temps_depuis_debut'].values
    Y = df['y_growth_model'].values
    N = len(T)

    if N < train_window + hold_out:
        return {'error': f"Donn√©es insuffisantes (N={N}, besoin de {train_window + hold_out})", 'R2_scores': {}}

    r2_scores = defaultdict(list)

    # Boucle de Hold-out Glissant
    for start in range(N - train_window - hold_out + 1):

        end_train = start + train_window
        end_test = end_train + hold_out

        T_train = T[start:end_train]
        Y_train = Y[start:end_train]
        T_test = T[end_train:end_test]
        Y_test = Y[end_train:end_test]

        for name, model_info in MODELS.items():
            func = model_info['func']
            p0 = model_info['p0']
            bounds = model_info['bounds']

            try:
                # Ajustement (Fit)
                popt, pcov = curve_fit(func, T_train, Y_train, p0=p0, bounds=bounds, maxfev=5000)

                # Pr√©diction (Predict)
                Y_pred = func(T_test, *popt)

                # Calcul R¬≤ pour la fen√™tre hold-out
                r2 = r2_score(Y_test, Y_pred)
                r2_scores[name].append(r2)

            except RuntimeError:
                r2_scores[name].append(np.nan)
            except Exception:
                r2_scores[name].append(np.nan)

    # Calcul des R¬≤ moyens et d√©termination du meilleur mod√®le
    mean_r2 = {}
    best_model_name = "N/A"
    best_r2_overall = -np.inf

    for name, scores in r2_scores.items():
        valid_scores = [s for s in scores if not np.isnan(s)]
        if valid_scores:
            mean_r2[name] = np.mean(valid_scores)
            if mean_r2[name] > best_r2_overall:
                best_r2_overall = mean_r2[name]
                best_model_name = name
        else:
            mean_r2[name] = np.nan

    return {
        'R2_scores': mean_r2,
        'best_model': best_model_name
    }


# ==============================================================================
# TEST A : TEMPS DE R√âSIDENCE (PREUVE DES ATTRACTEURS)
# ==============================================================================

from scipy.stats import gaussian_kde
from sklearn.mixture import GaussianMixture





def test_gmm_bimodality(data):
    """
    Test GMM : Compare un mod√®le √† 1 composante vs 2 composantes.

    Si BIC(2) < BIC(1), la distribution est mieux d√©crite par 2 modes.
    """
    data_reshaped = data.reshape(-1, 1)

    # Fit GMM 1 composante
    gmm1 = GaussianMixture(n_components=1, random_state=42)
    gmm1.fit(data_reshaped)
    bic1 = gmm1.bic(data_reshaped)

    # Fit GMM 2 composantes
    gmm2 = GaussianMixture(n_components=2, random_state=42)
    gmm2.fit(data_reshaped)
    bic2 = gmm2.bic(data_reshaped)

    # Fit GMM 3 composantes (pour comparaison)
    gmm3 = GaussianMixture(n_components=3, random_state=42)
    gmm3.fit(data_reshaped)
    bic3 = gmm3.bic(data_reshaped)

    delta_bic = bic1 - bic2  # Positif si 2 composantes pr√©f√©r√©es

    # Extraire les centres des 2 gaussiennes
    means = gmm2.means_.flatten()
    stds = np.sqrt(gmm2.covariances_.flatten())
    weights = gmm2.weights_

    print(f"\n--- TEST GMM (Gaussian Mixture Model) ---")
    print(f"BIC (1 composante) : {bic1:.1f}")
    print(f"BIC (2 composantes) : {bic2:.1f}")
    print(f"BIC (3 composantes) : {bic3:.1f}")
    print(f"ŒîBIC (1 vs 2) : {delta_bic:.1f} {'(2 pr√©f√©r√©)' if delta_bic > 10 else '(pas de pr√©f√©rence claire)'}")

    if delta_bic > 10:
        print(f"\n   Centres des 2 modes d√©tect√©s :")
        for i in range(2):
            print(f"   Mode {i + 1} : Œº = {means[i]:.3f}, œÉ = {stds[i]:.3f}, poids = {weights[i]:.2f}")

    return {
        'bic1': bic1, 'bic2': bic2, 'bic3': bic3,
        'delta_bic': delta_bic,
        'prefers_2': delta_bic > 10,  # R√®gle standard : ŒîBIC > 10 = forte pr√©f√©rence
        'means': means, 'stds': stds, 'weights': weights
    }


def calculate_traverse_times(all_dataframes):
    """
    Calcule le temps de travers√©e de la zone de transition (0.4 - 0.7).

    Un attracteur implique : travers√©e RAPIDE de la zone interm√©diaire.
    """
    ZONE_TRANS = (0.4, 0.7)

    traverse_durations = []

    for name, df in all_dataframes.items():
        if df is None or df.empty:
            continue

        gamma = df['monthly_gamma'].values
        in_transition = (gamma >= ZONE_TRANS[0]) & (gamma < ZONE_TRANS[1])

        # Identifier les s√©quences cons√©cutives dans la zone de transition
        current_duration = 0
        for is_trans in in_transition:
            if is_trans:
                current_duration += 1
            else:
                if current_duration > 0:
                    traverse_durations.append(current_duration)
                current_duration = 0

        # Capturer la derni√®re s√©quence si elle se termine dans la zone
        if current_duration > 0:
            traverse_durations.append(current_duration)

    if not traverse_durations:
        return None

    traverse_durations = np.array(traverse_durations)

    print(f"\n--- TEMPS DE TRAVERS√âE DE LA ZONE DE TRANSITION ---")
    print(f"Nombre de travers√©es d√©tect√©es : {len(traverse_durations)}")
    print(f"Dur√©e m√©diane : {np.median(traverse_durations):.1f} mois")
    print(f"Dur√©e moyenne : {np.mean(traverse_durations):.1f} mois")
    print(f"Dur√©e max : {np.max(traverse_durations)} mois")
    print(
        f"Travers√©es < 3 mois : {np.sum(traverse_durations < 3)} ({np.sum(traverse_durations < 3) / len(traverse_durations) * 100:.1f}%)")

    return {
        'n_traverses': len(traverse_durations),
        'median_traverse': np.median(traverse_durations),
        'mean_traverse': np.mean(traverse_durations),
        'max_traverse': np.max(traverse_durations),
        'pct_fast': np.sum(traverse_durations < 3) / len(traverse_durations) * 100
    }




# ==============================================================================
# FONCTION WRAPPER POUR LE MAIN
# ==============================================================================

def run_residence_time_test(all_dataframes):
    """
    Fonction principale √† appeler dans le main.
    Lance l'analyse compl√®te du temps de r√©sidence.
    """
    # 1. Analyse statistique
    results = analyze_residence_time(all_dataframes)

    # 2. Visualisation
    if results:
        plot_residence_time(all_dataframes, results)

    return results


def analyze_residence_time(all_dataframes):
    """
    TEST A : Analyse du Temps de R√©sidence
    V38: Pond√©ration par projet pour √©viter la pseudo-r√©plication.
    """
    print("\n" + "=" * 70)
    print("TEST A : TEMPS DE R√âSIDENCE (PREUVE DES ATTRACTEURS)")
    print("=" * 70)

    # === CORRECTION V38 : √âchantillonnage √©quilibr√© par projet ===
    # Probl√®me : Linux (300 mois) domine FastAPI (30 mois) dans le pooling na√Øf
    # Solution : Prendre N points al√©atoires par projet (pond√©ration implicite)

    SAMPLES_PER_PROJECT = 50  # Nombre max de mois par projet

    all_gamma = []
    project_contributions = {}

    for name, df in all_dataframes.items():
        if df is None or df.empty:
            continue

        gamma_values = df['monthly_gamma'].dropna().values

        if len(gamma_values) == 0:
            continue

        # Sous-√©chantillonnage si le projet est trop long
        if len(gamma_values) > SAMPLES_PER_PROJECT:
            # √âchantillonnage stratifi√© : on garde les extr√™mes + random au milieu
            indices = np.linspace(0, len(gamma_values) - 1, SAMPLES_PER_PROJECT, dtype=int)
            gamma_sampled = gamma_values[indices]
        else:
            gamma_sampled = gamma_values

        all_gamma.extend(gamma_sampled)
        project_contributions[name] = len(gamma_sampled)

    all_gamma = np.array(all_gamma)
    n_total = len(all_gamma)
    n_projects = len(project_contributions)

    if n_total < 100:
        print(f"‚ùå Donn√©es insuffisantes : {n_total} points (minimum 100)")
        return None

    print(f"\nüìä Donn√©es : {n_total} points de {n_projects} projets (max {SAMPLES_PER_PROJECT}/projet)")

    # 2. D√©finir les zones
    ZONE_BAS = (0.0, 0.4)  # R√©gime Exploration
    ZONE_TRANS = (0.4, 0.7)  # Zone de Transition
    ZONE_HAUT = (0.7, 1.0)  # R√©gime S√©dimentation

    # 3. Compter le temps dans chaque zone
    n_bas = np.sum((all_gamma >= ZONE_BAS[0]) & (all_gamma < ZONE_BAS[1]))
    n_trans = np.sum((all_gamma >= ZONE_TRANS[0]) & (all_gamma < ZONE_TRANS[1]))
    n_haut = np.sum((all_gamma >= ZONE_HAUT[0]) & (all_gamma <= ZONE_HAUT[1]))

    pct_bas = n_bas / n_total * 100
    pct_trans = n_trans / n_total * 100
    pct_haut = n_haut / n_total * 100

    # 4. Calculer la densit√© normalis√©e par largeur de zone
    width_bas = ZONE_BAS[1] - ZONE_BAS[0]  # 0.4
    width_trans = ZONE_TRANS[1] - ZONE_TRANS[0]  # 0.3
    width_haut = ZONE_HAUT[1] - ZONE_HAUT[0]  # 0.3

    density_bas = pct_bas / width_bas
    density_trans = pct_trans / width_trans
    density_haut = pct_haut / width_haut

    print(f"\n--- TEMPS DE R√âSIDENCE PAR ZONE ---")
    print(f"{'Zone':<20} | {'Mois':<8} | {'%':<8} | {'Densit√©':<10}")
    print("-" * 55)
    print(f"{'R√©gime BAS (0-0.4)':<20} | {n_bas:<8} | {pct_bas:<8.1f} | {density_bas:<10.1f}")
    print(f"{'TRANSITION (0.4-0.7)':<20} | {n_trans:<8} | {pct_trans:<8.1f} | {density_trans:<10.1f}")
    print(f"{'R√©gime HAUT (0.7-1)':<20} | {n_haut:<8} | {pct_haut:<8.1f} | {density_haut:<10.1f}")

    # 5. Test du Creux (preuve des attracteurs sym√©triques)
    creux_ratio = density_trans / min(density_bas, density_haut)
    has_creux = creux_ratio < 0.7

    print(f"\n--- TEST DU CREUX (Bimodalit√© Sym√©trique) ---")
    print(f"Ratio Densit√© Transition / Min(Bas,Haut) : {creux_ratio:.3f}")
    print(f"Creux significatif (< 0.7) : {'OUI' if has_creux else 'NON'}")

    # 6. Test GMM : 1 vs 2 composantes
    gmm_result = test_gmm_bimodality(all_gamma)

    # 7. NOUVEAU : Test d'Asym√©trie (Crit√®re 1-bis)
    asymmetry_result = test_asymmetry(gmm_result)

    # 8. Calcul du temps de travers√©e de la zone de transition
    traverse_times = calculate_traverse_times(all_dataframes)

    # 9. Verdict final
    print(f"\n" + "=" * 55)
    print("VERDICT TEMPS DE R√âSIDENCE")
    print("=" * 55)

    evidence_score = 0

    # Crit√®re 1 : Creux significatif OU Asym√©trie confirm√©e
    if has_creux:
        evidence_score += 1
        print("‚úÖ Crit√®re 1a : Creux dans la zone de transition (bimodalit√© sym√©trique)")
    elif asymmetry_result['has_asymmetry']:
        evidence_score += 1
        print(f"‚úÖ Crit√®re 1b : Asym√©trie confirm√©e (ratio œÉ = {asymmetry_result['concentration_ratio']:.2f})")
        print(
            f"   ‚Üí Mode HAUT concentr√© (œÉ={asymmetry_result['sigma_haut']:.3f}), Mode BAS diffus (œÉ={asymmetry_result['sigma_bas']:.3f})")
    else:
        print("‚ùå Crit√®re 1 : Ni creux ni asym√©trie d√©tect√©s")

    # Crit√®re 2 : GMM pr√©f√®re 2 composantes
    if gmm_result['prefers_2']:
        evidence_score += 1
        print(f"‚úÖ Crit√®re 2 : GMM pr√©f√®re 2 composantes (ŒîBIC = {gmm_result['delta_bic']:.1f})")
    else:
        print(f"‚ùå Crit√®re 2 : GMM pr√©f√®re 1 composante (ŒîBIC = {gmm_result['delta_bic']:.1f})")

    # Crit√®re 3 : Dip test significatif
    dip_stat, dip_pval = diptest(all_gamma)
    if dip_pval < 0.05:
        evidence_score += 1
        print(f"‚úÖ Crit√®re 3 : Dip test significatif (p = {dip_pval:.4f})")
    else:
        print(f"‚ùå Crit√®re 3 : Dip test non significatif (p = {dip_pval:.4f})")

    # Crit√®re 4 : Travers√©e rapide
    if traverse_times and traverse_times['median_traverse'] < 6:
        evidence_score += 1
        print(f"‚úÖ Crit√®re 4 : Travers√©e rapide (m√©diane = {traverse_times['median_traverse']:.1f} mois)")
    elif traverse_times:
        print(f"‚ö†Ô∏è  Crit√®re 4 : Travers√©e lente (m√©diane = {traverse_times['median_traverse']:.1f} mois)")

    print(f"\nüéØ SCORE FINAL : {evidence_score}/4")

    if evidence_score >= 3:
        print("‚úÖ ATTRACTEURS CONFIRM√âS : Les modes sont des √©tats stables distincts")
        if asymmetry_result['has_asymmetry'] and not has_creux:
            print("   ‚Üí Structure ASYM√âTRIQUE : Un attracteur stable (haut) + bassin exploratoire (bas)")
    elif evidence_score >= 2:
        print("‚ö†Ô∏è  ATTRACTEURS PROBABLES : Indices convergents mais pas d√©finitifs")
    else:
        print("‚ùå ATTRACTEURS NON CONFIRM√âS : Distribution possiblement continue")

    return {
        'n_total': n_total,
        'n_bas': n_bas, 'n_trans': n_trans, 'n_haut': n_haut,
        'pct_bas': pct_bas, 'pct_trans': pct_trans, 'pct_haut': pct_haut,
        'density_bas': density_bas, 'density_trans': density_trans, 'density_haut': density_haut,
        'creux_ratio': creux_ratio, 'has_creux': has_creux,
        'gmm_result': gmm_result,
        'asymmetry_result': asymmetry_result,
        'dip_stat': dip_stat, 'dip_pval': dip_pval,
        'traverse_times': traverse_times,
        'evidence_score': evidence_score
    }


def test_asymmetry(gmm_result):
    """
    TEST D'ASYM√âTRIE DES ATTRACTEURS

    V√©rifie si la bimodalit√© est asym√©trique :
    - Un mode concentr√© (attracteur stable, œÉ faible)
    - Un mode diffus (bassin exploratoire, œÉ √©lev√©)

    Pr√©diction Ontodynamique :
    - Mode 4 (Authentique) = attracteur √©troit
    - Modes 1-3 = r√©gime diffus
    """
    print(f"\n--- TEST D'ASYM√âTRIE (Bimodalit√© Asym√©trique) ---")

    if not gmm_result['prefers_2']:
        print("   ‚Üí GMM ne pr√©f√®re pas 2 composantes, test non applicable")
        return {
            'has_asymmetry': False,
            'concentration_ratio': np.nan,
            'sigma_haut': np.nan,
            'sigma_bas': np.nan,
            'interpretation': 'N/A'
        }

    means = gmm_result['means']
    stds = gmm_result['stds']
    weights = gmm_result['weights']

    # Identifier le mode haut (Œº > 0.6) et le mode bas (Œº < 0.6)
    if means[0] > means[1]:
        idx_haut, idx_bas = 0, 1
    else:
        idx_haut, idx_bas = 1, 0

    mu_haut, sigma_haut = means[idx_haut], stds[idx_haut]
    mu_bas, sigma_bas = means[idx_bas], stds[idx_bas]

    # Ratio de concentration : œÉ_bas / œÉ_haut
    # Si > 2 : le mode haut est significativement plus concentr√©
    concentration_ratio = sigma_bas / sigma_haut if sigma_haut > 0 else np.nan

    # Crit√®re : ratio > 2 ET mode haut au-dessus de 0.7
    has_asymmetry = (concentration_ratio > 2.0) and (mu_haut > 0.7)

    print(f"Mode HAUT : Œº = {mu_haut:.3f}, œÉ = {sigma_haut:.3f}")
    print(f"Mode BAS  : Œº = {mu_bas:.3f}, œÉ = {sigma_bas:.3f}")
    print(f"Ratio de concentration (œÉ_bas / œÉ_haut) : {concentration_ratio:.2f}")
    print(f"Asym√©trie significative (ratio > 2 ET Œº_haut > 0.7) : {'OUI' if has_asymmetry else 'NON'}")

    # Interpr√©tation ontodynamique
    if has_asymmetry:
        interpretation = "Attracteur unique en r√©gime haut + bassin exploratoire diffus"
        print(f"\n   üìê INTERPR√âTATION ONTODYNAMIQUE :")
        print(f"   Le Mode 4 (Authentique) est un ATTRACTEUR STABLE (œÉ={sigma_haut:.3f})")
        print(f"   Les Modes 1-3 forment un BASSIN EXPLORATOIRE diffus (œÉ={sigma_bas:.3f})")
        print(f"   ‚Üí Ceci confirme la transition UNIDIRECTIONNELLE vers la maturit√©")
    else:
        interpretation = "Structure non asym√©trique ou modes non s√©par√©s"

    return {
        'has_asymmetry': has_asymmetry,
        'concentration_ratio': concentration_ratio,
        'mu_haut': mu_haut,
        'mu_bas': mu_bas,
        'sigma_haut': sigma_haut,
        'sigma_bas': sigma_bas,
        'interpretation': interpretation
    }


def plot_residence_time(all_dataframes, residence_results):
    """
    Visualisation avanc√©e du Temps de R√©sidence avec :
    - Histogramme des valeurs de Œì
    - Overlay des 2 gaussiennes GMM
    - Zones color√©es (bas, transition, haut)
    - Annotations statistiques
    - NOUVEAU : Visualisation de l'asym√©trie
    """
    # Collecter les donn√©es
    all_gamma = []
    for df in all_dataframes.values():
        if df is not None and not df.empty:
            all_gamma.extend(df['monthly_gamma'].dropna().values)
    all_gamma = np.array(all_gamma)

    if len(all_gamma) < 50:
        return

    fig, axes = plt.subplots(2, 2, figsize=(14, 10))

    # ===== PLOT 1 : Histogramme avec zones et GMM =====
    ax1 = axes[0, 0]

    # Zones color√©es
    ax1.axvspan(0.0, 0.4, color='#e74c3c', alpha=0.15, label='Regime BAS')
    ax1.axvspan(0.4, 0.7, color='#f39c12', alpha=0.15, label='TRANSITION')
    ax1.axvspan(0.7, 1.0, color='#27ae60', alpha=0.15, label='Regime HAUT')

    # Histogramme
    counts, bins, patches = ax1.hist(all_gamma, bins=50, density=True,
                                     color='#3498db', alpha=0.7, edgecolor='black', linewidth=0.5)

    # Overlay GMM si disponible
    if residence_results and 'gmm_result' in residence_results:
        gmm = residence_results['gmm_result']
        if gmm['prefers_2']:
            x_range = np.linspace(0, 1, 200)
            for i in range(2):
                mu, sigma, weight = gmm['means'][i], gmm['stds'][i], gmm['weights'][i]
                gaussian = weight * stats.norm.pdf(x_range, mu, sigma)
                style = '-' if mu > 0.6 else '--'
                label = f'Mode {"HAUT" if mu > 0.6 else "BAS"}: mu={mu:.2f}, sigma={sigma:.3f}'
                ax1.plot(x_range, gaussian, style, linewidth=2.5, label=label)

    ax1.set_xlim(0, 1)
    ax1.set_xlabel('Efficacite Metabolique (Gamma)', fontsize=11)
    ax1.set_ylabel('Densite', fontsize=11)
    ax1.set_title('Distribution du Temps de Residence\n(Chaque point = 1 mois de vie)', fontsize=12, fontweight='bold')
    ax1.legend(loc='upper left', fontsize=8)

    # ===== PLOT 2 : Visualisation de l'Asym√©trie =====
    ax2 = axes[0, 1]

    if residence_results and 'asymmetry_result' in residence_results:
        asym = residence_results['asymmetry_result']

        if not np.isnan(asym.get('concentration_ratio', np.nan)):
            # Barres comparant les √©carts-types
            categories = ['Mode BAS\n(Exploratoire)', 'Mode HAUT\n(Attracteur)']
            sigmas = [asym['sigma_bas'], asym['sigma_haut']]
            colors = ['#e74c3c', '#27ae60']

            bars = ax2.bar(categories, sigmas, color=colors, edgecolor='black', alpha=0.8)

            # Ligne de ratio
            ax2.axhline(y=asym['sigma_haut'] * 2, color='gray', linestyle='--',
                        label=f'Seuil asymetrie (2x sigma_haut)')

            ax2.set_ylabel('Ecart-type (sigma)', fontsize=11)
            ax2.set_title(f'Asymetrie des Modes\nRatio = {asym["concentration_ratio"]:.2f}',
                          fontsize=12, fontweight='bold')
            ax2.legend(loc='upper right')

            # Annotation du ratio
            max_sigma = max(sigmas)
            ax2.annotate(f'Ratio: {asym["concentration_ratio"]:.1f}x',
                         xy=(0.5, max_sigma * 0.9),
                         ha='center', fontsize=14, fontweight='bold',
                         color='#8e44ad')

            # Colorer selon le verdict
            if asym['has_asymmetry']:
                ax2.set_facecolor('#e8f8f5')
        else:
            ax2.text(0.5, 0.5, 'Asymetrie non calculable\n(GMM ne prefere pas 2 modes)',
                     ha='center', va='center', fontsize=12, transform=ax2.transAxes)
            ax2.set_facecolor('#fdedec')

    # ===== PLOT 3 : Temps de travers√©e =====
    ax3 = axes[1, 0]

    if residence_results and residence_results.get('traverse_times'):
        ZONE_TRANS = (0.4, 0.7)
        traverse_durations = []
        for df in all_dataframes.values():
            if df is None or df.empty:
                continue
            gamma = df['monthly_gamma'].values
            in_transition = (gamma >= ZONE_TRANS[0]) & (gamma < ZONE_TRANS[1])
            current_duration = 0
            for is_trans in in_transition:
                if is_trans:
                    current_duration += 1
                else:
                    if current_duration > 0:
                        traverse_durations.append(current_duration)
                    current_duration = 0
            if current_duration > 0:
                traverse_durations.append(current_duration)

        if traverse_durations:
            # Limiter l'affichage aux travers√©es < 20 mois pour lisibilit√©
            traverse_display = [t for t in traverse_durations if t <= 20]
            ax3.hist(traverse_display, bins=range(1, 22),
                     color='#f39c12', edgecolor='black', alpha=0.8)
            ax3.axvline(x=np.median(traverse_durations), color='red', linestyle='--',
                        linewidth=2, label=f'Mediane: {np.median(traverse_durations):.1f} mois')
            ax3.axvline(x=6, color='green', linestyle=':',
                        linewidth=2, label='Seuil rapide (6 mois)')
            ax3.set_xlabel('Duree dans la zone de transition (mois)', fontsize=11)
            ax3.set_ylabel('Frequence', fontsize=11)
            ax3.set_title('Distribution des Temps de Traversee\n(Court = attracteurs forts)', fontsize=12,
                          fontweight='bold')
            ax3.legend()

            # Annotation pourcentage rapide
            pct_fast = np.sum(np.array(traverse_durations) < 3) / len(traverse_durations) * 100
            ax3.annotate(f'{pct_fast:.0f}% < 3 mois', xy=(0.7, 0.85), xycoords='axes fraction',
                         fontsize=11, fontweight='bold', color='#27ae60')

    # ===== PLOT 4 : R√©sum√© des crit√®res (SANS EMOJIS) =====
    ax4 = axes[1, 1]
    ax4.axis('off')

    if residence_results:
        # D√©terminer le type de structure
        has_creux = residence_results['has_creux']
        has_asym = residence_results.get('asymmetry_result', {}).get('has_asymmetry', False)

        if has_creux:
            structure_type = "BIMODALE SYMETRIQUE"
        elif has_asym:
            structure_type = "BIMODALE ASYMETRIQUE"
        else:
            structure_type = "INDETERMINEE"

        # Construire le texte sans emojis
        check = "[OK]"
        cross = "[X]"

        c1_status = check if (has_creux or has_asym) else cross
        c2_status = check if residence_results['gmm_result']['prefers_2'] else cross
        c3_status = check if residence_results['dip_pval'] < 0.05 else cross
        c4_status = check if residence_results['traverse_times']['median_traverse'] < 6 else "[?]"

        summary_text = f"""
RESUME DU TEST DES ATTRACTEURS
{'=' * 45}

Donnees analysees : {residence_results['n_total']} mois-projets

Structure detectee : {structure_type}

{c1_status} Critere 1 - Creux OU Asymetrie :
   Creux: {residence_results['creux_ratio']:.3f} (cible < 0.7)
   Asymetrie: {residence_results.get('asymmetry_result', {}).get('concentration_ratio', 0):.2f}x (cible > 2.0)

{c2_status} Critere 2 - GMM prefere 2 modes :
   Delta-BIC = {residence_results['gmm_result']['delta_bic']:.1f} (cible > 10)

{c3_status} Critere 3 - Dip test significatif :
   p-value = {residence_results['dip_pval']:.4f} (cible < 0.05)

{c4_status} Critere 4 - Traversee rapide :
   Mediane = {residence_results['traverse_times']['median_traverse']:.1f} mois (cible < 6)

{'=' * 45}
SCORE : {residence_results['evidence_score']}/4

{'ATTRACTEURS CONFIRMES' if residence_results['evidence_score'] >= 3 else 'RESULTAT INCERTAIN'}
        """

        ax4.text(0.05, 0.95, summary_text, transform=ax4.transAxes, fontsize=10,
                 verticalalignment='top', fontfamily='monospace',
                 bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))

    plt.tight_layout()
    filename = "omega_v34_residence_time_analysis.png"
    plt.savefig(filename, dpi=150)
    print(f"\n[OK] Analyse Temps de Residence sauvegardee : {filename}")
    plt.close(fig)
def calculate_and_plot_global_fit(df, project_name):
    """
    Calcule le R¬≤ d'Ajustement Global, l'AIC et le BIC pour chaque mod√®le
    sur la s√©rie compl√®te (Œì) et trace les courbes.

    Retourne: dict {modele: {R2: ..., AIC: ..., BIC: ..., Params: [L, k, t0, v]}} et T_total.
    """

    T = df['temps_depuis_debut'].values
    Y = df['y_growth_model'].values
    N = len(T)
    T_total = N  # Dur√©e totale en mois

    if N < 12: return {}

    T_train = T
    Y_train = Y

    metrics_results = defaultdict(dict)

    fig, ax = plt.subplots(figsize=(12, 6))
    ax.plot(T, Y, 'o-', color='#34495e', markersize=3, label=r'Gamma Observ√© ($\Gamma$)')

    for name, model_info in MODELS.items():
        func = model_info['func']
        p0 = model_info['p0']
        bounds = model_info['bounds']

        # D√©terminer le nombre de param√®tres (k)
        if name == 'Richards':
            k = 4
        elif name == 'Logistique-L1':
            k = 2  # Seuls k et t0 sont variables
        else:
            k = 3  # L, k, t0 sont variables

        try:
            popt, pcov = curve_fit(func, T_train, Y_train, p0=p0, bounds=bounds, maxfev=5000)

            # 1. Pr√©diction et Calcul des m√©triques
            Y_pred_fit = func(T_train, *popt)
            r2_fit = r2_score(Y_train, Y_pred_fit)
            aic_fit, bic_fit = calculate_aic_bic(Y_train, Y_pred_fit, k)

            # 2. Stockage des m√©triques
            metrics_results[name]['R2'] = r2_fit
            metrics_results[name]['AIC'] = aic_fit
            metrics_results[name]['BIC'] = bic_fit

            # 3. Stockage des param√®tres ajust√©s (L, k, t0, v)
            params_list = popt.tolist()
            if name == 'Logistique-L1':
                # [k, t0] -> [L=1.0, k, t0, v=nan]
                params = [1.0] + params_list + [np.nan]
            elif len(params_list) == 3:
                # [L, k, t0] -> [L, k, t0, v=nan]
                params = params_list + [np.nan]
            else:
                params = params_list
            # Richards [L, k, t0, v]
            metrics_results[name]['Params'] = params

            # 4. Trac√© du graphique
            T_fit = np.arange(T[0], T[-1] + 10)
            Y_fit = func(T_fit, *popt)

            ax.plot(T_fit, Y_fit, '--', label=f'{name} (R¬≤: {r2_fit:.3f}, AIC: {aic_fit:.1f})')

        except Exception as e:
            metrics_results[name]['R2'] = np.nan
            metrics_results[name]['AIC'] = np.nan
            metrics_results[name]['BIC'] = np.nan
            metrics_results[name]['Params'] = [np.nan] * 4  # 4 NaNs si √©chec
            pass

    # Correction du SyntaxWarning et utilisation d'une cha√Æne brute r"" pour LaTeX
    ax.set_title(r"Ajustement des Mod√®les de Croissance sur $\Gamma$ : " + project_name, fontsize=14)
    ax.set_xlabel("Temps depuis le d√©but (Mois)")
    ax.set_ylabel(r"Ratio Gamma ($\Gamma$)")
    ax.legend(loc='lower right')
    ax.grid(True)
    filename = f"omega_v34_{project_name.lower()}_regression_fit_ajust.png"
    plt.savefig(filename, dpi=150)
    plt.close(fig)

    # Ajout de T_total pour le retour
    metrics_results['T_total'] = T_total
    return dict(metrics_results)


def run_regression_analysis_on_gamma(all_dataframes, train_window=24, hold_out=2):
    """
    Calcule et affiche les R¬≤ d'Ajustement Global, AIC et BIC, et la synth√®se
    des param√®tres Logistiques (mod√®le choisi pour sa robustesse).
    """
    print("\n" + "=" * 70)
    print("VALIDATION DE LA COURBE DE CROISSANCE (AJUSTEMENT GLOBAL)")
    print("=" * 70)

    # Dictionnaires pour collecter les r√©sultats
    r2_results = defaultdict(dict)
    aic_results = defaultdict(dict)
    bic_results = defaultdict(dict)
    logistique_params_results = {}  # Cible : Mod√®le Logistique

    # Nom du mod√®le pour la synth√®se des param√®tres
    TARGET_MODEL = 'Logistique-L1'

    for name, df in all_dataframes.items():
        if df is None or df.empty: continue

       # print(f"[{name}] Calcul et ajustement du Œì...")

        # 1. Calculer toutes les m√©triques et g√©n√©rer le plot
        metrics_dict = calculate_and_plot_global_fit(df, name)

        if not metrics_dict: continue

        # 2. Collecte des r√©sultats R2, AIC, BIC et des param√®tres Logistiques
        r2_table_data = {}
        for model, metrics in metrics_dict.items():
            if model != 'T_total':
                r2_results[name][model] = metrics.get('R2')
                aic_results[name][model] = metrics.get('AIC')
                bic_results[name][model] = metrics.get('BIC')
                r2_table_data[model] = metrics.get('R2')

        # 3. Collection des Param√®tres Logistiques (Mod√®le Cible)
        if TARGET_MODEL in metrics_dict:
            T_total = metrics_dict['T_total']
            params = metrics_dict[TARGET_MODEL].get('Params', [np.nan] * 4)

            # Pour Logistique, on v√©rifie seulement L, k, t0 (les 3 premiers)
            if not any(np.isnan(params[:3])):
                L, k, t0, v_ignored = params

                # --- Interpr√©tation ---
                interpretation = []
                if L >= 0.9:
                    interpretation.append("Maturit√© haute (L‚âà1)")
                else:
                    interpretation.append(f"Maturit√© limit√©e (L={L:.2f})")

                t0_ratio = t0 / T_total
                if t0_ratio < 0.33:
                    interpretation.append("Inflexion pr√©coce")
                elif t0_ratio > 0.66:
                    interpretation.append("Inflexion tardive")
                else:
                    interpretation.append("Inflexion mod√©r√©e")

                logistique_params_results[name] = {
                    'L': L,
                    'k': k,
                    't0': t0,
                    't0/T': t0_ratio,
                    # Le param√®tre v est implicitement 1.0 ou ignor√© pour la Logistique standard
                    'v': 1.0,
                    'Interpr√©tation': "; ".join(interpretation)
                }

        # 4. Affichage des R¬≤ AJUST. (pour la console interm√©diaire)
        r2_table = pd.Series(r2_table_data).round(3).sort_values(ascending=False)
       # print(f"[{name}] R¬≤ Ajustement Global : \n{r2_table.to_string()}")
        best_model = r2_table.index[0] if not r2_table.empty and not np.isnan(r2_table.iloc[0]) else "N/A"
       # print(f"[{name}] Meilleur mod√®le d'ajustement : **{best_model}**\n")

    # --- SYNTH√àSE GLOBALE R¬≤ / AIC / BIC ---

    if r2_results:
        # 1. SYNTH√àSE R¬≤ (Moyenne)
        r2_summary = pd.DataFrame(r2_results).T
        r2_summary.loc['MOYENNE'] = r2_summary.mean(numeric_only=True)
        best_r2_model = r2_summary.loc['MOYENNE'].idxmax()

        print("\n" + "=" * 70)
        print("SYNTH√àSE GLOBALE DES R¬≤ D'AJUSTEMENT (Meilleur ajustement = R¬≤ max)")
        print("=" * 70)
        print(r2_summary.round(3).to_string())
        print(f"\nConclusion R¬≤ : Le mod√®le **{best_r2_model}** a la meilleure performance descriptive moyenne.")

        # 2. SYNTH√àSE AIC (Somme)
        aic_summary = pd.DataFrame(aic_results).T
        aic_summary.loc['SOMME'] = aic_summary.sum(numeric_only=True)
        best_aic_model = aic_summary.loc['SOMME'].idxmin()

        print("\n" + "=" * 70)
        print("SYNTH√àSE GLOBALE AIC (Akaike) (Meilleur mod√®le = AIC min)")
        print("=" * 70)
        print(aic_summary.round(1).to_string())
        print(f"\nConclusion AIC : Le mod√®le **{best_aic_model}** est pr√©f√©r√© (p√©nalit√© de parcimonie).")

        # 3. SYNTH√àSE BIC (Somme)
        bic_summary = pd.DataFrame(bic_results).T
        bic_summary.loc['SOMME'] = bic_summary.sum(numeric_only=True)
        best_bic_model = bic_summary.loc['SOMME'].idxmin()

        print("\n" + "=" * 70)
        print("SYNTH√àSE GLOBALE BIC (Bayesian) (Meilleur mod√®le = BIC min)")
        print("=" * 70)
        print(bic_summary.round(1).to_string())
        print(f"\nConclusion BIC : Le mod√®le **{best_bic_model}** est pr√©f√©r√© (p√©nalit√© plus stricte).")

    # --- SYNTH√àSE DES PARAM√àTRES DU MOD√àLE LOGISTIQUE ---
    if logistique_params_results:
        params_summary = pd.DataFrame(logistique_params_results).T

        print("\n" + "=" * 70)
        print(f"PARAM√àTRES OPTIMAUX DU MOD√àLE CHOISI ({TARGET_MODEL})")
        print("=" * 70)

        # Conversion explicite en num√©rique et gestion des erreurs pour la robustesse
        numeric_cols = ['L', 'k', 't0', 't0/T', 'v']
        for col in numeric_cols:
            params_summary[col] = pd.to_numeric(params_summary[col], errors='coerce')

        # Formater les colonnes
        params_summary['L'] = params_summary['L'].round(3)
        params_summary['k'] = params_summary['k'].round(3)
        params_summary['t0'] = params_summary['t0'].round(1)
        params_summary['v'] = params_summary['v'].round(2)
        params_summary['t0/T'] = params_summary['t0/T'].round(2)

        # R√©organiser les colonnes pour l'affichage final (v est fixe √† 1.0 ou ignor√© par le fit)
        cols = ['L', 'k', 't0', 't0/T', 'v', 'Interpr√©tation']

        print(params_summary[cols].to_string())

        print(
            f"\nL : Asymptote max de Gamma (Maturit√© finale). t‚ÇÄ/T : Proportion de la vie du projet pour atteindre l'inflexion.")
    return logistique_params_results
def process_repo(name, position):
    conf = REPOS_CONFIG[name]
    engine = OmegaV34Engine(name, conf, position=position)
    if engine.load_repo():


        # run_analysis retourne (df, metrics)
        df, metrics = engine.run_analysis()

        if df is not None:
            final_metrics = engine.global_metrics
            plot_dual_view(df, name, conf)

            # Retourne (name, metrics, df)
            return (name, final_metrics, df)

    return None


# ==============================================================================
# MAIN
# ==============================================================================
# ==============================================================================
# 5. VISUALISATION DE LA R√âGRESSION (AJOUT)
# ==============================================================================
def plot_growth_model_fit(df, project_name, train_window, reg_result):
    """
    Trace la s√©rie Gamma et superpose les meilleurs ajustements trouv√©s sur la
    derni√®re fen√™tre d'entra√Ænement.
    """

    T = df['temps_depuis_debut'].values
    Y = df['y_growth_model'].values
    N = len(T)

    # Note: On utilise N-2 pour s'assurer que m√™me avec hold_out=2, on a assez de points
    if N < 12: return  # Minimum 12 mois pour un plot significatif

    # On utilise toutes les donn√©es disponibles (T) pour ajuster le mod√®le,
    # pour obtenir le R¬≤ d'AJUSTEMENT.
    T_train = T
    Y_train = Y

    fig, ax = plt.subplots(figsize=(12, 6))
    ax.plot(T, Y, 'o-', color='#34495e', markersize=3, label='Gamma Observ√© (Œì)')

    for name, model_info in MODELS.items():
        # Nous n'avons plus besoin de reg_result['R2_scores'] pour la l√©gende,
        # mais gardons la structure de la boucle.

        func = model_info['func']
        p0 = model_info['p0']
        bounds = model_info['bounds']

        try:
            # AJUSTEMENT : On ajuste sur la p√©riode compl√®te disponible (T_train = T)
            popt, pcov = curve_fit(func, T_train, Y_train, p0=p0, bounds=bounds, maxfev=5000)

            # 1. Calcul du R¬≤ d'Ajustement Global
            Y_pred_fit = func(T_train, *popt)
            r2_fit = r2_score(Y_train, Y_pred_fit)

            # 2. Pr√©diction pour le graphique
            T_fit = np.arange(T[0], T[-1] + 10)  # Pr√©dit 10 mois en avance
            Y_fit = func(T_fit, *popt)

            # 3. Affichage du R¬≤ d'Ajustement Global dans la l√©gende
            ax.plot(T_fit, Y_fit, '--', label=f'{name} (R¬≤ AJUST. : {r2_fit:.3f})')

        except Exception as e:
            # Si le fit √©choue, on ne trace rien
            print(f"[PLOT] √âchec du fit pour {name}: {e}")
            pass

    ax.set_title(f"Ajustement des Mod√®les de Croissance sur $Gamma$ : {project_name}", fontsize=14)
    ax.set_xlabel("Temps depuis le d√©but (Mois)")
    ax.set_ylabel("Ratio Gamma (Œì)")
    ax.legend(loc='lower right')
    ax.grid(True)
    filename = f"omega_v34_{project_name.lower()}_regression_fit_ajust.png"
    plt.savefig(filename, dpi=150)
    print(f"‚úÖ Ajustement de r√©gression visualis√© : {filename}")
    plt.close(fig)


# ==============================================================================
# TEST K : CAUSALIT√â DE GRANGER (Œì = CONS√âQUENCE, PAS CAUSE)
# ==============================================================================

from statsmodels.tsa.stattools import grangercausalitytests, adfuller
from statsmodels.tsa.api import VAR


def test_granger_causality(all_dataframes, max_lag=6):
    """
    TEST K : Causalit√© de Granger

    Hypoth√®se Ontodynamique :
    - L'ACTIVIT√â (survie) Granger-cause Œì (Œì est une cons√©quence)
    - Œì ne Granger-cause PAS l'activit√© (Œì n'est pas un moteur)

    Si valid√© : Œì √©lev√© = SYMPT√îME de bonne sant√©, pas CAUSE de survie.
    """
    print("\n" + "=" * 70)
    print("TEST K : CAUSALIT√â DE GRANGER")
    print("=" * 70)
    print("\nHypoth√®se : Œì √©lev√© est une CONS√âQUENCE de la survie, pas sa CAUSE")
    print("           (L'activit√© pr√©c√®de Œì, pas l'inverse)\n")

    results_by_project = {}

    # Synth√®se globale
    activity_causes_gamma_count = 0
    gamma_causes_activity_count = 0
    valid_projects = 0

    for name, df in all_dataframes.items():
        if df is None or df.empty:
            continue

        # Pr√©parer les s√©ries temporelles
        result = run_granger_for_project(name, df, max_lag)

        if result is None:
            continue

        results_by_project[name] = result
        valid_projects += 1

        if result['activity_causes_gamma']:
            activity_causes_gamma_count += 1
        if result['gamma_causes_activity']:
            gamma_causes_activity_count += 1

    # ===== SYNTH√àSE GLOBALE =====
    print("\n" + "=" * 70)
    print("SYNTH√àSE GLOBALE - CAUSALIT√â DE GRANGER")
    print("=" * 70)

    print(f"\nProjets analys√©s : {valid_projects}")
    print(f"\n{'Direction causale':<35} | {'Projets':<10} | {'%':<10}")
    print("-" * 60)

    pct_act_gamma = activity_causes_gamma_count / valid_projects * 100 if valid_projects > 0 else 0
    pct_gamma_act = gamma_causes_activity_count / valid_projects * 100 if valid_projects > 0 else 0

    print(f"{'Activit√© ‚Üí Œì (attendu)':<35} | {activity_causes_gamma_count:<10} | {pct_act_gamma:<10.1f}%")
    print(f"{'Œì ‚Üí Activit√© (non attendu)':<35} | {gamma_causes_activity_count:<10} | {pct_gamma_act:<10.1f}%")

    # Verdict
    print("\n" + "-" * 60)
    print("VERDICT :")

    # Crit√®re : Activit√©‚ÜíŒì significativement plus fr√©quent que Œì‚ÜíActivit√©
    ratio = pct_act_gamma / pct_gamma_act if pct_gamma_act > 0 else float('inf')

    if pct_act_gamma > 50 and ratio > 1.5:
        print(f"‚úÖ VALID√â : L'activit√© Granger-cause Œì dans {pct_act_gamma:.0f}% des cas")
        print(f"           Œì Granger-cause l'activit√© dans seulement {pct_gamma_act:.0f}% des cas")
        print(f"           Ratio : {ratio:.1f}x")
        print("\n   ‚Üí Œì √©lev√© est bien une CONS√âQUENCE de la survie, pas sa cause")
        verdict = "VALIDATED"
    elif pct_act_gamma > pct_gamma_act:
        print(f"‚ö†Ô∏è  PARTIEL : Tendance correcte mais pas √©crasante")
        print(f"           Activit√©‚ÜíŒì : {pct_act_gamma:.0f}% vs Œì‚ÜíActivit√© : {pct_gamma_act:.0f}%")
        verdict = "PARTIAL"
    else:
        print(f"‚ùå NON VALID√â : Pas de direction causale claire")
        verdict = "NOT_VALIDATED"

    return {
        'valid_projects': valid_projects,
        'activity_causes_gamma': activity_causes_gamma_count,
        'gamma_causes_activity': gamma_causes_activity_count,
        'pct_act_gamma': pct_act_gamma,
        'pct_gamma_act': pct_gamma_act,
        'ratio': ratio,
        'verdict': verdict,
        'details': results_by_project
    }


def run_granger_for_project(name, df, max_lag=6):
    """
    Ex√©cute le test de Granger bidirectionnel pour un projet.
    V38: Diff√©renciation automatique si s√©ries non-stationnaires.
    """
    min_obs = max_lag * 3 + 10
    if len(df) < min_obs:
        print(f"[{name}] Donn√©es insuffisantes ({len(df)} < {min_obs})")
        return None

    activity = df['total_weight'].values
    gamma = df['monthly_gamma'].values

    # Nettoyer les NaN et Inf
    mask = np.isfinite(activity) & np.isfinite(gamma)
    activity = activity[mask]
    gamma = gamma[mask]

    if len(activity) < min_obs:
        return None

    # Test de stationnarit√© (ADF)
    adf_activity = adfuller(activity, maxlag=max_lag)
    adf_gamma = adfuller(gamma, maxlag=max_lag)

    activity_stationary = adf_activity[1] < 0.05
    gamma_stationary = adf_gamma[1] < 0.05

    # === CORRECTION V38 : Diff√©renciation si non-stationnaire ===
    # On travaille sur les VARIATIONS (Œî) plut√¥t que les valeurs absolues
    # Cela √©vite les faux positifs dus aux tendances communes

    if not activity_stationary:
        activity = np.diff(activity, prepend=activity[0])

    if not gamma_stationary:
        gamma = np.diff(gamma, prepend=gamma[0])

    # Normaliser APR√àS diff√©renciation
    activity_norm = (activity - np.mean(activity)) / (np.std(activity) + 1e-8)
    gamma_norm = (gamma - np.mean(gamma)) / (np.std(gamma) + 1e-8)

    # Cr√©er le DataFrame pour Granger
    data = pd.DataFrame({
        'activity': activity_norm,
        'gamma': gamma_norm
    })

    # Test 1 : Activit√© ‚Üí Œì (est-ce que l'activit√© pass√©e pr√©dit Œì ?)
    try:
        test_act_to_gamma = grangercausalitytests(
            data[['gamma', 'activity']],  # [Y, X] : X cause Y ?
            maxlag=max_lag,
            verbose=False
        )
        # Extraire la meilleure p-value (sur tous les lags)
        p_values_1 = [test_act_to_gamma[lag][0]['ssr_ftest'][1] for lag in range(1, max_lag + 1)]
        best_p_act_gamma = min(p_values_1)
        best_lag_1 = p_values_1.index(best_p_act_gamma) + 1
    except Exception as e:
        best_p_act_gamma = 1.0
        best_lag_1 = None

    # Test 2 : Œì ‚Üí Activit√© (est-ce que Œì pass√© pr√©dit l'activit√© ?)
    try:
        test_gamma_to_act = grangercausalitytests(
            data[['activity', 'gamma']],  # [Y, X] : X cause Y ?
            maxlag=max_lag,
            verbose=False
        )
        p_values_2 = [test_gamma_to_act[lag][0]['ssr_ftest'][1] for lag in range(1, max_lag + 1)]
        best_p_gamma_act = min(p_values_2)
        best_lag_2 = p_values_2.index(best_p_gamma_act) + 1
    except Exception as e:
        best_p_gamma_act = 1.0
        best_lag_2 = None

    # Seuil de significativit√©
    alpha = 0.05

    activity_causes_gamma = best_p_act_gamma < alpha
    gamma_causes_activity = best_p_gamma_act < alpha

    # Affichage condens√©
    act_gamma_str = f"p={best_p_act_gamma:.4f}" + (" ‚úì" if activity_causes_gamma else "")
    gamma_act_str = f"p={best_p_gamma_act:.4f}" + (" ‚úì" if gamma_causes_activity else "")

    print(f"[{name:<15}] Act‚ÜíŒì: {act_gamma_str:<18} | Œì‚ÜíAct: {gamma_act_str:<18}")

    return {
        'n_obs': len(activity),
        'activity_stationary': activity_stationary,
        'gamma_stationary': gamma_stationary,
        'p_activity_to_gamma': best_p_act_gamma,
        'p_gamma_to_activity': best_p_gamma_act,
        'best_lag_act_gamma': best_lag_1,
        'best_lag_gamma_act': best_lag_2,
        'activity_causes_gamma': activity_causes_gamma,
        'gamma_causes_activity': gamma_causes_activity
    }


def plot_granger_results(granger_results):
    """
    Visualisation des r√©sultats du test de Granger.
    """
    if not granger_results or 'details' not in granger_results:
        return

    details = granger_results['details']

    if not details:
        return

    fig, axes = plt.subplots(1, 3, figsize=(15, 5))

    # ===== PLOT 1 : Comparaison des p-values =====
    ax1 = axes[0]

    projects = list(details.keys())
    p_act_gamma = [details[p]['p_activity_to_gamma'] for p in projects]
    p_gamma_act = [details[p]['p_gamma_to_activity'] for p in projects]

    x = np.arange(len(projects))
    width = 0.35

    bars1 = ax1.bar(x - width / 2, p_act_gamma, width, label='Activite -> Gamma', color='#27ae60', alpha=0.8)
    bars2 = ax1.bar(x + width / 2, p_gamma_act, width, label='Gamma -> Activite', color='#e74c3c', alpha=0.8)

    ax1.axhline(y=0.05, color='black', linestyle='--', linewidth=2, label='Seuil alpha=0.05')

    ax1.set_ylabel('P-value (log scale)')
    ax1.set_yscale('log')
    ax1.set_xticks(x)
    ax1.set_xticklabels(projects, rotation=45, ha='right', fontsize=8)
    ax1.set_title('Test de Granger par Projet\n(p < 0.05 = causalite significative)')
    ax1.legend(loc='upper right', fontsize=8)
    ax1.set_ylim(1e-10, 1)

    # ===== PLOT 2 : Scatter p-values =====
    ax2 = axes[1]

    ax2.scatter(p_act_gamma, p_gamma_act, s=100, c='#3498db', alpha=0.7, edgecolors='black')

    # Zones
    ax2.axvline(x=0.05, color='green', linestyle='--', alpha=0.7)
    ax2.axhline(y=0.05, color='red', linestyle='--', alpha=0.7)

    # Quadrants
    ax2.fill_between([0, 0.05], 0.05, 1, color='#27ae60', alpha=0.1)  # Act‚ÜíŒì seulement (attendu)
    ax2.fill_between([0.05, 1], 0, 0.05, color='#e74c3c', alpha=0.1)  # Œì‚ÜíAct seulement (probl√®me)
    ax2.fill_between([0, 0.05], 0, 0.05, color='#f39c12', alpha=0.1)  # Bidirectionnel

    ax2.set_xlabel('P-value: Activite -> Gamma')
    ax2.set_ylabel('P-value: Gamma -> Activite')
    ax2.set_xscale('log')
    ax2.set_yscale('log')
    ax2.set_xlim(1e-10, 1)
    ax2.set_ylim(1e-10, 1)
    ax2.set_title('Direction Causale\n(Coin bas-gauche = bidirectionnel)')

    # Annotations des quadrants
    ax2.text(0.001, 0.3, 'ATTENDU\nAct->Gamma', ha='center', fontsize=9, color='#27ae60', fontweight='bold')
    ax2.text(0.3, 0.001, 'PROBLEME\nGamma->Act', ha='center', fontsize=9, color='#e74c3c', fontweight='bold')

    # Labels des projets
    for i, proj in enumerate(projects):
        ax2.annotate(proj, (p_act_gamma[i], p_gamma_act[i]), fontsize=7, alpha=0.7)

    # ===== PLOT 3 : R√©sum√© =====
    ax3 = axes[2]
    ax3.axis('off')

    # Compter les cas
    only_act_gamma = sum(
        1 for p in projects if details[p]['activity_causes_gamma'] and not details[p]['gamma_causes_activity'])
    only_gamma_act = sum(
        1 for p in projects if details[p]['gamma_causes_activity'] and not details[p]['activity_causes_gamma'])
    bidirectional = sum(
        1 for p in projects if details[p]['activity_causes_gamma'] and details[p]['gamma_causes_activity'])
    no_causality = sum(
        1 for p in projects if not details[p]['activity_causes_gamma'] and not details[p]['gamma_causes_activity'])

    summary_text = f"""
    RESULTATS DU TEST DE GRANGER
    {'=' * 40}

    Projets analyses : {len(projects)}

    DIRECTION CAUSALE DETECTEE :

    [OK] Activite -> Gamma SEULEMENT : {only_act_gamma}
         (Gamma = consequence de l'activite)

    [X]  Gamma -> Activite SEULEMENT : {only_gamma_act}
         (Gamma serait une cause - NON ATTENDU)

    [<>] BIDIRECTIONNEL : {bidirectional}
         (Feedback mutuel)

    [--] PAS DE CAUSALITE : {no_causality}
         (Series independantes)

    {'=' * 40}

    INTERPRETATION ONTODYNAMIQUE :

    Si Activite -> Gamma domine :
    -> Gamma eleve = SYMPTOME de sante
    -> La metabolisation produit la structure
    -> Confirme : "l'etre se fait"

    Verdict : {granger_results['verdict']}
    Ratio : {granger_results['ratio']:.1f}x
    """

    ax3.text(0.05, 0.95, summary_text, transform=ax3.transAxes, fontsize=10,
             verticalalignment='top', fontfamily='monospace',
             bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))

    plt.tight_layout()
    filename = "omega_v34_granger_causality.png"
    plt.savefig(filename, dpi=150)
    print(f"\n[OK] Analyse Granger sauvegardee : {filename}")
    plt.close(fig)


# ==============================================================================
# FONCTION WRAPPER POUR LE MAIN
# ==============================================================================

def run_granger_test(all_dataframes, max_lag=6):
    """
    Fonction principale √† appeler dans le main.
    Lance le test de causalit√© de Granger.
    """
    # 1. Test de Granger
    results = test_granger_causality(all_dataframes, max_lag=max_lag)

    # 2. Visualisation
    if results:
        plot_granger_results(results)

    return results


# ==============================================================================
# TEST K-BIS : GRANGER SEGMENT√â PAR PHASE
# ==============================================================================

def test_granger_by_phase(all_dataframes, max_lag=4):
    """
    Test de Granger SEGMENT√â par phase temporelle (FIXE √† 33% du temps).
    CORRECTION V37 : Suppression de la circularit√© (Gamma > 0.6).
    On utilise un crit√®re exog√®ne (le temps) pour d√©finir l'√âmergence vs Maturit√©.
    """
    print("\n" + "=" * 80)
    print("TEST K-BIS : GRANGER SEGMENT√â (TEMPOREL / NON-CIRCULAIRE)")
    print("=" * 80)
    print("\nHypoth√®se Ontodynamique :")
    print("  - Phase 1 (0-33% du temps) : L'activit√© doit pr√©c√©der la structure (Construction)")
    print("  - Phase 2 (33%-100%)       : La structure doit gagner en influence (Contrainte)\n")

    results_phase1 = {'act_gamma': 0, 'gamma_act': 0, 'total': 0}
    results_phase2 = {'act_gamma': 0, 'gamma_act': 0, 'total': 0}

    project_details = {}

    # En-t√™te du tableau
    header = f"{'Projet':<15} | {'Split (Mois)':<15} | {'Phase 1 (√âmergence)':<25} | {'Phase 2 (Maturit√©)':<25}"
    print(header)
    print("-" * len(header))

    for name, df in all_dataframes.items():
        if df is None or len(df) < 40:
            continue

        gamma = df['monthly_gamma'].values
        activity = df['total_weight'].values
        total_months = len(df)

        # ======================================================================
        # CORRECTION MAJEURE : COUPE TEMPORELLE NEUTRE (33%)
        # ======================================================================
        # On ne regarde pas Gamma pour couper. On coupe au premier tiers.
        transition_idx = int(total_months * 0.33)

        # S√©curit√© : Il faut assez de donn√©es dans les deux phases (min 15 mois)
        # Sinon le test de Granger (VAR) ne converge pas math√©matiquement.
        if transition_idx < 15 or (total_months - transition_idx) < 15:
            # print(f"[{name:<13}] Segments trop courts (Total: {total_months}, Split: {transition_idx})")
            continue

        # --- Ex√©cution Granger Phase 1 (0 -> 33%) ---
        phase1_result = run_granger_segment(
            activity[:transition_idx],
            gamma[:transition_idx],
            max_lag
        )

        # --- Ex√©cution Granger Phase 2 (33% -> Fin) ---
        phase2_result = run_granger_segment(
            activity[transition_idx:],
            gamma[transition_idx:],
            max_lag
        )

        # --- Agr√©gation des r√©sultats Phase 1 ---
        if phase1_result:
            results_phase1['total'] += 1
            if phase1_result['act_causes_gamma']:
                results_phase1['act_gamma'] += 1
            if phase1_result['gamma_causes_act']:
                results_phase1['gamma_act'] += 1

        # --- Agr√©gation des r√©sultats Phase 2 ---
        if phase2_result:
            results_phase2['total'] += 1
            if phase2_result['act_causes_gamma']:
                results_phase2['act_gamma'] += 1
            if phase2_result['gamma_causes_act']:
                results_phase2['gamma_act'] += 1

        # --- Formatage pour l'affichage ligne par ligne ---
        p1_str = "N/A"
        if phase1_result:
            sig = []
            if phase1_result['act_causes_gamma']: sig.append("Act->Œì")
            if phase1_result['gamma_causes_act']: sig.append("Œì->Act")
            if not sig: sig.append("-")
            p1_str = f"{', '.join(sig)} (p={phase1_result['p_act_gamma']:.3f})"

        p2_str = "N/A"
        is_inverted = False
        if phase2_result:
            sig = []
            if phase2_result['act_causes_gamma']: sig.append("Act->Œì")
            if phase2_result['gamma_causes_act']: sig.append("Œì->Act")
            if not sig: sig.append("-")
            p2_str = f"{', '.join(sig)} (p={phase2_result['p_act_gamma']:.3f})"

            # D√©tection visuelle de l'inversion pour l'utilisateur
            # Si Œì->Act appara√Æt en phase 2 alors qu'il n'√©tait pas dominant en phase 1
            if phase2_result['gamma_causes_act']:
                is_inverted = True

        marker = "‚òÖ" if is_inverted else ""
        print(f"[{name:<13}] | {transition_idx:<3}/{total_months:<3} mois      | {p1_str:<25} | {p2_str:<25} {marker}")

        project_details[name] = {
            'transition_idx': transition_idx,
            'phase1': phase1_result,
            'phase2': phase2_result
        }

    # ==========================================================================
    # SYNTH√àSE ET CALCUL DU SHIFT
    # ==========================================================================

    # On utilise la fonction evaluate_granger_shift (d√©j√† pr√©sente dans ton code)
    shift_result = evaluate_granger_shift(results_phase1, results_phase2)

    # Affichage des pourcentages pour v√©rification rapide
    pct1_ag = results_phase1['act_gamma'] / results_phase1['total'] * 100 if results_phase1['total'] > 0 else 0
    pct1_ga = results_phase1['gamma_act'] / results_phase1['total'] * 100 if results_phase1['total'] > 0 else 0

    pct2_ag = results_phase2['act_gamma'] / results_phase2['total'] * 100 if results_phase2['total'] > 0 else 0
    pct2_ga = results_phase2['gamma_act'] / results_phase2['total'] * 100 if results_phase2['total'] > 0 else 0

    print("\n" + "-" * 80)
    print("R√âSUM√â DES POURCENTAGES (TEMPOREL)")
    print("-" * 80)
    print(f"PHASE 1 (Jeunesse) : Act->Œì: {pct1_ag:.1f}%  vs  Œì->Act: {pct1_ga:.1f}%")
    print(f"PHASE 2 (Maturit√©) : Act->Œì: {pct2_ag:.1f}%  vs  Œì->Act: {pct2_ga:.1f}%")

    # Verdict bas√© sur le shift
    verdict_final = "INCONCLUSIVE"
    if shift_result['validated']:
        verdict_final = "VALIDATED_SHIFT"
    elif shift_result.get('shift_positive'):
        verdict_final = "PARTIAL"
    elif results_phase1['total'] < 3:
        verdict_final = "INSUFFICIENT_DATA"
    else:
        verdict_final = "NOT_VALIDATED"

    return {
        'phase1': results_phase1,
        'phase2': results_phase2,
        'pct1_ag': pct1_ag, 'pct1_ga': pct1_ga,
        'pct2_ag': pct2_ag, 'pct2_ga': pct2_ga,
        'shift_result': shift_result,
        'verdict': verdict_final,
        'details': project_details
    }
def run_granger_segment(activity, gamma, max_lag):
    """
    Ex√©cute le test de Granger sur un segment temporel.
    """
    min_obs = max_lag * 3 + 5
    if len(activity) < min_obs:
        return None

    # Nettoyer
    mask = np.isfinite(activity) & np.isfinite(gamma)
    activity = activity[mask]
    gamma = gamma[mask]

    if len(activity) < min_obs:
        return None

    # Normaliser
    act_norm = (activity - np.mean(activity)) / (np.std(activity) + 1e-8)
    gam_norm = (gamma - np.mean(gamma)) / (np.std(gamma) + 1e-8)

    data = pd.DataFrame({'activity': act_norm, 'gamma': gam_norm})

    # Test 1 : Act ‚Üí Œì
    try:
        test1 = grangercausalitytests(data[['gamma', 'activity']], maxlag=max_lag, verbose=False)
        p_act_gamma = min([test1[lag][0]['ssr_ftest'][1] for lag in range(1, max_lag + 1)])
    except:
        p_act_gamma = 1.0

    # Test 2 : Œì ‚Üí Act
    try:
        test2 = grangercausalitytests(data[['activity', 'gamma']], maxlag=max_lag, verbose=False)
        p_gamma_act = min([test2[lag][0]['ssr_ftest'][1] for lag in range(1, max_lag + 1)])
    except:
        p_gamma_act = 1.0

    return {
        'n_obs': len(activity),
        'p_act_gamma': p_act_gamma,
        'p_gamma_act': p_gamma_act,
        'act_causes_gamma': p_act_gamma < 0.05,
        'gamma_causes_act': p_gamma_act < 0.05
    }


def plot_granger_by_phase(granger_phase_results):
    """
    Visualisation des r√©sultats du test Granger segment√©.
    """
    if not granger_phase_results:
        return

    fig, axes = plt.subplots(1, 2, figsize=(12, 5))

    # ===== PLOT 1 : Comparaison Phase 1 vs Phase 2 =====
    ax1 = axes[0]

    categories = ['Act -> Gamma', 'Gamma -> Act']
    phase1_pcts = [granger_phase_results['pct1_ag'], granger_phase_results['pct1_ga']]
    phase2_pcts = [granger_phase_results['pct2_ag'], granger_phase_results['pct2_ga']]

    x = np.arange(len(categories))
    width = 0.35

    bars1 = ax1.bar(x - width / 2, phase1_pcts, width, label='Phase 1 (Emergence)', color='#3498db', alpha=0.8)
    bars2 = ax1.bar(x + width / 2, phase2_pcts, width, label='Phase 2 (Maturite)', color='#e74c3c', alpha=0.8)

    ax1.set_ylabel('% de projets avec causalite significative')
    ax1.set_title('Direction Causale par Phase de Vie')
    ax1.set_xticks(x)
    ax1.set_xticklabels(categories)
    ax1.legend()
    ax1.set_ylim(0, 100)

    # Annotations
    for bar, pct in zip(bars1, phase1_pcts):
        if pct > 0:
            ax1.annotate(f'{pct:.0f}%', xy=(bar.get_x() + bar.get_width() / 2, bar.get_height()),
                         ha='center', va='bottom', fontsize=10, fontweight='bold')
    for bar, pct in zip(bars2, phase2_pcts):
        if pct > 0:
            ax1.annotate(f'{pct:.0f}%', xy=(bar.get_x() + bar.get_width() / 2, bar.get_height()),
                         ha='center', va='bottom', fontsize=10, fontweight='bold')

    # ===== PLOT 2 : R√©sum√© textuel =====
    ax2 = axes[1]
    ax2.axis('off')

    verdict = granger_phase_results['verdict']
    verdict_symbol = "[OK]" if verdict == "VALIDATED" else "[??]" if verdict == "PARTIAL" else "[X]"

    summary_text = f"""
    RESULTATS TEST K-BIS : GRANGER PAR PHASE
    {'=' * 45}

    PHASE 1 (Emergence, avant Gamma > 0.6) :
       Projets : {granger_phase_results['phase1']['total']}
       Act -> Gamma : {granger_phase_results['pct1_ag']:.0f}%
       Gamma -> Act : {granger_phase_results['pct1_ga']:.0f}%

    PHASE 2 (Maturite, apres Gamma > 0.6) :
       Projets : {granger_phase_results['phase2']['total']}
       Act -> Gamma : {granger_phase_results['pct2_ag']:.0f}%
       Gamma -> Act : {granger_phase_results['pct2_ga']:.0f}%

    {'=' * 45}

    INTERPRETATION ONTODYNAMIQUE :

    Si Phase 1 = Act->Gamma dominant :
    -> L'activite CONSTRUIT la structure
    -> "L'etre se fait"

    Si Phase 2 = Gamma->Act augmente :
    -> La structure CONTRAINT l'action
    -> "L'etre contraint"

    {'=' * 45}
    VERDICT : {verdict_symbol} {verdict}
    """

    ax2.text(0.05, 0.95, summary_text, transform=ax2.transAxes, fontsize=10,
             verticalalignment='top', fontfamily='monospace',
             bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))

    plt.tight_layout()
    filename = "omega_v34_granger_by_phase.png"
    plt.savefig(filename, dpi=150)
    print(f"\n[OK] Analyse Granger par phase sauvegardee : {filename}")
    plt.close(fig)


def evaluate_granger_shift(results_phase1, results_phase2):
    """
    MISE √Ä JOUR V38 : Analyse de la Sym√©trie (Operational Closure).
    On ne cherche plus l'inversion (Shift > 1.5), mais la convergence vers 1.0.
    """
    print(f"\n" + "-" * 70)
    print("CRIT√àRE V38 : SYM√âTRIE DU COUPLAGE CAUSAL")
    print("-" * 70)

    if results_phase1['total'] < 3 or results_phase2['total'] < 3:
        print("‚ö†Ô∏è  Donn√©es insuffisantes.")
        return {'validated': False, 'coupling_p1': 0, 'coupling_p2': 0}

    # Calcul des pourcentages
    pct1_ag = results_phase1['act_gamma'] / results_phase1['total'] * 100
    pct1_ga = results_phase1['gamma_act'] / results_phase1['total'] * 100

    pct2_ag = results_phase2['act_gamma'] / results_phase2['total'] * 100
    pct2_ga = results_phase2['gamma_act'] / results_phase2['total'] * 100

    # AJOUT : Calcul du Coupling Ratio (Sym√©trie)
    # 0 = Dominance totale (Asym√©trique) | 1 = √âquilibre parfait (Sym√©trique)
    # On √©vite la division par z√©ro
    max_p1 = max(pct1_ag, pct1_ga)
    coupling_p1 = (min(pct1_ag, pct1_ga) / max_p1) if max_p1 > 0 else 0

    max_p2 = max(pct2_ag, pct2_ga)
    coupling_p2 = (min(pct2_ag, pct2_ga) / max_p2) if max_p2 > 0 else 0

    delta_coupling = coupling_p2 - coupling_p1

    print(f"PHASE 1 (√âmergence) : Act->Œì {pct1_ag:.1f}% vs Œì->Act {pct1_ga:.1f}%")
    print(f"   ‚Ü≥ Couplage : {coupling_p1:.2f} (Asym√©trique)")

    print(f"PHASE 2 (Maturit√©)  : Act->Œì {pct2_ag:.1f}% vs Œì->Act {pct2_ga:.1f}%")
    print(f"   ‚Ü≥ Couplage : {coupling_p2:.2f} (Sym√©trique)")

    print(f"   ‚Ü≥ Œî Convergence : +{delta_coupling:.2f}")

    # NOUVEAU VERDICT (Symetrization)
    # On valide si on finit proche de la sym√©trie (> 0.8) ou si on a progress√©
    symmetry_validated = coupling_p2 > 0.85 or (delta_coupling > 0.15 and coupling_p2 > 0.7)

    print(f"\n" + "=" * 70)
    if symmetry_validated:
        print("‚úÖ OPERATIONAL CLOSURE VALID√âE :")
        print("   Le syst√®me converge vers un couplage bidirectionnel sym√©trique.")
        verdict = "SYMMETRY_VALIDATED"
    elif delta_coupling > 0:
        print("‚ö†Ô∏è  CONVERGENCE PARTIELLE :")
        print("   Le syst√®me se sym√©trise mais reste domin√© par un p√¥le.")
        verdict = "PARTIAL_SYMMETRY"
    else:
        print("‚ùå PAS DE CONVERGENCE :")
        print("   Le syst√®me reste asym√©trique ou diverge.")
        verdict = "NOT_VALIDATED"
    print("=" * 70)

    return {
        'coupling_p1': coupling_p1,
        'coupling_p2': coupling_p2,
        'delta': delta_coupling,
        'validated': symmetry_validated,
        'verdict': verdict
    }

# ==============================================================================
# TEST L (V35) : ROLLING GRANGER & THE CAUSAL CROSSOVER
# ==============================================================================

def analyze_rolling_granger(name, df, window_size=30, max_lag=3):
    """
    Calcule la causalit√© de Granger sur une fen√™tre glissante pour d√©tecter
    le "Point Om√©ga" (Inversion de la dominance causale).
    """
    if len(df) < window_size + 10:
        return None

    # Pr√©paration des s√©ries
    activity = df['total_weight'].values
    gamma = df['monthly_gamma'].values
    dates = df.index.values

    # Lissage l√©ger pour stabiliser Granger sur petites fen√™tres
    activity = pd.Series(activity).rolling(3).mean().fillna(0).values
    gamma = pd.Series(gamma).rolling(3).mean().fillna(0).values

    timeline = []
    p_act_to_gamma = []
    p_gamma_to_act = []
    authority_index = [] # > 0 si Structure domine, < 0 si Activit√© domine

   # print(f"[{name}] Calcul du Rolling Granger (Fen√™tre={window_size} mois)...")

    for i in range(len(df) - window_size):
        # Segment courant
        act_seg = activity[i : i + window_size]
        gam_seg = gamma[i : i + window_size]
        current_date = dates[i + window_size]

        # Normalisation locale (Crucial pour Granger local)
        if np.std(act_seg) == 0 or np.std(gam_seg) == 0:
            continue

        # Cr√©ation DataFrame temporaire
        data_seg = pd.DataFrame({
            'act': (act_seg - np.mean(act_seg)) / np.std(act_seg),
            'gam': (gam_seg - np.mean(gam_seg)) / np.std(gam_seg)
        })

        try:
            # Test 1: Act -> Gamma
            g1 = grangercausalitytests(data_seg[['gam', 'act']], maxlag=max_lag, verbose=False)
            # On prend la meilleure p-value parmi les lags
            p_ag = min([g1[lag][0]['ssr_ftest'][1] for lag in range(1, max_lag+1)])

            # Test 2: Gamma -> Act
            g2 = grangercausalitytests(data_seg[['act', 'gam']], maxlag=max_lag, verbose=False)
            p_ga = min([g2[lag][0]['ssr_ftest'][1] for lag in range(1, max_lag+1)])

            # M√©trique de "Force Causale" (Probabilit√© de causalit√© = 1 - p)
            # Plus c'est proche de 1, plus c'est causal.
            strength_ag = 1.0 - p_ag
            strength_ga = 1.0 - p_ga

            # Calcul de l'Indice d'Autorit√© (-1 √† 1)
            # Si Act->Gamma domine (0.9 vs 0.1) => Index n√©gatif (Libert√©)
            # Si Gamma->Act domine (0.1 vs 0.9) => Index positif (Structure)
            index = strength_ga - strength_ag

            timeline.append(current_date)
            p_act_to_gamma.append(strength_ag)
            p_gamma_to_act.append(strength_ga)
            authority_index.append(index)

        except:
            continue

    return {
        'dates': timeline,
        'strength_ag': p_act_to_gamma, # Force Activit√© -> Structure
        'strength_ga': p_gamma_to_act, # Force Structure -> Activit√©
        'authority': authority_index
    }


def plot_causal_crossover(name, results):
    """
    VERSION ANGLAISE : Figure 3 (M√©canisme Causal).
    """
    if not results or len(results['dates']) < 5:
        return

    dates = results['dates']
    # Lissage visuel
    s_ag = pd.Series(results['strength_ag']).rolling(6).mean()
    s_ga = pd.Series(results['strength_ga']).rolling(6).mean()
    authority = pd.Series(results['authority']).rolling(6).mean()

    # Configuration Style
    plt.style.use('seaborn-v0_8-whitegrid')
    plt.rcParams['font.family'] = 'sans-serif'

    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 9), sharex=True)

    # --- PLOT 1 : The Duel ---
    ax1.plot(dates, s_ag, color='#3498db', linewidth=2, label='Activity $\u2192$ Structure (Construction)')
    ax1.plot(dates, s_ga, color='#e74c3c', linewidth=2, label='Structure $\u2192$ Activity (Constraint)')

    # Seuil de significativit√©
    ax1.axhline(y=0.95, color='green', linestyle=':', alpha=0.6, label='Significance Threshold (p < 0.05)')

    # Remplissage
    ax1.fill_between(dates, s_ag, s_ga, where=(s_ga > s_ag), color='#e74c3c', alpha=0.1, interpolate=True)
    ax1.fill_between(dates, s_ag, s_ga, where=(s_ag > s_ga), color='#3498db', alpha=0.1, interpolate=True)

    ax1.set_ylabel('Causal Strength (1 - p-value)', fontsize=11)
    ax1.set_title(f"Causal Crossover in the {name} Ecosystem", fontsize=14, fontweight='bold')
    ax1.legend(loc='lower left', frameon=True, fontsize=10)
    ax1.grid(True, alpha=0.3)

    # --- PLOT 2 : Authority Index ---
    ax2.plot(dates, authority, color='black', linewidth=1)

    # Zones Anglaises
    ax2.fill_between(dates, authority, 0, where=(authority > 0), color='#e74c3c', alpha=0.3,
                     label='Structural Dominance')
    ax2.fill_between(dates, authority, 0, where=(authority < 0), color='#3498db', alpha=0.3, label='Creative Dominance')

    ax2.axhline(y=0, color='black', linestyle='-', linewidth=1)
    ax2.set_ylabel('Causal Authority Index\n(Structure - Activity)', fontsize=11)
    ax2.set_ylim(-1, 1)
    ax2.legend(loc='upper left', frameon=True)

    # Point Omega
    omega_date = None
    for i in range(len(authority) - 12):
        if authority[i] > 0 and authority[i + 1:i + 12].mean() > 0:
            omega_date = dates[i]
            break

    if omega_date:
        ax2.axvline(x=omega_date, color='purple', linestyle='--', linewidth=2)
        ax2.text(omega_date, 0.8, r' $\Omega$ Point (Inversion)', color='purple', fontweight='bold', ha='left')

    plt.tight_layout()

    # On garde le nom de fichier dynamique pour supporter kubernetes ou autres
    filename = f"omega_v35_{name.lower()}_crossover.png"
    plt.savefig(filename, dpi=300)
    plt.close(fig)

# ==============================================================================
# MAIN V36
# ==============================================================================
def generate_project_recap(all_dataframes, global_results):
    """
    G√©n√®re un tableau r√©capitulatif complet des projets.
    Colonnes : Project | Domain | Duration | Status | Commits | Final Œì
    """
    print("\n" + "=" * 80)
    print("TABLEAU R√âCAPITULATIF DES PROJETS")
    print("=" * 80)

    # Dictionnaire de mapping pour les domaines (bas√© sur tes configs habituelles)
    DOMAIN_MAP = {
        # Kernels
        'LINUX': 'Kernel/OS', 'FREEBSD': 'Kernel/OS',
        # Infrastructure
        'KUBERNETES': 'Infrastructure', 'NGINX': 'Server', 'HTTPD_APACHE': 'Server',
        # Databases
        'POSTGRES': 'Database', 'REDIS': 'Database', 'SQLITE': 'Database',
        # Browsers
        'GECKO_FIREFOX': 'Browser', 'WEBKIT': 'Browser',
        # Compilers
        'GCC': 'Compiler', 'LLVM': 'Compiler', 'CPYTHON': 'Language Runtime',
        'GO': 'Language Runtime', 'RUST': 'Language Runtime', 'PHP': 'Language Runtime',
        'NODE': 'Language Runtime',
        # AI/ML
        'PYTORCH': 'AI/ML', 'TENSORFLOW': 'AI/ML', 'SCIPY': 'Scientific',
        'OCTAVE': 'Scientific', 'MATPLOTLIB': 'Scientific',
        # Web
        'REACT': 'Web Framework', 'VUE': 'Web Framework', 'ANGULAR': 'Web Framework',
        'RAILS': 'Web Framework', 'DJANGO': 'Web Framework', 'FASTAPI': 'Web Framework',
        'METEOR': 'Fullstack',
        # Tools
        'GIT_SCM': 'VCS', 'SUBVERSION': 'VCS', 'VSCODE': 'IDE', 'EMACS': 'IDE',
        'FFMPEG': 'Multimedia', 'CURL': 'Networking', 'WIRESHARK': 'Networking',
        'GIMP': 'Graphics',
        # Apps
        'BITCOIN': 'Blockchain', 'GODOT': 'Game Engine', 'WORDPRESS': 'CMS',
        'LIBREOFFICE': 'Office Suite', 'MEDIAWIKI': 'CMS',
        # Legacy
        'ANGULAR_JS_LEGACY': 'Web Framework (Dead)',
    }

    recap_data = []

    for name, df in all_dataframes.items():
        if df is None or df.empty:
            continue

        # 1. Domain
        domain = DOMAIN_MAP.get(name, 'Software')

        # 2. Duration (Mois)
        duration = len(df)

        # 3. Status
        status = PROJECT_STATUS.get(name, 'Unknown').capitalize()

        # 4. Commits Analyzed (R√©cup√©r√© via Delta 1 ou calcul√© approximativement)
        metrics = global_results.get(name, {})
        commits = metrics.get('total_commits', 0)
        # Fallback si Delta 1 n'est pas appliqu√© : estimation via somme des poids
        if commits == 0:
            commits = int(df['files_touched'].sum())  # Approximation

        # 5. Final Œì (Moyenne des 3 derniers mois pour lisser)
        if len(df) >= 3:
            final_gamma = df['monthly_gamma'].iloc[-3:].mean()
        else:
            final_gamma = df['monthly_gamma'].iloc[-1]

        recap_data.append({
            'Project': name,
            'Domain': domain,
            'Duration (months)': duration,
            'Status': status,
            'Commits analyzed': commits,
            'Final Œì': round(final_gamma, 3)
        })

    # Cr√©ation DataFrame et Affichage
    df_recap = pd.DataFrame(recap_data)

    # Tri par Gamma d√©croissant
    df_recap = df_recap.sort_values(by='Final Œì', ascending=False)

    # Affichage Console Joli
    print(df_recap.to_markdown(index=False))

    # Export CSV
    df_recap.to_csv("omega_v36_project_recap.csv", index=False)
    print("\n‚úÖ Tableau export√© : omega_v36_project_recap.csv")


# ==============================================================================
# MODULE DE BLINDAGE (V37) : SENSITIVITY & NULL MODEL
# ==============================================================================

class RobustnessValidator:
    def __init__(self, all_dataframes):
        # On ne garde que les projets significatifs (> 20 mois)
        self.dfs = {k: v for k, v in all_dataframes.items() if v is not None and len(v) > 20}



    def run_sensitivity_analysis(self):
        """Test si la bimodalit√© r√©siste au changement de param√®tres."""
        print("\nüîé [Robustness] Sensitivity Analysis (Parameter Scan)...")
        results = []

        # Param√®tres √† tester
        lambdas = [0.6, 0.7, 0.8, 0.9, 1.0]  # P√©nalit√© de relocation
        thresholds = [0.6, 0.65, 0.7, 0.75, 0.8]  # Seuil r√©gime haut

        # On collecte toutes les donn√©es pour un test global
        all_gamma_raw = []
        for df in self.dfs.values():
            all_gamma_raw.extend(df['monthly_gamma'].dropna().values)
        all_gamma_raw = np.array(all_gamma_raw)

        for lam, thresh in product(lambdas, thresholds):
            # SIMULATION: On ajuste Gamma proportionnellement pour √©viter de relancer Git
            # Hypoth√®se : Gamma d√©pend lin√©airement de la p√©nalit√© Œª
            gamma_sim = all_gamma_raw * (lam / 0.8)
            gamma_sim = np.clip(gamma_sim, 0, None)

            # Test Dip (Bimodalit√©)
            try:
                dip, pval = diptest(gamma_sim)
            except:
                pval = 1.0  # Fail safe

            results.append({
                'lambda': lam,
                'threshold': thresh,
                'p_val': pval
            })

        self._plot_heatmap(results)
        return results

    def _plot_heatmap(self, results):
        """
        VERSION ANGLAISE : Figure 4 (Robustesse).
        """
        try:
            df_res = pd.DataFrame(results)
            pivot = df_res.pivot(index='lambda', columns='threshold', values='p_val')

            plt.figure(figsize=(8, 6))
            sns.set_style("white")

            # Heatmap : On affiche la CONFIANCE (1 - p_value).
            # Green = High Confidence (Low p-value), Red = Low Confidence.
            ax = sns.heatmap(1 - pivot, annot=True, cmap='RdYlGn', vmin=0.9, vmax=1.0, fmt=".3f")

            plt.title('Robustness Landscape\n(Bimodality Confidence $1-p$)', fontsize=12, fontweight='bold',
                      pad=15)
            plt.ylabel(r'Relocation Penalty ($\lambda$)', fontsize=11)
            plt.xlabel('High Regime Threshold', fontsize=11)

            plt.tight_layout()
            plt.savefig("omega_v37_sensitivity_heatmap.png", dpi=300)
            plt.close()
            print("‚úÖ (Heatmap) saved in English.")
        except Exception as e:
            print(f"‚ö†Ô∏è Error plotting Heatmap: {e}")

    def run_null_model(self, n_permutations=500):
        """
        CORRECTION V37.3 (Finale) : Test de la Trajectoire Temporelle.
        Hypoth√®se : La mont√©e vers la maturit√© (S-Curve) est une propri√©t√© historique unique.
        Si on m√©lange le temps, la pente devient nulle.
        """
        print(f"\nüé≤ [Robustness 2/3] Null Model Validation (Temporal Trajectory)...")

        real_slopes = []
        null_slopes_means = []

        for name, df in self.dfs.items():
            # On nettoie les NaNs pour polyfit
            gamma = df['monthly_gamma'].dropna().values
            if len(gamma) < 20: continue

            # 1. Pente R√©elle (Direction de l'√©volution)
            # x = temps, y = gamma. Pente > 0 = Maturation.
            x = np.arange(len(gamma))
            try:
                real_slope, _ = np.polyfit(x, gamma, 1)
                real_slopes.append(real_slope)
            except:
                continue

            # 2. Permutations (Destruction de l'histoire)
            current_nulls = []
            for _ in range(n_permutations):
                shuffled = np.random.permutation(gamma)
                try:
                    null_slope, _ = np.polyfit(x, shuffled, 1)
                    current_nulls.append(null_slope)
                except:
                    pass

            if current_nulls:
                null_slopes_means.append(np.mean(current_nulls))

        if not real_slopes:
            print("‚ö†Ô∏è Pas assez de donn√©es.")
            return 0

        # Z-SCORE GLOBAL
        real_mean = np.mean(real_slopes)
        null_mean = np.mean(null_slopes_means)
        null_std = np.std(null_slopes_means) + 1e-9

        z_score = (real_mean - null_mean) / null_std

        print(f"üìä Pente Moyenne R√©elle : {real_mean:.6f} (Tendance √† la maturation)")
        print(f"üìä Pente Moyenne Nulle  : {null_mean:.6f} (Hasard pur)")
        print(f"‚úÖ Z-Score Trajectoire  : {z_score:.2f}")

        if z_score > 3:
            print("üöÄ VICTOIRE : La dynamique de maturation est statistiquement irr√©futable.")
        else:
            print("‚ö†Ô∏è SIGNAL FAIBLE : La trajectoire ne se distingue pas assez du bruit.")

        return z_score
    def run_covariate_control(self):
        print("\nüë• [Robustness 3/3] Covariate Control (Team Size)...")
        raw_counts = {'act_gamma': 0, 'gamma_act': 0, 'total': 0}
        norm_counts = {'act_gamma': 0, 'gamma_act': 0, 'total': 0}

        for name, df in self.dfs.items():
            if 'intensity_per_dev' not in df.columns: continue

            gamma = df['monthly_gamma'].fillna(0).values
            act_raw = df['total_weight'].fillna(0).values
            act_norm = df['intensity_per_dev'].fillna(0).values

            if len(gamma) < 20: continue

            raw_counts['total'] += 1
            norm_counts['total'] += 1

            res_raw = self._quick_granger(gamma, act_raw)
            if res_raw['gamma_causes_act']: raw_counts['gamma_act'] += 1
            if res_raw['act_causes_gamma']: raw_counts['act_gamma'] += 1

            res_norm = self._quick_granger(gamma, act_norm)
            if res_norm['gamma_causes_act']: norm_counts['gamma_act'] += 1
            if res_norm['act_causes_gamma']: norm_counts['act_gamma'] += 1

        # AJOUT : Calcul des ratios pour l'article
        if raw_counts['total'] > 0 and norm_counts['total'] > 0:
            ratio_raw = (raw_counts['gamma_act'] + 0.5) / (raw_counts['act_gamma'] + 0.5)
            ratio_norm = (norm_counts['gamma_act'] + 0.5) / (norm_counts['act_gamma'] + 0.5)
            persistence = (ratio_norm / ratio_raw) * 100 if ratio_raw > 0 else 0

            print(f"\nüìä R√©sultats Covariate Control :")
            print(f"   Projets test√©s       : {raw_counts['total']}")
            print(f"   Ratio Granger BRUT   : {ratio_raw:.2f} (Œì‚ÜíAct / Act‚ÜíŒì)")
            print(f"   Ratio Granger NORM   : {ratio_norm:.2f} (Œì‚ÜíAct / Act‚ÜíŒì)")
            print(f"   Persistance          : {persistence:.0f}%")

            if persistence >= 70:
                print("‚úÖ COVARIATE CONTROL PASSED")
                print("   ‚Üí L'architecture contraint IND√âPENDAMMENT de la taille d'√©quipe")
            else:
                print("‚ö†Ô∏è COVARIATE CONTROL WARNING")
                print("   ‚Üí Le shift pourrait √™tre d√ª √† la stabilisation d'√©quipe")

            return {'ratio_raw': ratio_raw, 'ratio_norm': ratio_norm, 'persistence': persistence}

        return None

    def _quick_granger(self, x, y, max_lag=3):
        """Helper rapide pour Granger"""
        try:
            # x -> y ?
            d = pd.DataFrame({'x': x, 'y': y})
            # On utilise verbose=False pour ne pas polluer la console
            g = grangercausalitytests(d[['y', 'x']], maxlag=max_lag, verbose=False)
            p_xy = min([g[i][0]['ssr_ftest'][1] for i in range(1, max_lag + 1)])

            # y -> x ?
            g2 = grangercausalitytests(d[['x', 'y']], maxlag=max_lag, verbose=False)
            p_yx = min([g2[i][0]['ssr_ftest'][1] for i in range(1, max_lag + 1)])

            return {'act_causes_gamma': p_xy < 0.05, 'gamma_causes_act': p_yx < 0.05}
        except:
            return {'act_causes_gamma': False, 'gamma_causes_act': False}


def plot_phase_space_academic(all_dataframes, crossover_results):
    """
    G√©n√®re la Figure 2 : Portrait de Phase (Densit√© + Trajectoire).
    Version : Publication Ready (English Labels).
    """
    print("\nGenerating : Phase Portrait (Academic/English)...")



    # 1. Extraction et Alignement des donn√©es
    x_gamma = []
    y_authority = []

    for name, res in crossover_results.items():
        if name not in all_dataframes: continue

        # R√©cup√©ration des donn√©es brutes
        auth = np.array(res['authority'])
        dates = res['dates']
        df = all_dataframes[name]

        # Alignement strict sur les dates
        # On ne prend que les mois o√π on a √Ä LA FOIS le Gamma et l'Autorit√©
        mask = df.index.isin(dates)

        if mask.sum() == len(dates):
            # Cas id√©al : tout correspond
            gammas = df.loc[dates, 'monthly_gamma'].values
        elif len(df) >= len(auth):
            # Fallback : on prend les derniers points (souvent align√©s par la fin)
            gammas = df['monthly_gamma'].iloc[-len(auth):].values
        else:
            continue

        # Nettoyage des NaNs
        valid = ~np.isnan(gammas) & ~np.isnan(auth)
        x_gamma.extend(gammas[valid])
        y_authority.extend(auth[valid])

    if not x_gamma:
        print("‚ùå Pas assez de donn√©es pour le Portrait de Phase.")
        return

    # 2. Calcul de la Trajectoire Moyenne (La Ligne Rouge)
    df_plot = pd.DataFrame({'gamma': x_gamma, 'auth': y_authority})

    # On d√©coupe l'axe X (Gamma) en 20 tranches (bins)
    df_plot['bin'] = pd.cut(df_plot['gamma'], bins=np.linspace(0, 1, 20))

    # On calcule la moyenne de l'Autorit√© (Y) pour chaque tranche
    mean_path = df_plot.groupby('bin', observed=False)['auth'].mean()
    bin_centers = [b.mid for b in mean_path.index]

    # 3. Configuration du Style (Acad√©mique)
    sns.set_style("whitegrid")
    plt.rcParams['font.family'] = 'sans-serif'
    plt.rcParams['font.sans-serif'] = ['Arial', 'DejaVu Sans', 'Liberation Sans']

    fig, ax = plt.subplots(figsize=(10, 7))

    # A. La Densit√© (Nuages Bleus - KDE)
    # Montre la fr√©quence des √©tats (O√π le syst√®me passe-t-il du temps ?)
    sns.kdeplot(
        x=x_gamma, y=y_authority,
        fill=True, cmap="Blues",
        thresh=0.05, levels=15, alpha=0.8,
        ax=ax, cbar=True, cbar_kws={'label': 'State Density (Frequency)'}
    )

    # B. La Trajectoire Moyenne (Ligne Rouge)
    # Montre l'√©volution type : du chaos (gauche) vers l'ordre (droite)
    ax.plot(bin_centers, mean_path.values, color='#e74c3c', linewidth=3, marker='o',
            label='Mean Evolutionary Trajectory')

    # C. Lignes de r√©f√©rence
    ax.axhline(0, color='black', linestyle='-', linewidth=0.8, alpha=0.6)
    ax.axvline(0.7, color='black', linestyle=':', linewidth=1.5, label='Maturity Threshold')

    # D. Annotations ACAD√âMIQUES (Sobres et Pr√©cises)

    # Zone de gauche (Construction / Exploration)
    ax.text(0.15, -0.45, "PHASE 1: CONSTRUCTION\n(Exploratory / High Variance)",
            ha='center', va='center', color='#555555', fontsize=10,
            bbox=dict(facecolor='white', alpha=0.8, edgecolor='none', pad=3))

    # Zone de droite (Attracteur Stable)
    ax.text(0.85, 0.15, "STABLE ATTRACTOR\n(Operational Closure)",
            ha='center', va='center', color='black', fontsize=11, fontweight='bold',
            bbox=dict(facecolor='white', alpha=0.9, edgecolor='black', boxstyle='round,pad=0.5'))

    # E. Labels et Titre
    ax.set_xlim(0, 1.05)
    ax.set_ylim(-0.6, 0.6)
    ax.set_xlabel(r"Structural Maturity ($\Gamma$)", fontsize=12)
    ax.set_ylabel("Causal Authority Index\n(>0: Structure-driven | <0: Activity-driven)", fontsize=11)

    ax.set_title("Sociotechnical Phase Space & Convergence Trajectory", fontsize=14, pad=15)

    ax.legend(loc='upper left', frameon=True)

    plt.tight_layout()
    filename = "omega_v38_phase_academic.png"
    plt.savefig(filename, dpi=300)
    print(f"‚úÖ Figure 2 (Phase Portrait) saved: {filename}")


# ==============================================================================
# MODULE DE VALIDATION SCIENTIFIQUE AVANC√âE (V40 - HARD SCIENCE)
# ==============================================================================
from scipy import stats
from statsmodels.tsa.stattools import grangercausalitytests

# ==============================================================================
# MODULE DE VALIDATION SCIENTIFIQUE ROBUSTE (V44 - REVIEWER PROOF)
# ==============================================================================



class ScientificValidator:
    """
    MODULE V44 : VALIDATION SCIENTIFIQUE 'REVIEWER-PROOF'
    Int√®gre bootstrap par projet, alignement strict et m√©triques de sym√©trie.
    """

    def __init__(self, all_dataframes):
        # On ne garde que les projets significatifs (> 24 mois)
        self.dfs = {k: v for k, v in all_dataframes.items() if v is not None and len(v) > 24}
        self.project_keys = list(self.dfs.keys())

        # Donn√©es align√©es (Gamma + Granger) calcul√©es √† la vol√©e
        self.aligned_data = None

    def _align_granger_gamma(self, crossover_results):
        """
        HELPER CRITIQUE : Aligne strictement Gamma et les scores Granger par date.
        Complexit√© O(N), utilise les index pandas (plus robuste que les listes).
        """
        pooled_data = []

        for name, res in crossover_results.items():
            if name not in self.dfs: continue

            df = self.dfs[name]

            # Pr√©paration Granger en DataFrame
            # V√©rification que les cl√©s existent bien dans res
            if 'strength_ag' not in res or 'strength_ga' not in res:
                continue

            granger_df = pd.DataFrame({
                'date': res['dates'],
                's_ag': res['strength_ag'],  # Act -> Struct
                's_ga': res['strength_ga']  # Struct -> Act
            })
            # Conversion date si n√©cessaire et indexation
            granger_df['date'] = pd.to_datetime(granger_df['date'])
            granger_df = granger_df.set_index('date')

            # Pr√©paration Gamma
            gamma_series = df['monthly_gamma']

            # INNER JOIN strict sur les dates (ne garde que les mois o√π on a TOUT)
            merged = granger_df.join(gamma_series, how='inner')
            merged['project'] = name

            # Calcul de la m√©trique de Sym√©trie (Coupling Ratio)
            # 1.0 = Parfaite sym√©trie (Operational Closure)
            # 0.0 = Dominance totale unilat√©rale
            # On utilise min/max comme sugg√©r√© par le reviewer
            vals = merged[['s_ag', 's_ga']]
            merged['coupling_ratio'] = vals.min(axis=1) / (vals.max(axis=1) + 1e-9)

            pooled_data.append(merged)

        if pooled_data:
            self.aligned_data = pd.concat(pooled_data)
        else:
            self.aligned_data = pd.DataFrame()

    def solve_gmm_intersection(self, gmm):
        """Trouve le point d'intersection (seuil naturel) entre deux gaussiennes."""
        means = gmm.means_.flatten()
        stds = np.sqrt(gmm.covariances_.flatten())
        weights = gmm.weights_

        # Ordre : idx 0 = bas, idx 1 = haut
        if means[0] > means[1]:
            means = means[::-1]
            stds = stds[::-1]
            weights = weights[::-1]

        def diff_pdf(x):
            p1 = weights[0] * stats.norm.pdf(x, means[0], stds[0])
            p2 = weights[1] * stats.norm.pdf(x, means[1], stds[1])
            return p1 - p2

        try:
            # Recherche racine entre les moyennes
            threshold = brentq(diff_pdf, means[0], means[1])
        except Exception:
            threshold = (means[0] + means[1]) / 2

        return threshold, means, stds, weights

    def run_test_1_endogenous_threshold(self):
        """TEST 1 : Seuil Endog√®ne GMM."""
        print("\nüß™ [TEST 1] D√©termination du Seuil Endog√®ne (GMM)...")

        # Collecte de tous les gammas
        all_gamma = []
        for df in self.dfs.values():
            all_gamma.extend(df['monthly_gamma'].dropna().values)

        if not all_gamma:
            print("   ‚ö†Ô∏è Pas de donn√©es Gamma suffisantes.")
            return 0.7

        all_gamma = np.array(all_gamma).reshape(-1, 1)

        try:
            gmm = GaussianMixture(n_components=2, random_state=42, n_init=10)
            gmm.fit(all_gamma)

            threshold, means, stds, weights = self.solve_gmm_intersection(gmm)

            print(f"   R√©gime 1 (Explo) : Œº={means[0]:.3f}, œÉ={stds[0]:.3f}, w={weights[0]:.2f}")
            print(f"   R√©gime 2 (Sedim) : Œº={means[1]:.3f}, œÉ={stds[1]:.3f}, w={weights[1]:.2f}")
            print(f"   üéØ SEUIL NATUREL : {threshold:.4f}")
            return threshold
        except Exception as e:
            print(f"   ‚ö†Ô∏è Erreur GMM: {e}. Fallback sur 0.7")
            return 0.7

    def run_test_2_local_robustness(self, natural_threshold):
        """
        TEST 2 : Robustesse Locale (Variance ET Sym√©trie).
        """
        print(f"\nüß™ [TEST 2] Robustesse Locale (Variance & Sym√©trie)...")

        if self.aligned_data is None or self.aligned_data.empty:
            print("   ‚ö†Ô∏è Pas de donn√©es align√©es pour tester la sym√©trie.")
            return

        # On teste autour du seuil naturel
        test_thresholds = np.arange(max(0.1, natural_threshold - 0.1), min(0.9, natural_threshold + 0.1), 0.02)
        results = []

        for t in test_thresholds:
            # S√©paration bas√©e sur le seuil t
            low_regime = self.aligned_data[self.aligned_data['monthly_gamma'] < t]
            high_regime = self.aligned_data[self.aligned_data['monthly_gamma'] >= t]

            if len(low_regime) < 10 or len(high_regime) < 10: continue

            # M√©trique 1 : Variance Collapse (Doit √™tre > 1.0)
            # On compare la variance du gamma dans le r√©gime bas vs haut
            var_low = low_regime['monthly_gamma'].var()
            var_high = high_regime['monthly_gamma'].var()
            var_ratio = var_low / (var_high + 1e-9)

            # M√©trique 2 : Symmetry Gain (Doit √™tre positif)
            sym_low = low_regime['coupling_ratio'].mean()
            sym_high = high_regime['coupling_ratio'].mean()
            sym_gain = sym_high - sym_low

            results.append({
                'threshold': t,
                'var_ratio': var_ratio,
                'sym_gain': sym_gain,
                'sym_high': sym_high
            })

        if not results:
            print("   ‚ö†Ô∏è Pas assez de donn√©es pour le test de robustesse.")
            return

        df_res = pd.DataFrame(results)

        # Export pour SI (Supplementary Information)
        df_res.to_csv("omega_v44_robustness_grid.csv", index=False)

        # Analyse
        mean_gain = df_res['sym_gain'].mean()
        valid_steps = (df_res['sym_gain'] > 0).mean() * 100

        print(f"   Analyse sur {len(df_res)} pas de seuils autour de {natural_threshold:.2f} :")
        print(f"   Gain moyen de Sym√©trie (Haut - Bas) : +{mean_gain:.3f}")
        print(f"   Robustesse (Cas positifs) : {valid_steps:.1f}%")

        if valid_steps > 80:
            print("   ‚úÖ ROBUSTE : Le r√©gime haut est structurellement plus sym√©trique.")
        else:
            print("   ‚ö†Ô∏è INSTABLE : La sym√©trie d√©pend trop du seuil choisi.")

    def run_test_3_continuous_dynamics(self):
        """
        TEST 3 : Dynamique Continue (LOESS).
        Relation Gamma -> Sym√©trie sans seuil.
        """
        print("\nüß™ [TEST 3] Dynamique Continue (Gamma -> Sym√©trie)...")

        if self.aligned_data is None or self.aligned_data.empty:
            print("   ‚ö†Ô∏è Donn√©es insuffisantes.")
            return

        # X = Gamma, Y = Coupling Ratio (0..1)
        data = self.aligned_data.dropna(subset=['monthly_gamma', 'coupling_ratio'])

        x = data['monthly_gamma'].values
        y = data['coupling_ratio'].values

        if len(x) < 20:
            print("   ‚ö†Ô∏è Trop peu de points pour LOESS.")
            return

        # Tri pour LOESS
        sort_idx = np.argsort(x)
        x_sorted = x[sort_idx]
        y_sorted = y[sort_idx]

        try:
            # Lissage LOESS
            lowess = sm.nonparametric.lowess(y_sorted, x_sorted, frac=0.3)
            y_smooth = lowess[:, 1]

            # Calcul de pente globale (Lin√©aire simple pour la tendance)
            slope, intercept = np.polyfit(x_sorted, y_sorted, 1)

            # Comparaison d√©but vs fin
            start_val = y_smooth[:20].mean()
            end_val = y_smooth[-20:].mean()
            delta = end_val - start_val

            print(f"   Points analys√©s : {len(x)}")
            print(f"   Pente globale : {slope:.4f}")
            print(f"   Progression Sym√©trie (Liss√©e) : {start_val:.2f} -> {end_val:.2f} (Œî={delta:+.2f})")

            pd.DataFrame({'gamma': x_sorted, 'symmetry_smooth': y_smooth}).to_csv("omega_v44_loess_symmetry.csv")

            if delta > 0.05:
                print("   ‚úÖ VALID√â : Tendance continue vers la sym√©trie causale.")
            else:
                print("   ‚ùå NON VALID√â : Pas de tendance claire.")
        except Exception as e:
            print(f"   ‚ö†Ô∏è Erreur LOESS: {e}")

    def run_test_4_bootstrap_by_project(self, n_iterations=100):
        """
        TEST 4 : Bootstrap PAR PROJET.
        """
        print(f"\nüß™ [TEST 4] Bootstrap Structurel (par Projet, n={n_iterations})...")

        boot_thresholds = []

        if not self.project_keys:
            print("   ‚ö†Ô∏è Aucun projet pour le bootstrap.")
            return

        for i in range(n_iterations):
            # 1. R√©√©chantillonner la LISTE des projets
            sampled_keys = resample(self.project_keys, replace=True, random_state=i)

            # 2. Reconstruire un corpus fictif
            fake_corpus_gamma = []
            for k in sampled_keys:
                fake_corpus_gamma.extend(self.dfs[k]['monthly_gamma'].dropna().values)

            if not fake_corpus_gamma: continue

            fake_corpus_gamma = np.array(fake_corpus_gamma).reshape(-1, 1)

            # 3. Recalculer le seuil GMM
            try:
                gmm = GaussianMixture(n_components=2, random_state=42)
                gmm.fit(fake_corpus_gamma)
                t, _, _, _ = self.solve_gmm_intersection(gmm)

                if 0.3 < t < 0.9:
                    boot_thresholds.append(t)
            except:
                pass

        if not boot_thresholds:
            print("   ‚ö†Ô∏è √âchec du Bootstrap.")
            return

        mean_t = np.mean(boot_thresholds)
        ci_lower = np.percentile(boot_thresholds, 2.5)
        ci_upper = np.percentile(boot_thresholds, 97.5)
        width = ci_upper - ci_lower

        print(f"   Seuil Moyen (Bootstrap) : {mean_t:.4f}")
        print(f"   IC 95% : [{ci_lower:.4f} - {ci_upper:.4f}]")
        print(f"   Largeur IC : {width:.4f}")

        if width < 0.2:
            print("   ‚úÖ STABLE : Le seuil est une propri√©t√© robuste du corpus.")
        else:
            print("   ‚ö†Ô∏è LARGE : Le seuil d√©pend fortement de quelques projets cl√©s.")

    def run_full_suite(self, crossover_results):
        print("\n" + "=" * 70)
        print("D√âMARRAGE SUITE DE VALIDATION V44 (HARD SCIENCE / REVIEWER PROOF)")
        print("=" * 70)

        # 0. Pr√©-calcul (Alignement Gamma-Granger)
        print("üõ†Ô∏è  Alignement temporel strict (Gamma ‚Üî Granger)...")
        self._align_granger_gamma(crossover_results)

        # Test 1 : Seuil Naturel
        natural_threshold = self.run_test_1_endogenous_threshold()

        # Test 2 : Robustesse Locale
        self.run_test_2_local_robustness(natural_threshold)

        # Test 3 : Dynamique Continue
        self.run_test_3_continuous_dynamics()

        # Test 4 : Bootstrap Structurel
        self.run_test_4_bootstrap_by_project()

        print("=" * 70 + "\n")
def validate_comod_granger_link(
        comod_results: dict,
        granger_phase_results: dict,
        all_dataframes: dict
):
    """
    Validate the full mechanism: Œì ‚Üë ‚Üí fan-in ‚Üë ‚Üí Granger S‚ÜíA ‚Üë

    This tests whether increased file interdependence (fan-in) mediates
    the relationship between structural maturity (Œì) and causal constraint.
    """
    from scipy import stats
    import numpy as np
    import pandas as pd

    print("\n--- Testing Mediation: Œì ‚Üí Fan-In ‚Üí Structural Constraint ---\n")

    correlations = comod_results.get('correlations', [])
    temporal_metrics = comod_results.get('temporal_metrics', {})

    if not correlations:
        print("‚ö†Ô∏è No correlation data available")
        return

    # Step 1: Extract projects with both co-mod and Granger data
    projects_with_data = []

    for name, gamma_df in all_dataframes.items():
        if gamma_df is None or gamma_df.empty:
            continue

        # Get co-mod temporal data
        comod_temporal = temporal_metrics.get(name, [])
        if not comod_temporal:
            continue

        # Get Granger phase data
        granger_details = granger_phase_results.get('details', {}).get(name)
        if not granger_details:
            continue

        # Calculate mature-phase metrics
        df_comod = pd.DataFrame(comod_temporal)
        n_windows = len(df_comod)
        if n_windows < 4:
            continue

        # Early phase (first half) vs Late phase (second half)
        early_fanin = df_comod.iloc[:n_windows // 2]['mean_fan_in'].mean()
        late_fanin = df_comod.iloc[n_windows // 2:]['mean_fan_in'].mean()

        # Gamma evolution
        early_gamma = gamma_df['monthly_gamma'].iloc[:len(gamma_df) // 2].mean()
        late_gamma = gamma_df['monthly_gamma'].iloc[len(gamma_df) // 2:].mean()

        # Granger coupling evolution (from Phase results)
        granger_p1 = granger_details.get('phase1', {})
        granger_p2 = granger_details.get('phase2', {})

        # Structural constraint strength (Œì‚ÜíActivity causality)
        p_ga_p1 = granger_p1.get('p_gamma_act', 1.0) if granger_p1 else 1.0
        p_ga_p2 = granger_p2.get('p_gamma_act', 1.0) if granger_p2 else 1.0

        # Convert p-values to "causal strength" (1 - p)
        strength_p1 = 1 - p_ga_p1
        strength_p2 = 1 - p_ga_p2

        projects_with_data.append({
            'project': name,
            'early_gamma': early_gamma,
            'late_gamma': late_gamma,
            'delta_gamma': late_gamma - early_gamma,
            'early_fanin': early_fanin,
            'late_fanin': late_fanin,
            'delta_fanin': late_fanin - early_fanin,
            'early_constraint': strength_p1,
            'late_constraint': strength_p2,
            'delta_constraint': strength_p2 - strength_p1
        })

    if len(projects_with_data) < 5:
        print(f"‚ö†Ô∏è Only {len(projects_with_data)} projects with complete data. Need at least 5.")
        return

    df = pd.DataFrame(projects_with_data)

    # Step 2: Test correlation chain
    print(f"Projects analyzed: {len(df)}\n")

    # Correlation 1: ŒîŒì ‚Üí ŒîFan-In
    r1, p1 = stats.spearmanr(df['delta_gamma'], df['delta_fanin'])
    print(f"[Link 1] ŒîŒì ‚Üí ŒîFan-In:")
    print(f"         r = {r1:.3f}, p = {p1:.4f} {'‚úÖ' if p1 < 0.05 and r1 > 0 else '‚ùå'}")

    # Correlation 2: ŒîFan-In ‚Üí ŒîConstraint
    r2, p2 = stats.spearmanr(df['delta_fanin'], df['delta_constraint'])
    print(f"[Link 2] ŒîFan-In ‚Üí ŒîConstraint:")
    print(f"         r = {r2:.3f}, p = {p2:.4f} {'‚úÖ' if p2 < 0.05 and r2 > 0 else '‚ùå'}")

    # Correlation 3: ŒîŒì ‚Üí ŒîConstraint (total effect)
    r3, p3 = stats.spearmanr(df['delta_gamma'], df['delta_constraint'])
    print(f"[Link 3] ŒîŒì ‚Üí ŒîConstraint (total):")
    print(f"         r = {r3:.3f}, p = {p3:.4f} {'‚úÖ' if p3 < 0.05 and r3 > 0 else '‚ùå'}")

    # Step 3: Simple mediation test (Sobel approximation)
    # If r1 and r2 are significant and in same direction, mediation is plausible
    print("\n--- Mediation Analysis ---")

    # Step 3: Complete mediation test with ALL cases
    print("\n--- Mediation Analysis ---")

    if r1 > 0 and r2 > 0:
        indirect = r1 * r2
        print(f"Indirect effect (r1 √ó r2): {indirect:.3f}")

        if r3 > 0:
            mediation_ratio = indirect / r3
            print(f"Mediation ratio (indirect/total): {mediation_ratio:.1%}")

    # VERDICT COMPLET
    if p1 >= 0.05:
        print("\n‚ùå NO MECHANISM DETECTED:")
        print(f"   Link 1 (ŒîŒì ‚Üí ŒîFan-In) NOT significant: p = {p1:.4f}")
        print("   ‚Üí Maturity does NOT affect coupling")
        verdict = "NO_MECHANISM"

    elif p1 < 0.05 and p2 >= 0.05:
        print("\n‚ùå MEDIATION REJECTED:")
        print(f"   Link 1 (ŒîŒì ‚Üí ŒîFan-In) significant: r = {r1:.3f}")
        print(f"   Link 2 (ŒîFan-In ‚Üí ŒîConstraint) NOT significant: p = {p2:.4f}")
        print("\n   ‚Üí Maturity affects coupling, but coupling does NOT predict constraint")
        print("   ‚Üí The mechanism is TOPOLOGICAL, not METRIC")
        print("   ‚Üí Causal symmetrization emerges from closure structure,")
        print("      not from aggregated local dependencies")
        verdict = "TOPOLOGICAL_MECHANISM"

    elif p1 < 0.05 and p2 < 0.05 and p3 >= 0.05:
        print("\n‚úÖ FULL MEDIATION:")
        print(f"   Link 1 & 2 significant, but Link 3 (direct) is NOT")
        print("   ‚Üí Fan-In FULLY mediates the Œì ‚Üí Constraint relationship")
        verdict = "FULL_MEDIATION"

    elif p1 < 0.05 and p2 < 0.05 and p3 < 0.05:
        print("\n‚úÖ PARTIAL MEDIATION CONFIRMED:")
        print(f"   All 3 links significant")
        print(f"   Mediation ratio: {mediation_ratio:.1%}")
        verdict = "PARTIAL_MEDIATION"

    else:
        print("\n‚ö†Ô∏è INCONCLUSIVE: Unexpected correlation pattern")
        verdict = "INCONCLUSIVE"

    return verdict

    # Step 4: Export results
    df.to_csv("comod_v41_mechanism_validation.csv", index=False)
    print(f"\nüìä Detailed results saved: comod_v41_mechanism_validation.csv")

    # Step 5: Visualize the mechanism
    plot_mechanism_validation(df)

    return df, verdict


def correlate_fanin_with_granger_temporal(
        comod_temporal_metrics: dict,
        crossover_results: dict,
        all_dataframes: dict
) -> dict:
    """
    Corr√©lation TEMPORELLE directe : Fan-In(t) ‚Üî Granger Ratio(t)

    Pour chaque projet, aligne les fen√™tres temporelles et calcule
    la corr√©lation entre couplage structurel et contrainte causale.
    """
    print("\n" + "=" * 80)
    print("CORR√âLATION TEMPORELLE : Fan-In(t) ‚Üî Granger Ratio(t)")
    print("=" * 80)
    print("\nHypoth√®se : Si fan-in ‚Üë ‚Üí Granger S‚ÜíA ‚Üë, alors r > 0\n")

    all_correlations = []

    for name in comod_temporal_metrics:
        # Skip si pas de donn√©es Granger rolling
        if name not in crossover_results:
            continue

        comod_data = comod_temporal_metrics[name]
        granger_data = crossover_results[name]

        if not comod_data or not granger_data:
            continue

        # === 1. Pr√©parer les s√©ries temporelles ===

        # Co-modification : DataFrame avec dates
        df_comod = pd.DataFrame(comod_data)
        if 'window_start' not in df_comod.columns:
            continue
        df_comod['date'] = pd.to_datetime(df_comod['window_start'])
        df_comod = df_comod.set_index('date').sort_index()

        # Granger rolling : Construire DataFrame
        granger_dates = pd.to_datetime(granger_data['dates'])
        df_granger = pd.DataFrame({
            'date': granger_dates,
            'authority': granger_data['authority'],
            'strength_ga': granger_data['strength_ga'],  # Œì ‚Üí Activity
            'strength_ag': granger_data['strength_ag'],  # Activity ‚Üí Œì
        })
        df_granger = df_granger.set_index('date').sort_index()

        # Calculer le ratio de contrainte structurelle
        # Ratio > 1 = Structure domine, Ratio < 1 = Activit√© domine
        df_granger['granger_ratio'] = (
                df_granger['strength_ga'] / (df_granger['strength_ag'] + 0.01)
        )

        # === 2. Aligner les s√©ries par trimestre ===

        # Resampler co-mod par trimestre (moyenne)
        comod_quarterly = df_comod[['mean_fan_in', 'normalized_fan_in']].resample('QE').mean()

        # Resampler Granger par trimestre (moyenne)
        granger_quarterly = df_granger[['granger_ratio', 'authority']].resample('QE').mean()

        # Merger sur l'index temporel
        merged = pd.merge(
            comod_quarterly,
            granger_quarterly,
            left_index=True,
            right_index=True,
            how='inner'
        )

        if len(merged) < 5:
            continue

        # === 3. Calculer les corr√©lations ===

        # V√©rifier qu'il n'y a pas de NaN dans les donn√©es
        merged_clean = merged.dropna()
        if len(merged_clean) < 5:
            continue

        # Corr√©lation principale : Fan-In vs Granger Ratio
        r_fanin_granger, p_fanin_granger = stats.spearmanr(
            merged_clean['mean_fan_in'].values,
            merged_clean['granger_ratio'].values
        )

        # Corr√©lation normalis√©e (contr√¥le biais commit size)
        r_norm_granger, p_norm_granger = stats.spearmanr(
            merged_clean['normalized_fan_in'].values,
            merged_clean['granger_ratio'].values
        )

        # Corr√©lation avec Authority Index
        r_fanin_auth, p_fanin_auth = stats.spearmanr(
            merged_clean['mean_fan_in'].values,
            merged_clean['authority'].values
        )

        result = {
            'project': name,
            'n_points': len(merged_clean),
            'r_fanin_granger': r_fanin_granger,
            'p_fanin_granger': p_fanin_granger,
            'r_norm_granger': r_norm_granger,
            'p_norm_granger': p_norm_granger,
            'r_fanin_authority': r_fanin_auth,
            'p_fanin_authority': p_fanin_auth,
        }

        all_correlations.append(result)

        # Affichage par projet
        sig = "‚úÖ" if p_fanin_granger < 0.05 else ""
        print(f"[{name:<18}] Fan-In‚ÜîGranger: r={r_fanin_granger:+.3f}, p={p_fanin_granger:.4f} {sig}")

    # === 4. Synth√®se globale ===

    if not all_correlations:
        print("\n‚ö†Ô∏è Pas assez de donn√©es pour la synth√®se")
        return {}

    df_results = pd.DataFrame(all_correlations)

    print("\n" + "-" * 60)
    print("SYNTH√àSE GLOBALE")
    print("-" * 60)

    r_values = df_results['r_fanin_granger'].values
    p_values = df_results['p_fanin_granger'].values

    # CORRECTION : Filtrer les NaN
    valid_mask = ~np.isnan(r_values) & ~np.isnan(p_values)
    r_valid = r_values[valid_mask]
    p_valid = p_values[valid_mask]

    print(f"Projets analys√©s     : {len(df_results)}")
    print(f"Projets valides      : {len(r_valid)}")

    if len(r_valid) == 0:
        print("‚ö†Ô∏è Aucune corr√©lation valide calcul√©e")
        return {
            'correlations': all_correlations,
            'mean_r': np.nan,
            'median_r': np.nan,
            't_stat': np.nan,
            'p_value': np.nan
        }

    print(f"Corr√©lation moyenne  : r = {np.mean(r_valid):+.3f}")
    print(f"Corr√©lation m√©diane  : r = {np.median(r_valid):+.3f}")
    print(f"Positives (r > 0)    : {np.sum(r_valid > 0)}/{len(r_valid)}")
    print(f"Significatives (p<.05): {np.sum(p_valid < 0.05)}/{len(p_valid)}")

    # Test t sur les corr√©lations VALIDES
    if len(r_valid) >= 3:
        t_stat, t_pval = stats.ttest_1samp(r_valid, 0)
        print(f"\nT-test (H0: mean r = 0): t={t_stat:.2f}, p={t_pval:.4f}")

        if t_pval < 0.05:
            if np.mean(r_valid) > 0:
                print("‚úÖ VALID√â : Fan-In ‚Üë ‚Üí Granger S‚ÜíA ‚Üë")
                print("   ‚Üí Le couplage structurel PR√âDIT la contrainte causale")
            else:
                print("‚úÖ INVERSE : Fan-In ‚Üë ‚Üí Granger S‚ÜíA ‚Üì")
                print("   ‚Üí La modularisation augmente l'autonomie structurelle")
        else:
            print("‚ö†Ô∏è NON SIGNIFICATIF : Pas de lien direct Fan-In ‚Üí Granger")
    else:
        t_stat, t_pval = np.nan, np.nan
        print(f"\n‚ö†Ô∏è Pas assez de donn√©es pour le t-test (n={len(r_valid)})")

    # === 5. Visualisation ===
    plot_fanin_granger_correlation(df_results, all_correlations)

    # Export CSV
    df_results.to_csv("comod_v42_fanin_granger_temporal.csv", index=False)
    print(f"\nüìä R√©sultats export√©s : comod_v42_fanin_granger_temporal.csv")

    return {
        'correlations': all_correlations,
        'mean_r': np.mean(r_valid),
        'median_r': np.median(r_valid),
        't_stat': t_stat,
        'p_value': t_pval
    }


def plot_fanin_granger_correlation(df_results: pd.DataFrame, correlations: list):
    """Visualise la corr√©lation Fan-In ‚Üî Granger."""

    fig, axes = plt.subplots(1, 3, figsize=(15, 5))

    # === Plot 1: Distribution des corr√©lations ===
    ax1 = axes[0]
    r_values = df_results['r_fanin_granger'].values

    colors = ['#27ae60' if p < 0.05 else '#95a5a6'
              for p in df_results['p_fanin_granger']]

    ax1.barh(df_results['project'], r_values, color=colors, edgecolor='black')
    ax1.axvline(0, color='red', linestyle='-', linewidth=1)
    ax1.axvline(np.mean(r_values), color='blue', linestyle='--',
                linewidth=2, label=f'Mean: {np.mean(r_values):.2f}')
    ax1.set_xlabel('Spearman r (Fan-In ‚Üî Granger Ratio)')
    ax1.set_title('Per-Project Correlations\n(Green = p < 0.05)')
    ax1.legend()
    ax1.set_xlim(-1, 1)

    # === Plot 2: Histogramme des r ===
    ax2 = axes[1]
    ax2.hist(r_values, bins=15, color='#3498db', edgecolor='black', alpha=0.7)
    ax2.axvline(0, color='red', linestyle='--', linewidth=2, label='r = 0')
    ax2.axvline(np.mean(r_values), color='green', linestyle='-',
                linewidth=2, label=f'Mean: {np.mean(r_values):.2f}')
    ax2.set_xlabel('Spearman r')
    ax2.set_ylabel('Number of Projects')
    ax2.set_title('Distribution of Fan-In ‚Üî Granger Correlations')
    ax2.legend()

    # === Plot 3: Raw vs Normalized ===
    ax3 = axes[2]
    ax3.scatter(
        df_results['r_fanin_granger'],
        df_results['r_norm_granger'],
        s=100, alpha=0.7, c='#9b59b6', edgecolors='black'
    )
    ax3.plot([-1, 1], [-1, 1], 'k--', alpha=0.5, label='y = x')
    ax3.set_xlabel('Raw r (Fan-In ‚Üî Granger)')
    ax3.set_ylabel('Normalized r (Bias-controlled)')
    ax3.set_title('Bias Control Check\n(Divergence = Commit Style Effect)')
    ax3.set_xlim(-1, 1)
    ax3.set_ylim(-1, 1)
    ax3.legend()
    ax3.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.savefig("comod_v42_fanin_granger_correlation.png", dpi=150)
    print("‚úÖ Plot saved: comod_v42_fanin_granger_correlation.png")
    plt.close(fig)
def plot_mechanism_validation(df: pd.DataFrame):
    """Visualize the mediation mechanism."""
    import matplotlib.pyplot as plt
    import seaborn as sns

    fig, axes = plt.subplots(1, 3, figsize=(15, 5))

    # Plot 1: ŒîŒì vs ŒîFan-In
    ax1 = axes[0]
    ax1.scatter(df['delta_gamma'], df['delta_fanin'], s=100, alpha=0.7, c='#3498db')
    z1 = np.polyfit(df['delta_gamma'], df['delta_fanin'], 1)
    x_line = np.linspace(df['delta_gamma'].min(), df['delta_gamma'].max(), 100)
    ax1.plot(x_line, np.poly1d(z1)(x_line), 'r--', linewidth=2)
    ax1.axhline(0, color='gray', linestyle=':', alpha=0.5)
    ax1.axvline(0, color='gray', linestyle=':', alpha=0.5)
    ax1.set_xlabel('ŒîŒì (Maturity Change)')
    ax1.set_ylabel('ŒîFan-In (Coupling Change)')
    ax1.set_title('Link 1: Maturity ‚Üí Coupling')
    ax1.grid(True, alpha=0.3)

    # Plot 2: ŒîFan-In vs ŒîConstraint
    ax2 = axes[1]
    ax2.scatter(df['delta_fanin'], df['delta_constraint'], s=100, alpha=0.7, c='#e74c3c')
    z2 = np.polyfit(df['delta_fanin'], df['delta_constraint'], 1)
    x_line = np.linspace(df['delta_fanin'].min(), df['delta_fanin'].max(), 100)
    ax2.plot(x_line, np.poly1d(z2)(x_line), 'r--', linewidth=2)
    ax2.axhline(0, color='gray', linestyle=':', alpha=0.5)
    ax2.axvline(0, color='gray', linestyle=':', alpha=0.5)
    ax2.set_xlabel('ŒîFan-In (Coupling Change)')
    ax2.set_ylabel('ŒîConstraint (Granger S‚ÜíA)')
    ax2.set_title('Link 2: Coupling ‚Üí Constraint')
    ax2.grid(True, alpha=0.3)

    # Plot 3: Full chain visualization
    ax3 = axes[2]

    # Color by delta_gamma (maturity change)
    colors = df['delta_gamma']
    scatter = ax3.scatter(
        df['delta_fanin'],
        df['delta_constraint'],
        c=colors,
        cmap='RdYlGn',
        s=100,
        alpha=0.8,
        edgecolors='black'
    )
    plt.colorbar(scatter, ax=ax3, label='ŒîŒì')
    ax3.axhline(0, color='gray', linestyle=':', alpha=0.5)
    ax3.axvline(0, color='gray', linestyle=':', alpha=0.5)
    ax3.set_xlabel('ŒîFan-In')
    ax3.set_ylabel('ŒîConstraint')
    ax3.set_title('Full Mechanism\n(Color = ŒîŒì)')
    ax3.grid(True, alpha=0.3)

    # Annotate projects
    for _, row in df.iterrows():
        ax3.annotate(
            row['project'][:8],
            (row['delta_fanin'], row['delta_constraint']),
            fontsize=7,
            alpha=0.7
        )

    plt.tight_layout()
    plt.savefig("comod_v41_mechanism_validation.png", dpi=150)
    print("‚úÖ Mechanism plot saved: comod_v41_mechanism_validation.png")
    plt.close(fig)


def plot_bidirectional_architecture_patterns(
        correlations: list,
        crossover_results: dict,
        output_path: str = "figure_architecture_paths.png"
):
    """
    Figure for publication: Two architectural paths to operational closure.
    Shows that consolidation and modularization both lead to causal symmetrization.
    """

    # Filter valid correlations
    valid = [c for c in correlations if c and 'r_gamma_fanin' in c and not np.isnan(c['r_gamma_fanin'])]
    df = pd.DataFrame(valid)

    # Classify projects
    df['pattern'] = df['r_gamma_fanin'].apply(
        lambda r: 'Consolidation' if r > 0.15 else ('Modularization' if r < -0.15 else 'Neutral')
    )
    df['significant'] = df['p_gamma_fanin'] < 0.05

    fig = plt.figure(figsize=(14, 5))

    # === Panel A: Distribution of correlations ===
    ax1 = fig.add_subplot(131)

    colors = []
    for _, row in df.iterrows():
        if row['r_gamma_fanin'] > 0.15:
            colors.append('#2ecc71')  # Green for consolidation
        elif row['r_gamma_fanin'] < -0.15:
            colors.append('#e74c3c')  # Red for modularization
        else:
            colors.append('#95a5a6')  # Gray for neutral

    # Sort by correlation for visual clarity
    df_sorted = df.sort_values('r_gamma_fanin', ascending=True)
    colors_sorted = [colors[i] for i in df_sorted.index]

    bars = ax1.barh(range(len(df_sorted)), df_sorted['r_gamma_fanin'],
                    color=colors_sorted, edgecolor='black', linewidth=0.5)

    ax1.axvline(0, color='black', linestyle='-', linewidth=1)
    ax1.axvline(0.15, color='green', linestyle='--', alpha=0.5)
    ax1.axvline(-0.15, color='red', linestyle='--', alpha=0.5)

    ax1.set_xlabel('Spearman r (Œì vs Fan-In)', fontsize=11)
    ax1.set_ylabel('Projects (sorted)', fontsize=11)
    ax1.set_title('A. Two Architectural Paths', fontsize=12, fontweight='bold')
    ax1.set_xlim(-0.8, 0.9)

    # Legend
    from matplotlib.patches import Patch
    legend_elements = [
        Patch(facecolor='#2ecc71', edgecolor='black', label=f'Consolidation (n={sum(df["r_gamma_fanin"] > 0.15)})'),
        Patch(facecolor='#e74c3c', edgecolor='black', label=f'Modularization (n={sum(df["r_gamma_fanin"] < -0.15)})'),
        Patch(facecolor='#95a5a6', edgecolor='black', label=f'Neutral (n={sum(abs(df["r_gamma_fanin"]) <= 0.15)})')
    ]
    ax1.legend(handles=legend_elements, loc='lower right', fontsize=9)

    ax1.set_yticks([])

    # === Panel B: Both paths reach high Œì ===
    ax2 = fig.add_subplot(132)

    # Get final Gamma for each project (you'll need to pass this data)
    # For now, simulate with the correlation data
    consolidation = df[df['r_gamma_fanin'] > 0.15]['r_gamma_fanin'].values
    modularization = df[df['r_gamma_fanin'] < -0.15]['r_gamma_fanin'].values

    # Box plot of correlations by group
    box_data = [
        df[df['pattern'] == 'Consolidation']['r_gamma_fanin'].values,
        df[df['pattern'] == 'Neutral']['r_gamma_fanin'].values,
        df[df['pattern'] == 'Modularization']['r_gamma_fanin'].values,
    ]

    bp = ax2.boxplot(box_data, labels=['Consolidation', 'Neutral', 'Modularization'],
                     patch_artist=True)

    colors_box = ['#2ecc71', '#95a5a6', '#e74c3c']
    for patch, color in zip(bp['boxes'], colors_box):
        patch.set_facecolor(color)
        patch.set_alpha(0.7)

    ax2.axhline(0, color='black', linestyle='--', alpha=0.5)
    ax2.set_ylabel('Spearman r (Œì vs Fan-In)', fontsize=11)
    ax2.set_title('B. Distinct Strategies', fontsize=12, fontweight='bold')
    ax2.grid(True, alpha=0.3, axis='y')

    # === Panel C: Conceptual diagram ===
    ax3 = fig.add_subplot(133)
    ax3.set_xlim(0, 10)
    ax3.set_ylim(0, 10)
    ax3.axis('off')

    # Draw the triangle
    # Top: Œì (Maturity)
    ax3.text(5, 9, 'Œì (Maturity)', ha='center', va='center', fontsize=12, fontweight='bold',
             bbox=dict(boxstyle='round,pad=0.3', facecolor='#3498db', edgecolor='black'))

    # Bottom left: Fan-In
    ax3.text(1.5, 2, 'Fan-In\n(Coupling)', ha='center', va='center', fontsize=10,
             bbox=dict(boxstyle='round,pad=0.3', facecolor='#f39c12', edgecolor='black'))

    # Bottom right: Granger
    ax3.text(8.5, 2, 'Granger\nSymmetry', ha='center', va='center', fontsize=10,
             bbox=dict(boxstyle='round,pad=0.3', facecolor='#27ae60', edgecolor='black'))

    # Arrows
    # Edge 1: Œì ‚Üí Fan-In (bidirectional result)
    ax3.annotate('', xy=(2.2, 3), xytext=(4.3, 8.2),
                 arrowprops=dict(arrowstyle='->', color='orange', lw=2))
    ax3.text(2.3, 5.8, '¬±', fontsize=14, color='orange', fontweight='bold')
    ax3.text(1.2, 5.2, 'Bidirectional\n(r = ¬±0.3‚Äì0.8)', fontsize=8, color='gray')

    # Edge 2: Fan-In ‚Üí Granger (NOT significant)
    ax3.annotate('', xy=(7.5, 2), xytext=(3, 2),
                 arrowprops=dict(arrowstyle='->', color='red', lw=2, linestyle='--'))
    ax3.text(5, 1, '‚úó n.s.', fontsize=11, color='red', ha='center', fontweight='bold')
    ax3.text(5, 0.3, '(p = 0.44)', fontsize=8, color='gray', ha='center')

    # Edge 3: Œì ‚Üí Granger (validated elsewhere)
    ax3.annotate('', xy=(7.8, 3), xytext=(5.7, 8.2),
                 arrowprops=dict(arrowstyle='->', color='green', lw=2))
    ax3.text(7.5, 5.8, '‚úì', fontsize=14, color='green', fontweight='bold')
    ax3.text(8.2, 5.2, 'Symmetrization\n(0.60 ‚Üí 0.94)', fontsize=8, color='gray')

    ax3.set_title('C. Mechanistic Triangle', fontsize=12, fontweight='bold')

    # Add interpretation text at bottom
    ax3.text(5, -0.8, 'Mechanism is TOPOLOGICAL, not METRIC:\nBoth paths achieve closure',
             ha='center', va='top', fontsize=9, style='italic',
             bbox=dict(boxstyle='round,pad=0.3', facecolor='#ecf0f1', edgecolor='gray'))

    plt.tight_layout()
    plt.savefig(output_path, dpi=300, bbox_inches='tight', facecolor='white')
    print(f"‚úÖ Publication figure saved: {output_path}")
    plt.close(fig)

    # Print stats for caption
    print("\n--- FIGURE STATISTICS (for caption) ---")
    print(f"Total projects: {len(df)}")
    print(f"Consolidation (r > 0.15): {sum(df['r_gamma_fanin'] > 0.15)}")
    print(f"Modularization (r < -0.15): {sum(df['r_gamma_fanin'] < -0.15)}")
    print(f"Neutral: {sum(abs(df['r_gamma_fanin']) <= 0.15)}")
    print(f"Significant (p < 0.05): {sum(df['significant'])}")

    return df
def run_hindcasting_test(all_dataframes, project_status=None):
    """Fonction wrapper pour le main."""

    return results




def test_variance_collapse_symmetrization(crossover_results, all_dataframes):
    """
    PHASE 7B ;  CAUSAL VARIANCE COLLAPSE TEST

    Hypothesis: Structural maturity (Gamma) does not strictly force mean symmetry to 0,
    but constrains the *variance* of causal imbalance.
    Mature systems are 'locked' into a narrow channel of interaction.
    """
    print("\n" + "=" * 80)
    print("PHASE 13: CAUSAL VARIANCE COLLAPSE (CONSTRAINT ATTRACTOR)")
    print("=" * 80)

    pooled_data = []

    # --- 1. DATA POOLING ---
    for name, res in crossover_results.items():
        if name not in all_dataframes:
            continue

        df_proj = all_dataframes[name]
        dates = res['dates']
        # Forces causales (1 - p_value)
        s_ag = np.array(res['strength_ag'])
        s_ga = np.array(res['strength_ga'])

        # Alignement temporel
        indices = [i for i, d in enumerate(dates) if d in df_proj.index]
        matched_dates = [dates[i] for i in indices]

        gammas = df_proj.loc[matched_dates, 'monthly_gamma'].values
        s_ag = s_ag[indices]
        s_ga = s_ga[indices]

        for g, ag, ga in zip(gammas, s_ag, s_ga):
            if not np.isnan(g) and not np.isnan(ag) and not np.isnan(ga):
                pooled_data.append({
                    'gamma': g,
                    'strength_ag': ag,
                    'strength_ga': ga
                })

    df = pd.DataFrame(pooled_data)

    if len(df) < 50:
        print("‚ö†Ô∏è Insufficient data.")
        return

    # --- 2. METRIC: CAUSAL IMBALANCE ---
    # |Act->Str - Str->Act|
    # 0 = Sym√©trie, 1 = Dominance totale
    df['imbalance'] = np.abs(df['strength_ag'] - df['strength_ga'])

    # --- 3. BINNING & VARIANCE ANALYSIS ---
    # On d√©coupe Gamma en 10 tranches pour observer l'√©volution de la distribution
    # On utilise qcut pour avoir un nombre √©quitable de points par bin, ou cut pour des intervalles fixes.
    # Ici 'cut' est pr√©f√©rable pour voir la physique de l'axe Gamma.
    df['gamma_bin'] = pd.cut(df['gamma'], bins=np.linspace(0, 1, 11))

    # On calcule les statistiques par bin
    bin_stats = df.groupby('gamma_bin', observed=True)['imbalance'].agg([
        'count', 'mean', 'var',
        lambda x: x.quantile(0.9) - x.quantile(0.1)  # Inter-decile range (Spread)
    ]).rename(columns={'<lambda_0>': 'spread'})

    bin_stats['gamma_mid'] = bin_stats.index.map(lambda x: x.mid).astype(float)

    # Filtrer les bins vides ou trop petits
    valid_bins = bin_stats[bin_stats['count'] > 10].copy()

    if len(valid_bins) < 3:
        print("‚ö†Ô∏è Not enough populated bins for trend analysis.")
        return

    # --- 4. STATISTICAL TEST (Correlation on Variance/Spread) ---
    # Hypoth√®se : Plus Gamma augmente, plus le Spread diminue (Corr√©lation N√©gative)

    r_spread, p_spread = stats.spearmanr(valid_bins['gamma_mid'], valid_bins['spread'])
    r_var, p_var = stats.spearmanr(valid_bins['gamma_mid'], valid_bins['var'])

    print(f"\nüìä Variance Collapse Statistics (across {len(valid_bins)} bins):")
    print(f"   Correlation (Gamma vs Spread Q90-Q10): r = {r_spread:.3f}, p = {p_spread:.4f}")
    print(f"   Correlation (Gamma vs Variance):       r = {r_var:.3f},    p = {p_var:.4f}")

    # Verdict logic
    is_validated = (r_spread < -0.5 and p_spread < 0.05) or (r_var < -0.5 and p_var < 0.05)

    if is_validated:
        print("\n‚úÖ VALIDATED: Strong evidence of constraint attractor.")
        print("   As structural maturity increases, the envelope of possible causal asymmetry collapses.")
    else:
        print("\n‚ö†Ô∏è INCONCLUSIVE: Variance does not significantly decrease.")

    # --- 5. VISUALIZATION: THE FUNNEL OF CONSTRAINT ---

    plt.style.use('seaborn-v0_8-whitegrid')
    fig, ax = plt.subplots(figsize=(10, 6))

    # A. Nuage de points (Discret)
    ax.scatter(df['gamma'], df['imbalance'], alpha=0.1, color='#95a5a6', s=5, label='Raw Observations')

    # B. Calcul des quantiles glissants pour le lissage visuel
    # On trie pour le plot continu
    df_sorted = df.sort_values('gamma')
    window = int(0.15 * len(df))  # 15% sliding window

    rolling_gamma = df_sorted['gamma'].rolling(window, center=True).mean()
    rolling_q10 = df_sorted['imbalance'].rolling(window, center=True).quantile(0.1)
    rolling_q50 = df_sorted['imbalance'].rolling(window, center=True).median()
    rolling_q90 = df_sorted['imbalance'].rolling(window, center=True).quantile(0.9)

    # C. Zone de Contrainte (The Funnel)
    ax.fill_between(rolling_gamma, rolling_q10, rolling_q90,
                    color='#e74c3c', alpha=0.2, label='Constraint Envelope (10th-90th %)')

    # D. M√©diane
    ax.plot(rolling_gamma, rolling_q50, color='#c0392b', linewidth=2, linestyle='--', label='Median Imbalance')

    # E. Annotations th√©oriques
    ax.arrow(0.2, 0.8, 0.4, -0.4, head_width=0.02, color='black', alpha=0.5)
    ax.text(0.4, 0.85, "Phase 1: High Freedom\n(Large Variance)", ha='center', color='#3498db', fontweight='bold')
    ax.text(0.85, 0.15, "Phase 2: Locked\n( collapsed)", ha='center', color='#27ae60', fontweight='bold')

    ax.set_xlabel(r'Structural Maturity ($\Gamma$)', fontsize=12)
    ax.set_ylabel('Causal Imbalance (|Act->Str - Str->Act|)', fontsize=12)
    ax.set_title('Variance Collapse: The Emergence of Distributed Constraint', fontsize=14, fontweight='bold')
    ax.set_xlim(0, 1.0)
    ax.set_ylim(0, 1.0)
    ax.legend(loc='upper right')

    plt.tight_layout()
    plt.savefig("omega_v44_variance_collapse.png", dpi=300)
    print(f"‚úÖ Visualization saved: omega_v44_variance_collapse.png")
    plt.close()

    return {'r_spread': r_spread, 'p_spread': p_spread}
if __name__ == "__main__":
    # La m√©thode 'spawn' est cruciale pour la compatibilit√© multiprocessing sur macOS/Windows
    try:
        multiprocessing.set_start_method('spawn', force=True)
    except RuntimeError:
        pass  # D√©j√† d√©fini ou pas n√©cessaire

    # Cr√©ation du dossier de cache s'il n'existe pas
    if not os.path.exists(CACHE_DIR):
        os.makedirs(CACHE_DIR)
    tqdm.set_lock(multiprocessing.RLock())
    print("=" * 80)
    print(f"OMEGA V36 - PARALLEL EXECUTION ({MAX_WORKERS} Workers)")
    print("=" * 80)
    # On ajoute quelques sauts de ligne pour laisser de la place aux barres tqdm
    print("\n" * (len(REPOS_CONFIG) + 1))

    global_results = {}
    all_dataframes = {}

    # ==========================================================================
    # PHASE 1 : EXTRACTION DES DONN√âES
    # ==========================================================================
    print("\n" + "#" * 80)
    print("PHASE 1 : EXTRACTION ET ANALYSE DES REPOSITORIES")
    print("#" * 80)

    with concurrent.futures.ProcessPoolExecutor(max_workers=MAX_WORKERS) as executor:
        future_to_repo = {executor.submit(process_repo, name, i): name for i, name in enumerate(REPOS_CONFIG.keys())}
        for future in concurrent.futures.as_completed(future_to_repo):
            name = future_to_repo[future]
            try:
                result = future.result()
                if result:
                    name, metrics, df = result
                    global_results[name] = metrics
                    all_dataframes[name] = df
                    tqdm.write(f"‚úÖ {name} termin√© ({len(df)} mois)")
            except Exception as e:
                tqdm.write(f"‚ùå Erreur {name}: {e}")

    if not global_results:
        print("\n‚ùå Aucun projet analys√©. V√©rifiez les chemins dans REPOS_CONFIG.")
        exit(1)

    print(f"\nüìä Projets analys√©s avec succ√®s : {len(global_results)}/{len(REPOS_CONFIG)}")

    # ==========================================================================
    # PHASE 2 : COMPARAISON DES M√âTRIQUES GAMMA (NOUVEAU V36)
    # ==========================================================================
    print("\n" + "#" * 80)
    print("PHASE 2 : COMPARAISON DES M√âTRIQUES GAMMA (V36)")
    print("#" * 80)

    gamma_comparison = compare_gamma_metrics(all_dataframes)

    # ==========================================================================
    # PHASE 3 : VISUALISATIONS DE BASE
    # ==========================================================================
    print("\n" + "#" * 80)
    print("PHASE 3 : G√âN√âRATION DES VISUALISATIONS")
    print("#" * 80)

    # 3.1 Diagramme de Phase Global (Moyennes)
    print("\n[3.1] Diagramme de Phase Global...")
    plot_phase_diagram(global_results)

    # 3.2 Nuage de Dispersion
    print("\n[3.2] Nuage de Dispersion Ontologique...")
    plot_dispersion_cloud(all_dataframes)

    # 3.3 Histogramme de Bimodalit√©
    print("\n[3.3] Histogramme de Bimodalit√©...")
    plot_bimodality_histogram(all_dataframes)

    # 3.4 Vues Duales par projet
    print("\n[3.4] G√©n√©ration des vues duales par projet...")
    for name, df in all_dataframes.items():
        if df is not None and not df.empty:
            plot_dual_view(df, name, REPOS_CONFIG[name])

    # ==========================================================================
    # PHASE 4 : TESTS STATISTIQUES PRINCIPAUX
    # ==========================================================================
    print("\n" + "#" * 80)
    print("PHASE 4 : TESTS STATISTIQUES (HYPOTH√àSES H_2-H_5)")
    print("#" * 80)

    # 4.1 Test H_2 : Bimodalit√© (Hartigan Dip Test)
    print("\n[4.1] TEST H_2 : Bimodalit√©...")
    bimodality_results = run_statistical_tests(all_dataframes)

    # 4.2 Test A : Temps de R√©sidence (Attracteurs)
    print("\n[4.2] TEST A : Temps de R√©sidence...")
    residence_results = run_residence_time_test(all_dataframes)

    # ==========================================================================
    # PHASE 5 : ANALYSE DE R√âGRESSION (COURBES DE CROISSANCE)
    # ==========================================================================
    print("\n" + "#" * 80)
    print("PHASE 5 : R√âGRESSION - MOD√àLES DE CROISSANCE")
    print("#" * 80)

    logistic_results = run_regression_analysis_on_gamma(
        all_dataframes=all_dataframes,
        train_window=24,
        hold_out=2
    )

    # ==========================================================================
    # PHASE 6 : TESTS DE CAUSALIT√â DE GRANGER
    # ==========================================================================
    print("\n" + "#" * 80)
    print("PHASE 6 : CAUSALIT√â DE GRANGER")
    print("#" * 80)

    # 6.1 Test K : Granger simple
    print("\n[6.1] TEST K : Causalit√© de Granger (global)...")
    granger_results = run_granger_test(all_dataframes, max_lag=6)

    # 6.2 Test K-bis : Granger segment√© par phase
    print("\n[6.2] TEST K-BIS : Granger segment√© par phase...")
    granger_phase_results = test_granger_by_phase(all_dataframes, max_lag=4)
    plot_granger_by_phase(granger_phase_results)

    # ==========================================================================
    # PHASE 7 : D√âTECTION DE LA SINGULARIT√â (CROSSOVER)
    # ==========================================================================
    print("\n" + "#" * 80)
    print("PHASE 7 : D√âTECTION DU POINT Œ© (ROLLING GRANGER)")
    print("#" * 80)

    crossover_results = {}
    for name, df in all_dataframes.items():
        if len(df) > 60:  # Minimum 5 ans d'historique

            crossover_res = analyze_rolling_granger(name, df, window_size=36, max_lag=3)
            if crossover_res:
                plot_causal_crossover(name, crossover_res)
                crossover_results[name] = crossover_res
        else:
            print(f"[{name}] Historique trop court ({len(df)} mois < 60)")

    # ==========================================================================
    # VISUALISATION FINALE : LA CONVERGENCE UNIVERSELLE (FIGURE 3)
    # ==========================================================================
    print("\n" + "-" * 60)
    print("G√âN√âRATION DE LA FIGURE UNIVERSELLE ")
    print("-" * 60)

    # Appel de la nouvelle fonction
    # Note : crossover_results doit √™tre un dictionnaire {nom_projet: resultat_rolling}
    # Il est rempli dans la boucle Phase 7 ci-dessus
    if crossover_results:
        # On passe aussi all_dataframes pour avoir les Gamma
        plot_phase_space_academic(all_dataframes, crossover_results)

    # ==========================================================================
    # PHASE 7B : TESTS continuous symmetrization
    # ==========================================================================
    print("\n" + "#" * 80)
    print("PHASE 7B : TESTS continuous symmetrization")
    print("#" * 80)
    if crossover_results and all_dataframes:
        test_variance_collapse_symmetrization(crossover_results, all_dataframes)

        # ==========================================================================
        # PHASE 7-TER : ANALYSE DE SURVIE (NATURE-READY)
        # ==========================================================================
        print("\n" + "#" * 80)
        print("PHASE 7-TER : ANALYSE DE SURVIE (Asymmetry Exposure vs Survival)")
        print("#" * 80)

        if crossover_results and all_dataframes:
            # On passe PROJECT_STATUS juste pour info, mais le calcul est ind√©pendant
            try:
                survival_val = SurvivalAsymmetryValidator(all_dataframes, crossover_results, PROJECT_STATUS)
                survival_val.prepare_data()  # Etape cruciale ajout√©e
                survival_val.run_kaplan_meier()
                survival_val.run_cox_model()
            except NameError:
                print("‚ö†Ô∏è Erreur : SurvivalAsymmetryValidator ou lifelines manquant.")
            except Exception as e:
                print(f"‚ö†Ô∏è Erreur lors de l'analyse de survie : {e}")
        else:
            print("‚ö†Ô∏è Donn√©es insuffisantes pour l'analyse de survie (manque Granger ou Dataframes).")
    print("\n" + "#" * 80)
    print("PHASE 8A :causal_symmetry_diagnostic")
    print("#" * 80)
    # ==========================================================================
    # PHASE 8A : CASUAL SIMETRY
    # ==========================================================================
    from causal_sim import run_causal_symmetry_diagnostic
    diagnostic_results = run_causal_symmetry_diagnostic(
        all_dataframes,
        crossover_results
    )
    # ==========================================================================
    # PHASE 8 : TESTS COMPL√âMENTAIRES
    # ==========================================================================
    print("\n" + "#" * 80)
    print("PHASE 8 : TESTS COMPL√âMENTAIRES (IRR√âVERSIBILIT√â, VARIANCE)")
    print("#" * 80)

    pct_universal = run_complementary_tests(all_dataframes, logistic_results)

    generate_project_recap(all_dataframes, global_results)

    # ==========================================================================
    # PHASE 9 : BLINDAGE SCIENTIFIQUE COMPLET (V44 + V37)
    # ==========================================================================
    print("\n" + "#" * 80)
    print("PHASE 9 : VALIDATION SCIENTIFIQUE AVANC√âE")
    print("#" * 80)

    # --- PARTIE 1 : Validation "Reviewer-Proof" (Classe ScientificValidator V44) ---
    # Remplace les anciens tests V40 (gamma_robustness, temporal_cut) qui plantaient
    if crossover_results:
        print("\n--- [A] VALIDATION STRUCTURELLE (V44) ---")
        sci_validator = ScientificValidator(all_dataframes)
        sci_validator.run_full_suite(crossover_results)
    else:
        print("‚ö†Ô∏è Crossover results manquants. Validation V44 ignor√©e.")

    # --- PARTIE 2 : Tests de Robustesse Compl√©mentaires (Classe RobustnessValidator V37) ---
    # On conserve ces tests car ils apportent des infos diff√©rentes (Heatmap, Z-Score)
    print("\n--- [B] TESTS DE ROBUSTESSE COMPL√âMENTAIRES (V37) ---")
    validator = RobustnessValidator(all_dataframes)

    # 1. Sensitivity Analysis (Heatmap) -> Toujours utile pour voir l'impact des param√®tres
    validator.run_sensitivity_analysis()

    # 2. Null Model (Z-Score) -> Preuve que la trajectoire n'est pas al√©atoire
    validator.run_null_model(n_permutations=200)

    # 3. Covariate Control -> V√©rifie que ce n'est pas juste la taille de l'√©quipe
    validator.run_covariate_control()

    # Note : On ne lance plus run_predictive_validation ici car la PHASE 10 (Hindcasting)
    # fait exactement la m√™me chose en beaucoup plus complet juste apr√®s.

    print("\nüèÅ PHASE 9 TERMIN√âE.")
    # ==========================================================================
    # PHASE 10 : HINDCASTING
    # ==========================================================================
    print("\n" + "#" * 80)
    print("PHASE 10 : VALIDATION PR√âDICTIVE (HINDCASTING)")
    print("#" * 80)

    print("\n" + "#" * 80)
    print("PHASE 10 : VALIDATION PR√âDICTIVE (HINDCASTING)")
    print("#" * 80)

    validator = HindcastingValidator(all_dataframes, PROJECT_STATUS)
    results = validator.run_full_validation()
    # ==========================================================================
    # PHASE 11 : VALIDATION EXTERNE (ANTI-CIRCULARIT√â)
    # ==========================================================================
    print("\n" + "#" * 80)
    print("PHASE 11 : VALIDATION EXTERNE (Gouvernance & Institutionnalisation)")
    print("#" * 80)

    # Cette fonction lance les calculs ET g√©n√®re le graphique annot√©
    validator = ExternalMaturityValidator(all_dataframes, GOVERNANCE_TIER)
    gov_results = validator.validate_governance()

    # ==========================================================================
    # PHASE 12: CO-MODIFICATION COUPLING ANALYSIS (V41)
    # ==========================================================================
    print("\\n" + "#" * 80)
    print("PHASE 12 : CO-MODIFICATION COUPLING ANALYSIS (V41)")
    print("#" * 80)
    print("\\nHypothesis: Œì ‚Üë ‚Üí fan-in ‚Üë ‚Üí Granger S‚ÜíA ‚Üë")
    print("Testing if structural maturity correlates with file interdependence...\\n")

    # Run the analysis
    comod_results = run_comodification_analysis(
        repos_config=REPOS_CONFIG,
        gamma_dataframes=all_dataframes,
        cache_dir=CACHE_DIR + "comod/",
        max_workers=6
          # Use 0.5 for faster analysis on large repos
    )

    # ==========================================================================
    # PHASE 12-BIS: VALIDATE THE MECHANISM (Œì ‚Üí Fan-In ‚Üí Granger)
    # ==========================================================================

    if comod_results['correlations']:
        print("\\n" + "=" * 80)
        print("MECHANISM VALIDATION: Œì ‚Üí Fan-In ‚Üí Structural Constraint")
        print("=" * 80)

        # Merge co-modification metrics with Granger results
        validate_comod_granger_link(
            comod_results=comod_results,
            granger_phase_results=granger_phase_results,
            all_dataframes=all_dataframes
        )

    # ==========================================================================
    # PHASE 12-TER: CORR√âLATION TEMPORELLE Fan-In ‚Üî Granger
    # ==========================================================================

    if comod_results['temporal_metrics'] and crossover_results:
        print("\n" + "=" * 80)
        print("PHASE 12-TER : CORR√âLATION TEMPORELLE Fan-In(t) ‚Üî Granger(t)")
        print("=" * 80)

        fanin_granger_results = correlate_fanin_with_granger_temporal(
            comod_temporal_metrics=comod_results['temporal_metrics'],
            crossover_results=crossover_results,
            all_dataframes=all_dataframes
        )

    if comod_results.get('correlations'):
        print("\n" + "-" * 60)
        print("G√âN√âRATION FIGURE : Two Architectural Paths")
        print("-" * 60)

        plot_bidirectional_architecture_patterns(
            correlations=comod_results['correlations'],
            crossover_results=crossover_results,
            output_path="figure_architecture_paths.png"
        )
    # ==========================================================================
    # PHASE 13: SYNTH√àSE FINALE
    # ==========================================================================
    print("\n" + "#" * 80)
    print("SYNTH√àSE FINALE V36")
    print("#" * 80)

    print(f"""
‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë                           R√âSULTATS OMEGA V36                                ‚ïë
‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£
‚ïë  Projets analys√©s     : {len(global_results):>3} / {len(REPOS_CONFIG):<3}                                        ‚ïë
‚ïë  Horizon de survie    : {SURVIVAL_HORIZON_MONTHS} mois                                              ‚ïë
‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£
‚ïë  M√âTRIQUES GAMMA (Corr√©lations)                                              ‚ïë
‚ïë  ‚îú‚îÄ Structure vs Content : r = {gamma_comparison['corr_structure_content']:.3f}                                 ‚ïë
‚ïë  ‚îú‚îÄ Structure vs Hybrid  : r = {gamma_comparison['corr_structure_hybrid']:.3f}                                 ‚ïë
‚ïë  ‚îî‚îÄ Content vs Hybrid    : r = {gamma_comparison['corr_content_hybrid']:.3f}                                 ‚ïë
‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£
‚ïë  TESTS STATISTIQUES                                                          ‚ïë""")
    granger_coupling = granger_phase_results.get('shift_result', {})
    cp1 = granger_coupling.get('coupling_p1', 0)
    cp2 = granger_coupling.get('coupling_p2', 0)

    print(f"""
    ‚ïë  ‚îú‚îÄ H_2 (Bimodalit√©)      : {"‚úÖ VALID√â" if bimodality_results['p_value'] < 0.05 else "‚ö†Ô∏è  PARTIEL"} (p={bimodality_results['p_value']:.4f})              ‚ïë
    ‚ïë  ‚îú‚îÄ H_4 (Universalit√©)    : {"‚úÖ VALID√â" if pct_universal >= 90 else "‚ùå NON"} ({pct_universal:.0f}% reach high Œì)                ‚ïë
    ‚ïë  ‚îú‚îÄ H_1 (Sym√©trie Causale): {granger_coupling.get('verdict', 'N/A')}                           ‚ïë
    ‚ïë  ‚îÇ   ‚îî‚îÄ Coupling Ratio   : {cp1:.2f} (Phase 1) ‚Üí {cp2:.2f} (Phase 2)            ‚ïë
    ‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£
    """)


    print(f"""‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£
‚ïë  FICHIERS G√âN√âR√âS                                                            ‚ïë
‚ïë  ‚îú‚îÄ omega_v36_gamma_comparison.png     (Comparaison des Œì)                   ‚ïë
‚ïë  ‚îú‚îÄ omega_v34_bimodality_histogram.png (Distribution)                        ‚ïë
‚ïë  ‚îú‚îÄ omega_v34_residence_time_analysis.png (Attracteurs)                      ‚ïë
‚ïë  ‚îú‚îÄ omega_v34_granger_by_phase.png     (Causalit√©)                           ‚ïë
‚ïë  ‚îú‚îÄ omega_v34_dispersion_cloud.png     (Dispersion)                          ‚ïë
‚ïë  ‚îî‚îÄ omega_v35_*_crossover.png          (Singularit√©s par projet)             ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
""")

    # ==========================================================================
    # EXPORT CSV (optionnel, pour analyse externe)
    # ==========================================================================
    print("\n[EXPORT] Sauvegarde des donn√©es en CSV...")

    # Concat√©ner tous les DataFrames avec le nom du projet
    all_data = []
    for name, df in all_dataframes.items():
        if df is not None and not df.empty:
            df_export = df.copy()
            df_export['project'] = name
            df_export['project_status'] = PROJECT_STATUS.get(name, 'unknown')
            all_data.append(df_export)

    if all_data:
        combined_df = pd.concat(all_data, ignore_index=False)

        # S√©lectionner les colonnes pertinentes
        export_cols = [
            'project', 'project_status',
            'total_weight', 'sedimented_weight',
            'monthly_gamma', 'gamma_s', 'gamma_c',
            'v2_lines', 'v3_lines', 'files_touched', 'files_survived',
            'temps_depuis_debut', 'n_contributors', 'intensity_per_dev'
        ]

        # Filtrer les colonnes qui existent
        export_cols = [c for c in export_cols if c in combined_df.columns]

        combined_df[export_cols].to_csv("omega_v36_all_data.csv")
        print(f"   ‚úÖ Donn√©es export√©es : omega_v36_all_data.csv ({len(combined_df)} lignes)")
        # ==========================================================================
        # TEST DE VALIDIT√â PHYSIQUE (GAMMA <= 1.0)
        # ==========================================================================
        print("\n" + "=" * 80)
        print("V√âRIFICATION : BORNAGE NATUREL DE GAMMA (SANS CLIP)")
        print("=" * 80)

        max_gamma_found = 0
        projects_exceeding = []

        for name, df in all_dataframes.items():
            if df is not None and not df.empty:
                local_max = df['monthly_gamma'].max()
                if local_max > max_gamma_found:
                    max_gamma_found = local_max

                # On tol√®re une infime erreur de virgule flottante (1.00000001)
                if local_max > 1.00001:
                    projects_exceeding.append((name, local_max))

        print(f"Gamma Maximum absolu observ√© : {max_gamma_found:.6f}")

        if not projects_exceeding:
            print("‚úÖ SUCC√àS : Aucun projet ne d√©passe naturellement 1.0.")
            print("   La structure est physiquement conservatrice (Operational Closure valid√©e).")
        else:
            print(f"‚ö†Ô∏è ATTENTION : {len(projects_exceeding)} projet(s) d√©passent 1.0 :")
            for p, v in projects_exceeding:
                print(f"   - {p}: {v:.4f}")
    print("\n" + "=" * 80)
    print("ANALYSE V36 TERMIN√âE")
    print("=" * 80)